{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"miniproj3_supplementary_results.ipynb","provenance":[{"file_id":"1id7t1S5Fv0C3cN0eI8KngkD0xF70weC4","timestamp":1606568889464},{"file_id":"1d7iBHSM2eOP3TGPn8MSVF1qjRH4o8kQw","timestamp":1606407311528},{"file_id":"1ilOMxRzzhYlOKa2_5S6ST2hoU4sPihWg","timestamp":1606398472395},{"file_id":"1h8zi8lgBTiwKf0Dq3bsyE5iZ4LZrtwWy","timestamp":1606237990212}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EUPQSmyKUlV","executionInfo":{"status":"ok","timestamp":1607348499589,"user_tz":300,"elapsed":2375,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"dc1f2169-f749-4763-ec04-9ff6bf68181f"},"source":["! [ ! -z \"$COLAB_GPU\" ] && pip install torch scikit-learn==0.21.* skorch"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: scikit-learn==0.21.* in /usr/local/lib/python3.6/dist-packages (0.21.3)\n","Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.9.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.*) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.*) (0.17.0)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.7)\n","Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JmtFAox0KOWI","executionInfo":{"status":"ok","timestamp":1607348571713,"user_tz":300,"elapsed":3526,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["import pickle\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","import torch\n","import time\n","import pandas as pd\n","\n","from itertools import islice\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import log_loss, make_scorer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from skorch import NeuralNetClassifier\n","from skorch.dataset import CVSplit\n","\n","from skorch.helper import SliceDataset\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"HF8WhppgKOWK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607348575996,"user_tz":300,"elapsed":490,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"ffa5052d-27a1-4cc5-8665-e1fa09499f78"},"source":["# Check CPU/GPU\n","USE_CUDA = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","  USE_CUDA = 1\n","  print(f\"Nvidia Cuda/GPU is available!\")\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Nvidia Cuda/GPU is available!\n","Mon Dec  7 13:42:55 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FBWEmIrVKhpM"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKVQdIIYKk2t","executionInfo":{"status":"ok","timestamp":1607348614576,"user_tz":300,"elapsed":33773,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"439e72af-1692-4401-a265-9818bf5d9e46"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive' )"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkj5rFJtKxUD","executionInfo":{"status":"ok","timestamp":1607348620154,"user_tz":300,"elapsed":1139,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"927e10f3-0e2e-42d9-bf3f-b5094544cbbd"},"source":["# can't upload input data files to Github because it's too big? \n","# so each of us will need to change this to where we store data on google drive\n","\n","%cd '/content/gdrive/MyDrive/Colab/ECSE551Miniproject/Mini-Project3'"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/Colab/ECSE551Miniproject/Mini-Project3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"Fk8H22FFKOWK","executionInfo":{"status":"ok","timestamp":1607348628608,"user_tz":300,"elapsed":6218,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"9815defb-1c61-47d7-fcf4-0a17cbef2db5"},"source":["# Read a pickle file and disply its samples\n","# Note that image data are stored as unit8 so each element is an integer value between 0 and 255\n","data = pickle.load( open( './Train.pkl', 'rb' ), encoding='float32')\n","targets = np.genfromtxt('./TrainLabels.csv', delimiter=',', skip_header=1)[:,1:]-5\n","plt.imshow(data[1234,:,:],cmap='gray', vmin=0, vmax=256)\n","print(data.shape, targets.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(60000, 64, 128) (60000, 1)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"SuYS_eofLMwO","executionInfo":{"status":"ok","timestamp":1607348631365,"user_tz":300,"elapsed":335,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["# Transforms are common image transformations. They can be chained together using Compose.\n","# Here we normalize images img=(img-0.5)/0.5\n","img_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5], [0.5])\n","])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNjLB1BFKOWQ","executionInfo":{"status":"ok","timestamp":1607348644206,"user_tz":300,"elapsed":549,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["class MyDataset(Dataset):\n","    \n","    def __init__(self, img_file, label_file, transform=None, idx = None):\n","      \"\"\"function load training or testing data\"\"\"\n","\n","      # read out img_file (.pkl)\n","      self.data = pickle.load( open( img_file, 'rb' ), encoding='bytes')\n","\n","      # read out label_file (.csv)\n","      self.targets = np.genfromtxt(label_file, delimiter=',', skip_header=1)[:,1:]-5\n","\n","      if idx is not None:\n","        # idx: binary vector for creating training and validation set.\n","        # Only return samples where idx is not None\n","\n","        # set idx for target\n","        self.targets = self.targets[idx]\n","        # set idx for data\n","        self.data = self.data[idx]\n","\n","      # transform to normalize images\n","      self.transform = transform\n","\n","    def __len__(self):\n","      \"\"\"function to get len of target\"\"\"\n","\n","      return len(self.targets)\n","\n","    def __getitem__(self, index):\n","      \"\"\"function to get img and target\"\"\"\n","\n","      # get img and target\n","      img, target = self.data[index], int(self.targets[index])\n","      img = Image.fromarray(img.astype('uint8'), mode='L')\n","      \n","      # doing transform to normalize images\n","      if self.transform is not None:\n","        img = self.transform(img)\n","\n","      return img, target\n","\n","class TestDataset(Dataset):\n","    \n","    def __init__(self, img_file, transform=None, idx = None):\n","      \"\"\"function load training or testing data\"\"\"\n","\n","      # read out img_file (.pkl)\n","      self.data = pickle.load( open( img_file, 'rb' ), encoding='bytes')\n","\n","      if idx is not None:\n","        # idx: binary vector for creating training and validation set.\n","        # Only return samples where idx is not None\n","\n","        # set idx for data\n","        self.data = self.data[idx]\n","\n","      # transform to normalize images\n","      self.transform = transform\n","\n","    def __len__(self):\n","      \"\"\"function to get len of target\"\"\"\n","\n","      return len(self.data)\n","\n","    def __getitem__(self, index):\n","      \"\"\"function to get img and target\"\"\"\n","\n","      # get img and target\n","      img = self.data[index]\n","      img = Image.fromarray(img.astype('uint8'), mode='L')\n","      \n","      # doing transform to normalize images\n","      if self.transform is not None:\n","        img = self.transform(img)\n","\n","      return img\n","  "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4QG8A4-KOWR","executionInfo":{"status":"ok","timestamp":1607348653427,"user_tz":300,"elapsed":2662,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["# Read image data and their label into a Dataset class\n","# Test a portion!! by changing idx!!\n","sample_number = 60000\n","dataset = MyDataset('./Train.pkl', './TrainLabels.csv',transform=img_transform, idx=np.arange(sample_number))\n","\n","# Read Test data (10000)\n","test_final = TestDataset('./Test.pkl',transform=img_transform, idx=np.arange(10000))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMAioQnGKOWR","executionInfo":{"status":"ok","timestamp":1607348664841,"user_tz":300,"elapsed":10271,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["# prepare y_data for sklearn to work\n","y_data = np.array([y for x, y in iter(dataset)])"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r6IwSGzljjku"},"source":["# Fit a CNN"]},{"cell_type":"code","metadata":{"id":"FOWSlkbk-ucu","executionInfo":{"status":"ok","timestamp":1607348664842,"user_tz":300,"elapsed":4826,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["# Imports\n","\"\"\"\n","Code Reference: https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/CNN_architectures\n","\"\"\"\n","\n","# hyperparameter \n","drop_ratio = 0.25\n","kernel_size_CNN = (3,3)\n","\n","# declare number of input and class label\n","num_in =1\n","num_class = 9\n","\n","VGG_types = {\n","    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n","    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n","    \"VGG16\": [\n","        64,\n","        64,\n","        \"M\",\n","        128,\n","        128,\n","        \"M\",\n","        256,\n","        256,\n","        256,\n","        \"M\",\n","        512,\n","        512,\n","        512,\n","        \"M\",\n","        512,\n","        512,\n","        512,\n","        \"M\",\n","    ],\n","    \"VGG19\": [\n","        64,\n","        64,\n","        \"M\",\n","        128,\n","        128,\n","        \"M\",\n","        256,\n","        256,\n","        256,\n","        256,\n","        \"M\",\n","        512,\n","        512,\n","        512,\n","        512,\n","        \"M\",\n","        512,\n","        512,\n","        512,\n","        512,\n","        \"M\",\n","    ],\n","}\n","\n","# VGG_net class inherits from nn.Module class\n","class VGG_net(nn.Module):\n","\n","    def __init__(self, in_channels=num_in, num_classes=num_class, VGG_type = VGG_types[\"VGG16\"]):\n","      \"\"\"constructor\"\"\"\n","\n","      # Call the __init__() function of nn.Module class\n","      super(VGG_net, self).__init__()\n","      \n","      # creating neural network\n","      # (1) convolutional neural network\n","      self.in_channels = in_channels\n","      self.conv_layers = self.create_conv_layers(VGG_type)\n","\n","      # (2) fully connected neural network\n","      self.fcs = nn.Sequential(\n","          nn.Linear(4096, 4096),\n","          nn.ReLU(),\n","          nn.Dropout(p = drop_ratio),\n","          nn.Linear(4096, 4096),\n","          nn.ReLU(),\n","          nn.Dropout(p = drop_ratio),\n","          nn.Linear(4096, num_classes),\n","      )\n","\n","    def create_conv_layers(self, architecture):\n","      \"\"\"function help to create convolutional neural network\"\"\"\n","\n","      # list for layers\n","      layers = []\n","\n","      # define number of input channel\n","      in_channels = self.in_channels\n","\n","      # for loop to construct convolutional neural network according to architecture\n","      for x in architecture:\n","          if type(x) == int:\n","              # create output channel number (when it is not Maxpooling)\n","              out_channels = x\n","\n","              # add layer with parameter with convolutional neural network\n","              layers += [\n","                  nn.Conv2d(\n","                      in_channels=in_channels,\n","                      out_channels=out_channels,\n","                      kernel_size = (3,3),\n","                      stride=(1, 1),\n","                      padding=(1, 1),\n","                  ),\n","\n","                  # Batchnorm to improve performance\n","                  nn.BatchNorm2d(x),\n","                  nn.ReLU(),\n","              ]\n","\n","              # input channel for next input\n","              in_channels = x\n","\n","          elif x == \"M\":\n","            # Maxpooling\n","            layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n","\n","      return nn.Sequential(*layers)\n","        \n","    def forward(self, x):\n","      \"\"\"create forward path for VGG\"\"\"\n","      \n","      # convolutional neural network\n","      x = self.conv_layers(x)\n","      x = x.reshape(x.shape[0], -1)\n","\n","      # fully connected neural network\n","      x = self.fcs(x)\n","\n","      # softmax at the last output layer to get result\n","      m = nn.Softmax(dim = 1)\n","      return m(x)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfQ5j6hc3Ahi","executionInfo":{"status":"ok","timestamp":1607348671615,"user_tz":300,"elapsed":8329,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"14509d6a-9bcc-4f75-8cf8-84d22b3faa32"},"source":["#load model from previous tests or train a new model\n","\n","Loading_flag = False\n","\n","if (Loading_flag):\n","  #  initalization\n","  network = VGG_net().to(device)\n","  optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n","  checkpoint = torch.load(\"./model_trained_1126.tar\")\n","\n","  # load network \n","  network.load_state_dict(checkpoint['model_state_dict'])\n","\n","  # load optimizer \n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","  epoch = checkpoint['epoch']\n","  loss = checkpoint['loss']\n","  network.train()\n","else: \n","  network = VGG_net().to(device)\n","  #optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n","\n","\n","print (network)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["VGG_net(\n","  (conv_layers): Sequential(\n","    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU()\n","    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU()\n","    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): ReLU()\n","    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (16): ReLU()\n","    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (19): ReLU()\n","    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (22): ReLU()\n","    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (26): ReLU()\n","    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (29): ReLU()\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (32): ReLU()\n","    (33): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (36): ReLU()\n","    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (39): ReLU()\n","    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (42): ReLU()\n","    (43): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (fcs): Sequential(\n","    (0): Linear(in_features=4096, out_features=4096, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.25, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU()\n","    (5): Dropout(p=0.25, inplace=False)\n","    (6): Linear(in_features=4096, out_features=9, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ql5koZedKOWS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607348679357,"user_tz":300,"elapsed":864,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"11479f4f-cd8d-4541-d5e6-32dcf6c46afe"},"source":["#setting up skorch NeuralNetClassifier with Callbacks\n","\n","from skorch.callbacks import LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","from skorch.callbacks import Callback\n","\n","class Monitor(Callback):\n","    def on_epoch_end(self, net, **kwargs):\n","         print('learning rate =',net.optimizer_.param_groups[0]['lr'])\n","\n","callbacks=[\n","        ('print', Monitor()), ('lr_scheduler',\n","                     LRScheduler(policy=ReduceLROnPlateau, \n","                     monitor = \"train_loss\",\n","                     ))\n","    ]\n","\n","drop_ratio=0\n","kernel_size_CNN=(4,4)\n","\n","torch.manual_seed(0)\n","network = VGG_net().to(device)\n","\n","print (device)\n","cnn = NeuralNetClassifier(\n","    network,\n","    max_epochs=20,\n","    lr=1e-4,\n","    optimizer=torch.optim.Adam,\n","    batch_size=32,\n","    device=device,\n","    iterator_train__num_workers=4,\n","    iterator_valid__num_workers=4,\n","    callbacks=callbacks,\n","    train_split=CVSplit(3)\n",")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5sT0D-O1KOWS","executionInfo":{"status":"ok","timestamp":1607350493732,"user_tz":300,"elapsed":1805518,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"0b4fc057-396b-445c-b11d-d5ea7551efb6"},"source":["#fit a VGG net with skorch\n","\n","import time \n","\n","#fit\n","startall = time.time()\n","cnn.fit(dataset, y=y_data)\n","endall = time.time()\n","\n","print(f\"Execution Time: {endall-startall:.2f} s\")\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4790\u001b[0m       \u001b[32m0.5452\u001b[0m        \u001b[35m1.1788\u001b[0m  90.1478\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9865\u001b[0m       \u001b[32m0.7497\u001b[0m        \u001b[35m0.7988\u001b[0m  90.2687\n","learning rate = 0.0001\n","      3        \u001b[36m0.6938\u001b[0m       \u001b[32m0.7950\u001b[0m        \u001b[35m0.7025\u001b[0m  90.3246\n","learning rate = 0.0001\n","      4        \u001b[36m0.5059\u001b[0m       \u001b[32m0.8617\u001b[0m        \u001b[35m0.4991\u001b[0m  90.2094\n","learning rate = 0.0001\n","      5        \u001b[36m0.3726\u001b[0m       \u001b[32m0.8720\u001b[0m        \u001b[35m0.4669\u001b[0m  89.9373\n","learning rate = 0.0001\n","      6        \u001b[36m0.2909\u001b[0m       \u001b[32m0.8854\u001b[0m        \u001b[35m0.4163\u001b[0m  90.0045\n","learning rate = 0.0001\n","      7        \u001b[36m0.2364\u001b[0m       \u001b[32m0.9030\u001b[0m        \u001b[35m0.3837\u001b[0m  90.5535\n","learning rate = 0.0001\n","      8        \u001b[36m0.1926\u001b[0m       0.8868        0.4564  90.5174\n","learning rate = 0.0001\n","      9        \u001b[36m0.1611\u001b[0m       \u001b[32m0.9224\u001b[0m        \u001b[35m0.3189\u001b[0m  90.3028\n","learning rate = 0.0001\n","     10        \u001b[36m0.1393\u001b[0m       0.9034        0.3678  90.3003\n","learning rate = 0.0001\n","     11        \u001b[36m0.1204\u001b[0m       0.9084        0.4434  90.0106\n","learning rate = 1e-05\n","     12        \u001b[36m0.0528\u001b[0m       \u001b[32m0.9473\u001b[0m        \u001b[35m0.2340\u001b[0m  90.3879\n","learning rate = 1e-05\n","     13        \u001b[36m0.0269\u001b[0m       \u001b[32m0.9493\u001b[0m        0.2412  90.1242\n","learning rate = 1e-05\n","     14        \u001b[36m0.0173\u001b[0m       \u001b[32m0.9507\u001b[0m        0.2586  90.0663\n","learning rate = 1e-05\n","     15        \u001b[36m0.0121\u001b[0m       0.9473        0.2871  90.0599\n","learning rate = 1e-05\n","     16        \u001b[36m0.0099\u001b[0m       0.9491        0.2898  90.2043\n","learning rate = 1e-05\n","     17        \u001b[36m0.0068\u001b[0m       0.9450        0.3432  89.8011\n","learning rate = 1e-05\n","     18        \u001b[36m0.0056\u001b[0m       0.9492        0.3259  89.7844\n","learning rate = 1e-05\n","     19        \u001b[36m0.0041\u001b[0m       \u001b[32m0.9529\u001b[0m        0.3235  89.8567\n","learning rate = 1e-05\n","     20        \u001b[36m0.0031\u001b[0m       0.9499        0.3514  89.6626\n","Execution Time: 1805.13 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTLFvaujkVho","executionInfo":{"status":"ok","timestamp":1606678648321,"user_tz":300,"elapsed":1003,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"3f0051d8-4549-4a53-d77b-113efb163148"},"source":["# save trained model\n","\n","print (\"save network \\n\")\n","torch.save({\n","      'model_state_dict': network.state_dict(),\n","      'optimizer_state_dict': optimizer.state_dict(),\n","      }, 'model_trained_VGG_16_lr1e-4_80epochs_lr-5_20epochs_lr1e-6_15epochs_1129.tar', _use_new_zipfile_serialization=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["save network \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6_tdLoJpihsm"},"source":["# Grid Search"]},{"cell_type":"markdown","metadata":{"id":"n2PIcUQp6VCd"},"source":["Grid search template with skorch. We did not use these for tests in the end, due to very long training time."]},{"cell_type":"code","metadata":{"id":"rVYUUi57KOWT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606313143817,"user_tz":-480,"elapsed":1329859,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"3553f859-0802-4eef-ff52-05997522fd07"},"source":["from sklearn.model_selection import GridSearchCV\n","from skorch.helper import SliceDataset\n","\n","cnn.set_params(max_epochs=5, verbose=False, train_split=False, callbacks=[])\n","\n","#parameters to grid search\n","uni_kernel_test = [3, 5, 7]\n","\n","params = {\n","    'module_uni_kernel': uni_kernel_test,\n","}\n","\n","cnn.initialize();\n","\n","gs = GridSearchCV(cnn, param_grid=params, scoring='accuracy', verbose=1, cv=3)\n","\n","data_train_sliceable = SliceDataset(dataset)\n","\n","gs.fit(data_train_sliceable, y_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The sizes of 1st fully connected input:  7540\n","Fitting 3 folds for each of 3 candidates, totalling 9 fits\n","The sizes of 1st fully connected input:  7540\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"],"name":"stderr"},{"output_type":"stream","text":["The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 19.1min finished\n"],"name":"stderr"},{"output_type":"stream","text":["The sizes of 1st fully connected input:  7540\n","The sizes of 1st fully connected input:  7540\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=3, error_score='raise-deprecating',\n","             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n","  module_=Cnn(\n","    (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n","    (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n","    (conv2_drop): Dropout2d(p=0.25, inplace=False)\n","    (fc1): Linear(in_features=7540, out_features=2000, bias=True)\n","    (fc2): Linear(in_features=2000, out_features=500, bias=True)\n","    (fc3): Linear(in_features=500, out_features=15, bias=True)\n","    (fc1_drop): Dropout(p=0.25, inplace=False)\n","  ),\n","),\n","             iid='warn', n_jobs=None,\n","             param_grid={'module_uni_kernel': [3, 5, 7]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring='accuracy', verbose=1)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"lZxW8VzEjCoK","executionInfo":{"status":"ok","timestamp":1606314005429,"user_tz":-480,"elapsed":923,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"277e71a2-cfb2-4420-b808-55c611e66470"},"source":["# Plot results\n","parameter_plot = uni_kernel_test\n","\n","save_fig = False\n","fig, ax = plt.subplots()\n","ax.plot(parameter_plot, gs.cv_results_[\"mean_test_score\"])\n","#ax.set_xscale(\"log\")\n","ax.set_xlabel(r\"$\\kernel size$\")\n","ax.set_ylabel(\"Mean Test Score\")\n","ax.grid()\n","\n","ax2 = ax.twinx()\n","ax2.plot(parameter_plot, gs.cv_results_[\"mean_fit_time\"], color=\"r\", linestyle=\"--\")\n","ax2.set_ylabel(\"Mean Fit Time [s]\")\n","\n","if save_fig:\n","  plt.tight_layout()\n","  plt.savefig(\"alpha_grid_search.pdf\", bbox_inches=\"tight\", pad_inches=0)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"FqUo7L8c30bv","executionInfo":{"status":"error","timestamp":1606751846351,"user_tz":300,"elapsed":264,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"d4ce51f5-384f-4b7f-8ee8-06d0066381b0"},"source":["cnn.optimizer_.param_groups[0]['lr']"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-8e0bc5273f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetClassifier' object has no attribute 'optimizer_'"]}]},{"cell_type":"markdown","metadata":{"id":"Otz-blkSzMIw"},"source":["# Test batch size and learning rate"]},{"cell_type":"markdown","metadata":{"id":"0zvYVYtZeOdN"},"source":["Fixed parameters: \n","- VGG-16\n","- drop ratio = 0.25\n","- Optimizer = Adam, no weight decay\n","- epochs = 20\n","\n","Testing parameters:\n","- batch size = 32, 64, 128\n","- learning rate = 5e-5, 1e-4, 5e-4 (fixed)\n","- find corrolation of these two parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ErMY6PFzQrH","executionInfo":{"status":"ok","timestamp":1606774428062,"user_tz":300,"elapsed":16170111,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"dda75ad4-9dcb-4d6e-8a07-96d005df5726"},"source":["batch_size_pool = [32, 64, 128]\n","learning_rate_pool = [5e-5, 1e-4, 5e-4]\n","batch_size = []\n","learning_rate = []\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n","callbacks = []\n","\n","for b_size in batch_size_pool:\n","\n","  for l_r in learning_rate_pool:\n","\n","    network = VGG_net().to(device)\n","    torch.manual_seed(0)\n","    cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs=20,\n","        lr=l_r,\n","        optimizer=torch.optim.Adam,\n","        batch_size=b_size,\n","        device=device,\n","        iterator_train__num_workers=4,\n","        iterator_valid__num_workers=4,\n","        callbacks=callbacks,\n","    )\n","    print(f\"fitting with batch size of {cnn.get_params()['batch_size']} and learning rate of {cnn.get_params()['lr']}\")\n","\n","    startall = time.time()\n","    cnn.fit(dataset, y=y_data)\n","    endall = time.time()\n","    timeall = endall-startall\n","\n","    train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","    valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","    valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","    dur.append(np.average(cnn.history[:, 'dur']))\n","    batch_size.append(b_size)\n","    learning_rate.append(l_r)\n","    total_time.append(timeall)\n","    \n","    print(f\"batch size: {b_size}\")\n","    print(f\"learning rate: {l_r}\")\n","    print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","    print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","    print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","    print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","    print(f\"Total Time: {timeall:.2f} s\")\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fitting with batch size of 32 and learning rate of 5e-05\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4992\u001b[0m       \u001b[32m0.4901\u001b[0m        \u001b[35m1.3817\u001b[0m  103.0763\n","      2        \u001b[36m0.9839\u001b[0m       \u001b[32m0.7039\u001b[0m        \u001b[35m0.9065\u001b[0m  102.9231\n","      3        \u001b[36m0.5850\u001b[0m       \u001b[32m0.8562\u001b[0m        \u001b[35m0.4865\u001b[0m  103.2315\n","      4        \u001b[36m0.3940\u001b[0m       \u001b[32m0.8708\u001b[0m        \u001b[35m0.4386\u001b[0m  103.3250\n","      5        \u001b[36m0.2953\u001b[0m       \u001b[32m0.8857\u001b[0m        \u001b[35m0.4136\u001b[0m  102.7297\n","      6        \u001b[36m0.2319\u001b[0m       \u001b[32m0.8971\u001b[0m        \u001b[35m0.4070\u001b[0m  102.8143\n","      7        \u001b[36m0.1939\u001b[0m       0.8921        0.4122  102.7355\n","      8        \u001b[36m0.1623\u001b[0m       \u001b[32m0.9135\u001b[0m        \u001b[35m0.3290\u001b[0m  102.7239\n","      9        \u001b[36m0.1373\u001b[0m       0.8871        0.4566  102.9640\n","     10        \u001b[36m0.1203\u001b[0m       \u001b[32m0.9198\u001b[0m        0.3336  102.8465\n","     11        \u001b[36m0.1061\u001b[0m       0.9080        0.4418  102.5042\n","     12        \u001b[36m0.0998\u001b[0m       \u001b[32m0.9259\u001b[0m        \u001b[35m0.3184\u001b[0m  102.7741\n","     13        \u001b[36m0.0882\u001b[0m       0.9250        0.3209  103.0276\n","     14        \u001b[36m0.0804\u001b[0m       0.9181        0.3893  103.0757\n","     15        \u001b[36m0.0772\u001b[0m       \u001b[32m0.9286\u001b[0m        \u001b[35m0.3139\u001b[0m  103.1384\n","     16        \u001b[36m0.0684\u001b[0m       0.9279        \u001b[35m0.3006\u001b[0m  103.5511\n","     17        \u001b[36m0.0633\u001b[0m       0.9180        0.3635  103.6016\n","     18        \u001b[36m0.0630\u001b[0m       \u001b[32m0.9380\u001b[0m        \u001b[35m0.2884\u001b[0m  103.5721\n","     19        \u001b[36m0.0572\u001b[0m       0.9379        0.3329  103.5049\n","     20        \u001b[36m0.0504\u001b[0m       \u001b[32m0.9405\u001b[0m        0.2943  103.0609\n","batch size: 32\n","learning rate: 5e-05\n","lowest train loss: 0.05036345354408604\n","lowest valid loss: 0.28837356451162194\n","max valid accuracy: 0.9405297351324338\n","time per epoch: 103.05902562141418\n","Total Time: 2064.22 s\n","fitting with batch size of 32 and learning rate of 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4847\u001b[0m       \u001b[32m0.5766\u001b[0m        \u001b[35m1.1439\u001b[0m  102.9273\n","      2        \u001b[36m0.9927\u001b[0m       \u001b[32m0.7355\u001b[0m        \u001b[35m0.8308\u001b[0m  103.4710\n","      3        \u001b[36m0.7109\u001b[0m       \u001b[32m0.8153\u001b[0m        \u001b[35m0.6122\u001b[0m  103.5660\n","      4        \u001b[36m0.5329\u001b[0m       \u001b[32m0.8233\u001b[0m        0.6159  103.2213\n","      5        \u001b[36m0.4327\u001b[0m       \u001b[32m0.8340\u001b[0m        \u001b[35m0.5949\u001b[0m  102.9891\n","      6        \u001b[36m0.3541\u001b[0m       \u001b[32m0.8865\u001b[0m        \u001b[35m0.4471\u001b[0m  102.9340\n","      7        \u001b[36m0.3058\u001b[0m       \u001b[32m0.9000\u001b[0m        \u001b[35m0.3879\u001b[0m  102.9309\n","      8        \u001b[36m0.2475\u001b[0m       \u001b[32m0.9080\u001b[0m        \u001b[35m0.3761\u001b[0m  103.4336\n","      9        \u001b[36m0.2126\u001b[0m       \u001b[32m0.9224\u001b[0m        \u001b[35m0.3186\u001b[0m  103.2126\n","     10        \u001b[36m0.1832\u001b[0m       0.9139        0.3560  102.9321\n","     11        \u001b[36m0.1535\u001b[0m       \u001b[32m0.9282\u001b[0m        0.3260  103.4063\n","     12        \u001b[36m0.1350\u001b[0m       \u001b[32m0.9327\u001b[0m        \u001b[35m0.3128\u001b[0m  103.6043\n","     13        \u001b[36m0.1234\u001b[0m       0.9081        0.4243  103.3506\n","     14        \u001b[36m0.1107\u001b[0m       \u001b[32m0.9413\u001b[0m        \u001b[35m0.2862\u001b[0m  103.2253\n","     15        \u001b[36m0.1000\u001b[0m       0.9387        0.3084  103.0738\n","     16        \u001b[36m0.0966\u001b[0m       \u001b[32m0.9437\u001b[0m        \u001b[35m0.2686\u001b[0m  103.2981\n","     17        \u001b[36m0.0813\u001b[0m       \u001b[32m0.9556\u001b[0m        \u001b[35m0.2224\u001b[0m  103.2516\n","     18        0.0815       0.9476        0.2571  103.2848\n","     19        \u001b[36m0.0711\u001b[0m       0.9408        0.3054  103.1113\n","     20        \u001b[36m0.0686\u001b[0m       0.9492        0.2464  103.2866\n","batch size: 32\n","learning rate: 0.0001\n","lowest train loss: 0.06859995570263466\n","lowest valid loss: 0.22236808775721298\n","max valid accuracy: 0.9556055305680493\n","time per epoch: 103.22553962469101\n","Total Time: 2067.45 s\n","fitting with batch size of 32 and learning rate of 0.0005\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m2.0403\u001b[0m       \u001b[32m0.2073\u001b[0m        \u001b[35m1.8482\u001b[0m  103.2683\n","      2        \u001b[36m1.5487\u001b[0m       \u001b[32m0.3569\u001b[0m        \u001b[35m1.5173\u001b[0m  103.1624\n","      3        \u001b[36m1.3110\u001b[0m       0.1179        3.4269  103.3670\n","      4        \u001b[36m1.1398\u001b[0m       \u001b[32m0.6619\u001b[0m        \u001b[35m1.0339\u001b[0m  103.1179\n","      5        \u001b[36m1.0040\u001b[0m       \u001b[32m0.7580\u001b[0m        \u001b[35m0.8795\u001b[0m  103.2589\n","      6        \u001b[36m0.9110\u001b[0m       \u001b[32m0.8021\u001b[0m        \u001b[35m0.7531\u001b[0m  103.3168\n","      7        \u001b[36m0.8166\u001b[0m       0.7373        0.8527  103.1443\n","      8        \u001b[36m0.7702\u001b[0m       0.7675        0.7806  102.9585\n","      9        \u001b[36m0.7125\u001b[0m       \u001b[32m0.8140\u001b[0m        \u001b[35m0.6699\u001b[0m  103.2352\n","     10        \u001b[36m0.6649\u001b[0m       \u001b[32m0.8608\u001b[0m        \u001b[35m0.5667\u001b[0m  103.1343\n","     11        \u001b[36m0.6546\u001b[0m       0.8338        0.6383  103.1767\n","     12        \u001b[36m0.5995\u001b[0m       0.6657        0.9234  103.2319\n","     13        0.5995       \u001b[32m0.8617\u001b[0m        \u001b[35m0.5616\u001b[0m  103.7470\n","     14        \u001b[36m0.5464\u001b[0m       0.7978        0.6710  102.9701\n","     15        0.5537       \u001b[32m0.8773\u001b[0m        \u001b[35m0.5090\u001b[0m  103.0595\n","     16        \u001b[36m0.5393\u001b[0m       0.8543        0.5484  102.7119\n","     17        \u001b[36m0.5310\u001b[0m       0.8673        0.5151  102.8099\n","     18        \u001b[36m0.4810\u001b[0m       0.8202        0.6628  102.9718\n","     19        0.4869       \u001b[32m0.8793\u001b[0m        0.5219  103.2085\n","     20        \u001b[36m0.4291\u001b[0m       0.8638        0.5158  103.4069\n","batch size: 32\n","learning rate: 0.0005\n","lowest train loss: 0.4290958262249426\n","lowest valid loss: 0.5089610111022341\n","max valid accuracy: 0.8793103448275862\n","time per epoch: 103.16288863420486\n","Total Time: 2066.28 s\n","fitting with batch size of 64 and learning rate of 5e-05\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5394\u001b[0m       \u001b[32m0.5282\u001b[0m        \u001b[35m1.1919\u001b[0m  86.7787\n","      2        \u001b[36m1.0219\u001b[0m       \u001b[32m0.6981\u001b[0m        \u001b[35m0.9253\u001b[0m  86.7864\n","      3        \u001b[36m0.6380\u001b[0m       \u001b[32m0.7316\u001b[0m        \u001b[35m0.8996\u001b[0m  86.6967\n","      4        \u001b[36m0.4150\u001b[0m       \u001b[32m0.8499\u001b[0m        \u001b[35m0.5190\u001b[0m  86.7273\n","      5        \u001b[36m0.3134\u001b[0m       0.7482        0.9503  86.7627\n","      6        \u001b[36m0.2447\u001b[0m       \u001b[32m0.8537\u001b[0m        0.5380  86.9512\n","      7        \u001b[36m0.1958\u001b[0m       \u001b[32m0.8606\u001b[0m        0.5507  86.9177\n","      8        \u001b[36m0.1613\u001b[0m       \u001b[32m0.8919\u001b[0m        \u001b[35m0.4398\u001b[0m  86.9207\n","      9        \u001b[36m0.1422\u001b[0m       \u001b[32m0.8978\u001b[0m        \u001b[35m0.3947\u001b[0m  86.9506\n","     10        \u001b[36m0.1219\u001b[0m       0.8873        0.4574  86.9925\n","     11        \u001b[36m0.1046\u001b[0m       \u001b[32m0.9059\u001b[0m        \u001b[35m0.3875\u001b[0m  86.9756\n","     12        \u001b[36m0.0933\u001b[0m       \u001b[32m0.9063\u001b[0m        0.4259  87.0109\n","     13        \u001b[36m0.0849\u001b[0m       0.9001        0.4663  86.8451\n","     14        \u001b[36m0.0810\u001b[0m       \u001b[32m0.9099\u001b[0m        0.4204  86.9381\n","     15        \u001b[36m0.0745\u001b[0m       0.9007        0.4455  86.9021\n","     16        \u001b[36m0.0686\u001b[0m       \u001b[32m0.9159\u001b[0m        \u001b[35m0.3687\u001b[0m  86.8156\n","     17        \u001b[36m0.0599\u001b[0m       0.8987        0.4687  86.8711\n","     18        \u001b[36m0.0594\u001b[0m       \u001b[32m0.9165\u001b[0m        \u001b[35m0.3629\u001b[0m  86.8950\n","     19        \u001b[36m0.0560\u001b[0m       0.9132        0.4196  86.8434\n","     20        \u001b[36m0.0522\u001b[0m       0.9053        0.4776  87.0181\n","batch size: 64\n","learning rate: 5e-05\n","lowest train loss: 0.05218408180821973\n","lowest valid loss: 0.36290921570955664\n","max valid accuracy: 0.9165417291354323\n","time per epoch: 86.87996697425842\n","Total Time: 1739.20 s\n","fitting with batch size of 64 and learning rate of 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5093\u001b[0m       \u001b[32m0.5822\u001b[0m        \u001b[35m1.1277\u001b[0m  86.7851\n","      2        \u001b[36m0.9845\u001b[0m       \u001b[32m0.7341\u001b[0m        \u001b[35m0.8459\u001b[0m  86.9074\n","      3        \u001b[36m0.6952\u001b[0m       \u001b[32m0.7971\u001b[0m        \u001b[35m0.7054\u001b[0m  86.8792\n","      4        \u001b[36m0.5030\u001b[0m       \u001b[32m0.8116\u001b[0m        \u001b[35m0.6457\u001b[0m  87.0517\n","      5        \u001b[36m0.3674\u001b[0m       0.8075        0.7176  86.8798\n","      6        \u001b[36m0.2761\u001b[0m       \u001b[32m0.8792\u001b[0m        \u001b[35m0.4825\u001b[0m  86.9484\n","      7        \u001b[36m0.2292\u001b[0m       \u001b[32m0.9011\u001b[0m        \u001b[35m0.3910\u001b[0m  86.8541\n","      8        \u001b[36m0.1955\u001b[0m       \u001b[32m0.9041\u001b[0m        \u001b[35m0.3888\u001b[0m  86.8325\n","      9        \u001b[36m0.1569\u001b[0m       \u001b[32m0.9189\u001b[0m        \u001b[35m0.3288\u001b[0m  86.9100\n","     10        \u001b[36m0.1429\u001b[0m       0.8911        0.4815  86.9418\n","     11        \u001b[36m0.1239\u001b[0m       \u001b[32m0.9242\u001b[0m        \u001b[35m0.3279\u001b[0m  87.0391\n","     12        \u001b[36m0.1150\u001b[0m       0.9236        \u001b[35m0.3193\u001b[0m  86.9513\n","     13        \u001b[36m0.1047\u001b[0m       0.9197        0.3439  87.0544\n","     14        \u001b[36m0.0897\u001b[0m       \u001b[32m0.9263\u001b[0m        0.3261  87.0208\n","     15        \u001b[36m0.0762\u001b[0m       \u001b[32m0.9414\u001b[0m        \u001b[35m0.2808\u001b[0m  86.9640\n","     16        0.0835       0.9287        0.3089  87.1320\n","     17        \u001b[36m0.0655\u001b[0m       0.9388        0.2825  86.9526\n","     18        \u001b[36m0.0649\u001b[0m       0.9264        0.3409  86.9153\n","     19        \u001b[36m0.0641\u001b[0m       0.9342        0.2815  86.9101\n","     20        \u001b[36m0.0527\u001b[0m       \u001b[32m0.9440\u001b[0m        0.2839  87.0856\n","batch size: 64\n","learning rate: 0.0001\n","lowest train loss: 0.05272333963035727\n","lowest valid loss: 0.2807738796464805\n","max valid accuracy: 0.9440279860069966\n","time per epoch: 86.95075639486313\n","Total Time: 1740.55 s\n","fitting with batch size of 64 and learning rate of 0.0005\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.9340\u001b[0m       \u001b[32m0.3173\u001b[0m        \u001b[35m1.5577\u001b[0m  86.8780\n","      2        \u001b[36m1.4149\u001b[0m       0.1127        7.3733  87.0639\n","      3        \u001b[36m1.1672\u001b[0m       \u001b[32m0.6438\u001b[0m        \u001b[35m1.0025\u001b[0m  87.0045\n","      4        \u001b[36m0.9569\u001b[0m       0.1147        6.0272  87.0698\n","      5        \u001b[36m0.8229\u001b[0m       \u001b[32m0.6662\u001b[0m        1.0140  86.9240\n","      6        \u001b[36m0.7181\u001b[0m       \u001b[32m0.7567\u001b[0m        \u001b[35m0.7460\u001b[0m  86.8903\n","      7        \u001b[36m0.6490\u001b[0m       \u001b[32m0.7624\u001b[0m        0.7828  86.8777\n","      8        \u001b[36m0.6014\u001b[0m       0.1597        3.3586  86.9998\n","      9        \u001b[36m0.5737\u001b[0m       0.6467        0.9253  87.0627\n","     10        \u001b[36m0.5257\u001b[0m       \u001b[32m0.8306\u001b[0m        \u001b[35m0.5897\u001b[0m  86.8954\n","     11        \u001b[36m0.4842\u001b[0m       0.7727        0.6986  87.0538\n","     12        \u001b[36m0.4757\u001b[0m       0.1138        9.7862  87.0031\n","     13        \u001b[36m0.4383\u001b[0m       0.8287        \u001b[35m0.5671\u001b[0m  86.8997\n","     14        \u001b[36m0.4129\u001b[0m       \u001b[32m0.8774\u001b[0m        \u001b[35m0.4398\u001b[0m  86.9222\n","     15        \u001b[36m0.3927\u001b[0m       \u001b[32m0.8786\u001b[0m        \u001b[35m0.4376\u001b[0m  86.9061\n","     16        0.3971       0.8687        0.4561  86.9400\n","     17        \u001b[36m0.3731\u001b[0m       0.8611        0.4976  87.0349\n","     18        \u001b[36m0.3244\u001b[0m       0.5840        1.1260  86.8898\n","     19        \u001b[36m0.3131\u001b[0m       0.2126        3.2388  86.9965\n","     20        \u001b[36m0.3053\u001b[0m       0.8503        0.4416  86.8946\n","batch size: 64\n","learning rate: 0.0005\n","lowest train loss: 0.30531617702506664\n","lowest valid loss: 0.4375525065288928\n","max valid accuracy: 0.8785607196401799\n","time per epoch: 86.96035977602006\n","Total Time: 1740.82 s\n","fitting with batch size of 128 and learning rate of 5e-05\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.6045\u001b[0m       \u001b[32m0.3903\u001b[0m        \u001b[35m1.5496\u001b[0m  78.9682\n","      2        \u001b[36m1.1265\u001b[0m       \u001b[32m0.3992\u001b[0m        1.9780  78.9995\n","      3        \u001b[36m0.7446\u001b[0m       \u001b[32m0.7619\u001b[0m        \u001b[35m0.7601\u001b[0m  79.0196\n","      4        \u001b[36m0.4471\u001b[0m       0.6873        1.1652  79.0752\n","      5        \u001b[36m0.3389\u001b[0m       0.6415        1.2683  79.0560\n","      6        \u001b[36m0.2700\u001b[0m       \u001b[32m0.8259\u001b[0m        \u001b[35m0.6675\u001b[0m  79.0781\n","      7        \u001b[36m0.2149\u001b[0m       0.7848        0.8613  78.9833\n","      8        \u001b[36m0.1776\u001b[0m       0.8148        0.7655  78.9947\n","      9        \u001b[36m0.1418\u001b[0m       \u001b[32m0.8739\u001b[0m        \u001b[35m0.5062\u001b[0m  79.1459\n","     10        \u001b[36m0.1319\u001b[0m       0.8437        0.6500  79.1428\n","     11        \u001b[36m0.1088\u001b[0m       0.8127        0.8557  79.1038\n","     12        \u001b[36m0.0904\u001b[0m       \u001b[32m0.8771\u001b[0m        0.5261  79.0907\n","     13        \u001b[36m0.0862\u001b[0m       0.8637        0.6127  79.1560\n","     14        \u001b[36m0.0765\u001b[0m       0.8711        0.6072  79.1099\n","     15        \u001b[36m0.0714\u001b[0m       \u001b[32m0.9008\u001b[0m        \u001b[35m0.4264\u001b[0m  79.0969\n","     16        \u001b[36m0.0595\u001b[0m       0.8882        0.5266  79.0560\n","     17        0.0603       0.8797        0.5842  79.1325\n","     18        \u001b[36m0.0533\u001b[0m       0.8736        0.5975  79.0660\n","     19        \u001b[36m0.0497\u001b[0m       0.8839        0.5566  79.0635\n","     20        0.0526       0.8644        0.6467  78.9493\n","batch size: 128\n","learning rate: 5e-05\n","lowest train loss: 0.049693901819888864\n","lowest valid loss: 0.42640344403136793\n","max valid accuracy: 0.9007996001999\n","time per epoch: 79.06440020799637\n","Total Time: 1582.13 s\n","fitting with batch size of 128 and learning rate of 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5291\u001b[0m       \u001b[32m0.2727\u001b[0m        \u001b[35m2.6094\u001b[0m  79.0081\n","      2        \u001b[36m1.0369\u001b[0m       \u001b[32m0.4289\u001b[0m        \u001b[35m1.8825\u001b[0m  79.1475\n","      3        \u001b[36m0.7059\u001b[0m       \u001b[32m0.6428\u001b[0m        \u001b[35m1.2032\u001b[0m  79.1570\n","      4        \u001b[36m0.4539\u001b[0m       \u001b[32m0.8588\u001b[0m        \u001b[35m0.4913\u001b[0m  79.1580\n","      5        \u001b[36m0.3474\u001b[0m       0.8401        0.5842  79.1059\n","      6        \u001b[36m0.2781\u001b[0m       \u001b[32m0.8914\u001b[0m        \u001b[35m0.3891\u001b[0m  79.2198\n","      7        \u001b[36m0.2210\u001b[0m       0.7588        1.0279  79.0528\n","      8        \u001b[36m0.1906\u001b[0m       0.8834        0.4111  79.1320\n","      9        \u001b[36m0.1621\u001b[0m       \u001b[32m0.9014\u001b[0m        \u001b[35m0.3520\u001b[0m  79.1194\n","     10        \u001b[36m0.1425\u001b[0m       0.8692        0.5420  79.0168\n","     11        \u001b[36m0.1231\u001b[0m       \u001b[32m0.9107\u001b[0m        \u001b[35m0.3518\u001b[0m  79.1722\n","     12        \u001b[36m0.1009\u001b[0m       0.8942        0.4569  79.2097\n","     13        \u001b[36m0.0936\u001b[0m       0.8681        0.5603  79.2217\n","     14        \u001b[36m0.0816\u001b[0m       0.8841        0.5052  79.1225\n","     15        \u001b[36m0.0769\u001b[0m       \u001b[32m0.9220\u001b[0m        0.3747  79.0836\n","     16        \u001b[36m0.0708\u001b[0m       0.9086        0.4160  79.1248\n","     17        \u001b[36m0.0636\u001b[0m       \u001b[32m0.9245\u001b[0m        \u001b[35m0.3097\u001b[0m  79.0668\n","     18        0.0637       0.8159        0.9524  79.1515\n","     19        \u001b[36m0.0536\u001b[0m       \u001b[32m0.9352\u001b[0m        \u001b[35m0.2767\u001b[0m  79.0041\n","     20        \u001b[36m0.0527\u001b[0m       \u001b[32m0.9360\u001b[0m        0.3008  79.0845\n","batch size: 128\n","learning rate: 0.0001\n","lowest train loss: 0.05272191281183941\n","lowest valid loss: 0.2766761491231852\n","max valid accuracy: 0.936031984007996\n","time per epoch: 79.11793946027755\n","Total Time: 1583.20 s\n","fitting with batch size of 128 and learning rate of 0.0005\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m2.1391\u001b[0m       \u001b[32m0.2486\u001b[0m        \u001b[35m1.8815\u001b[0m  79.1099\n","      2        \u001b[36m1.5759\u001b[0m       \u001b[32m0.3047\u001b[0m        \u001b[35m1.6401\u001b[0m  79.0055\n","      3        \u001b[36m1.3498\u001b[0m       \u001b[32m0.5255\u001b[0m        \u001b[35m1.2530\u001b[0m  79.0029\n","      4        \u001b[36m1.1973\u001b[0m       0.2715        1.6303  78.9918\n","      5        \u001b[36m1.0569\u001b[0m       0.1114        3.8304  79.1183\n","      6        \u001b[36m0.9198\u001b[0m       \u001b[32m0.6047\u001b[0m        \u001b[35m1.0839\u001b[0m  79.0416\n","      7        \u001b[36m0.8208\u001b[0m       \u001b[32m0.7992\u001b[0m        \u001b[35m0.7022\u001b[0m  79.1277\n","      8        \u001b[36m0.7508\u001b[0m       0.7797        0.7590  79.1620\n","      9        \u001b[36m0.6856\u001b[0m       \u001b[32m0.8057\u001b[0m        0.7128  79.2086\n","     10        \u001b[36m0.6458\u001b[0m       0.7271        0.8262  79.0352\n","     11        \u001b[36m0.5966\u001b[0m       0.7871        0.7202  79.0356\n","     12        \u001b[36m0.5547\u001b[0m       0.5483        1.1878  79.0555\n","     13        \u001b[36m0.5217\u001b[0m       0.4467        1.6241  79.0670\n","     14        0.5242       \u001b[32m0.8810\u001b[0m        \u001b[35m0.4685\u001b[0m  79.0048\n","     15        \u001b[36m0.4401\u001b[0m       0.8722        0.4737  79.0370\n","     16        0.4513       0.7562        0.7135  79.1133\n","     17        \u001b[36m0.4067\u001b[0m       0.5115        1.2177  79.1570\n","     18        \u001b[36m0.3939\u001b[0m       0.8124        0.6414  79.0512\n","     19        \u001b[36m0.3860\u001b[0m       0.4704        1.5222  79.0461\n","     20        \u001b[36m0.3755\u001b[0m       \u001b[32m0.9022\u001b[0m        \u001b[35m0.4021\u001b[0m  79.1310\n","batch size: 128\n","learning rate: 0.0005\n","lowest train loss: 0.37549259304881083\n","lowest valid loss: 0.40213909555073285\n","max valid accuracy: 0.902215558887223\n","time per epoch: 79.07510333061218\n","Total Time: 1582.34 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9zkAnnFTw6l5"},"source":["Observation on the side: \n","- Batch size 32, learning rate 5e-5, max accuracy starts to oscillate as early as epoch #6.\n","- Batch size 64, learning rate 5e-5, did some crazy jumping around\n","- max valid accuracy sometimes doesn't occur at min valid loss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VcgGAZnNQkz","executionInfo":{"status":"ok","timestamp":1606774750894,"user_tz":300,"elapsed":351,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"7d19dbd4-a0e1-400c-e93e-cf455b4da916"},"source":["epochs =[20, 17, 19, 18, 20, 15, 15, 20, 20]\n","\n","rawdata= {'batch_size': batch_size,'learning_rate': learning_rate, 'train_loss': train_loss, \n","          'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, \n","          'total_time': total_time, 'best_result_epoch': epochs}\n","bsize_lr = pd.DataFrame(rawdata, columns = ['batch_size','learning_rate','train_loss',\n","                                            'valid_loss','valid_acc','time/epoch','total_time', 'best_result_epoch'])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   batch_size  learning_rate  ...   total_time  best_result_epoch\n","0          32        0.00005  ...  2064.220238                 20\n","1          32        0.00010  ...  2067.452497                 17\n","2          32        0.00050  ...  2066.282730                 19\n","3          64        0.00005  ...  1739.201634                 18\n","4          64        0.00010  ...  1740.548820                 20\n","5          64        0.00050  ...  1740.824659                 15\n","6         128        0.00005  ...  1582.125470                 15\n","7         128        0.00010  ...  1583.204564                 20\n","8         128        0.00050  ...  1582.344212                 20\n","\n","[9 rows x 8 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"luVvYMEnPSPf","executionInfo":{"status":"ok","timestamp":1606774766460,"user_tz":300,"elapsed":352,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"dd082adb-4f15-43de-88b7-beacfc8fe192"},"source":["bsize_lr"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>batch_size</th>\n","      <th>learning_rate</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","      <th>total_time</th>\n","      <th>best_result_epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>32</td>\n","      <td>0.00005</td>\n","      <td>0.050363</td>\n","      <td>0.288374</td>\n","      <td>0.940530</td>\n","      <td>103.059026</td>\n","      <td>2064.220238</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>32</td>\n","      <td>0.00010</td>\n","      <td>0.068600</td>\n","      <td>0.222368</td>\n","      <td>0.955606</td>\n","      <td>103.225540</td>\n","      <td>2067.452497</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>32</td>\n","      <td>0.00050</td>\n","      <td>0.429096</td>\n","      <td>0.508961</td>\n","      <td>0.879310</td>\n","      <td>103.162889</td>\n","      <td>2066.282730</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>64</td>\n","      <td>0.00005</td>\n","      <td>0.052184</td>\n","      <td>0.362909</td>\n","      <td>0.916542</td>\n","      <td>86.879967</td>\n","      <td>1739.201634</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>64</td>\n","      <td>0.00010</td>\n","      <td>0.052723</td>\n","      <td>0.280774</td>\n","      <td>0.944028</td>\n","      <td>86.950756</td>\n","      <td>1740.548820</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>64</td>\n","      <td>0.00050</td>\n","      <td>0.305316</td>\n","      <td>0.437553</td>\n","      <td>0.878561</td>\n","      <td>86.960360</td>\n","      <td>1740.824659</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>128</td>\n","      <td>0.00005</td>\n","      <td>0.049694</td>\n","      <td>0.426403</td>\n","      <td>0.900800</td>\n","      <td>79.064400</td>\n","      <td>1582.125470</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>128</td>\n","      <td>0.00010</td>\n","      <td>0.052722</td>\n","      <td>0.276676</td>\n","      <td>0.936032</td>\n","      <td>79.117939</td>\n","      <td>1583.204564</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>128</td>\n","      <td>0.00050</td>\n","      <td>0.375493</td>\n","      <td>0.402139</td>\n","      <td>0.902216</td>\n","      <td>79.075103</td>\n","      <td>1582.344212</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   batch_size  learning_rate  ...   total_time  best_result_epoch\n","0          32        0.00005  ...  2064.220238                 20\n","1          32        0.00010  ...  2067.452497                 17\n","2          32        0.00050  ...  2066.282730                 19\n","3          64        0.00005  ...  1739.201634                 18\n","4          64        0.00010  ...  1740.548820                 20\n","5          64        0.00050  ...  1740.824659                 15\n","6         128        0.00005  ...  1582.125470                 15\n","7         128        0.00010  ...  1583.204564                 20\n","8         128        0.00050  ...  1582.344212                 20\n","\n","[9 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"3tUYzhmUPcLt"},"source":["bsize_lr.to_csv(\"bsize_lr.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jq3tqa6ULcOz","executionInfo":{"status":"ok","timestamp":1606757017752,"user_tz":300,"elapsed":185,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"2256749d-01f6-49ac-eacd-ec3473d10fff"},"source":["np.max(cnn.history[:, 'valid_acc'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8071797434616025"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"lkoieK6PLhOr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606839434038,"user_tz":300,"elapsed":709,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"cb693ae4-6922-4c5e-9a6e-064ad12d284f"},"source":["drop_ratio"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.25"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"izYWUG0t33E6"},"source":["# Test dropout ratio\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C9NsLIYCfDYS"},"source":["Fixed parameters:\n","- VGG-16\n","- batch size = 32\n","- Optimizer = Adam, no weight decay\n","- learning rate = 1e-4\n","\n","Testing parameters:\n","- drop ratio = 0, 0.1, 0.25, 0.3, 0.5\n","- changing epochs and learning rate to better understand the results\n","- added 5-fold cross-validate to obtain reliable results"]},{"cell_type":"markdown","metadata":{"id":"u25h8D9w6rd-"},"source":["### Tests with dropout ratio for different epochs"]},{"cell_type":"markdown","metadata":{"id":"gizqjhxW6zB5"},"source":["#### 20 epochs, fixed lr=1e-4"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3_jK58v37up","executionInfo":{"status":"ok","timestamp":1606853098444,"user_tz":300,"elapsed":10469869,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"a5225a3e-d734-42d3-836f-14aec10cf6d6"},"source":["drop_ratio_pool = [0, 0.1, 0.25, 0.3, 0.5]\n","dropout_ratio = []\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n","callbacks = []\n","\n","for dr in drop_ratio_pool:\n","\n","  drop_ratio=dr\n","\n","  network = VGG_net().to(device)\n","  torch.manual_seed(0)\n","  cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=20,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n","  )\n","  print(f\"fitting with drop ratio of {dr}\")\n","\n","  startall = time.time()\n","  cnn.fit(dataset, y=y_data)\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  dropout_ratio.append(dr)\n","  total_time.append(timeall)\n","    \n","  print(f\"drop ratio: {drop_ratio}\")\n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fitting with drop ratio of 0\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4777\u001b[0m       \u001b[32m0.5177\u001b[0m        \u001b[35m1.3133\u001b[0m  104.0150\n","      2        \u001b[36m0.9358\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.7335\u001b[0m  103.9541\n","      3        \u001b[36m0.6068\u001b[0m       \u001b[32m0.8380\u001b[0m        \u001b[35m0.5591\u001b[0m  104.1988\n","      4        \u001b[36m0.4183\u001b[0m       \u001b[32m0.8731\u001b[0m        \u001b[35m0.4480\u001b[0m  104.2431\n","      5        \u001b[36m0.3115\u001b[0m       \u001b[32m0.9035\u001b[0m        \u001b[35m0.3657\u001b[0m  104.2606\n","      6        \u001b[36m0.2499\u001b[0m       0.8970        0.4061  104.2821\n","      7        \u001b[36m0.2031\u001b[0m       \u001b[32m0.9105\u001b[0m        \u001b[35m0.3375\u001b[0m  104.2382\n","      8        \u001b[36m0.1618\u001b[0m       \u001b[32m0.9205\u001b[0m        \u001b[35m0.2989\u001b[0m  104.4110\n","      9        \u001b[36m0.1363\u001b[0m       \u001b[32m0.9246\u001b[0m        \u001b[35m0.2939\u001b[0m  104.1458\n","     10        \u001b[36m0.1201\u001b[0m       \u001b[32m0.9279\u001b[0m        0.2993  103.7520\n","     11        \u001b[36m0.1051\u001b[0m       \u001b[32m0.9394\u001b[0m        \u001b[35m0.2387\u001b[0m  103.9073\n","     12        \u001b[36m0.0937\u001b[0m       \u001b[32m0.9439\u001b[0m        \u001b[35m0.2337\u001b[0m  103.4285\n","     13        \u001b[36m0.0878\u001b[0m       0.9316        0.2828  104.2376\n","     14        \u001b[36m0.0737\u001b[0m       0.9360        0.2682  103.9705\n","     15        \u001b[36m0.0673\u001b[0m       0.9352        0.2912  103.6895\n","     16        \u001b[36m0.0653\u001b[0m       0.9426        0.2457  104.2841\n","     17        \u001b[36m0.0581\u001b[0m       0.9407        0.2830  103.7294\n","     18        \u001b[36m0.0564\u001b[0m       \u001b[32m0.9504\u001b[0m        \u001b[35m0.2329\u001b[0m  103.6804\n","     19        \u001b[36m0.0503\u001b[0m       0.9497        \u001b[35m0.2178\u001b[0m  103.9324\n","     20        \u001b[36m0.0460\u001b[0m       \u001b[32m0.9511\u001b[0m        0.2426  103.6363\n","drop ratio: 0\n","lowest train loss: 0.046038850279844015\n","lowest valid loss: 0.2178337293368763\n","max valid accuracy: 0.9511077794436116\n","time per epoch: 103.99983863830566\n","Total Time: 2083.04 s\n","fitting with drop ratio of 0.1\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4791\u001b[0m       \u001b[32m0.5073\u001b[0m        \u001b[35m1.2610\u001b[0m  103.7440\n","      2        \u001b[36m0.9634\u001b[0m       \u001b[32m0.7563\u001b[0m        \u001b[35m0.7929\u001b[0m  104.2974\n","      3        \u001b[36m0.6538\u001b[0m       \u001b[32m0.8068\u001b[0m        \u001b[35m0.6511\u001b[0m  103.9984\n","      4        \u001b[36m0.4537\u001b[0m       \u001b[32m0.8736\u001b[0m        \u001b[35m0.4523\u001b[0m  104.2332\n","      5        \u001b[36m0.3463\u001b[0m       0.8653        0.4795  103.8361\n","      6        \u001b[36m0.2722\u001b[0m       \u001b[32m0.8866\u001b[0m        \u001b[35m0.4351\u001b[0m  103.9729\n","      7        \u001b[36m0.2177\u001b[0m       \u001b[32m0.9062\u001b[0m        \u001b[35m0.3740\u001b[0m  104.0652\n","      8        \u001b[36m0.1831\u001b[0m       \u001b[32m0.9095\u001b[0m        0.3783  104.2012\n","      9        \u001b[36m0.1562\u001b[0m       \u001b[32m0.9130\u001b[0m        \u001b[35m0.3446\u001b[0m  103.7010\n","     10        \u001b[36m0.1371\u001b[0m       0.8987        0.4207  103.9142\n","     11        \u001b[36m0.1151\u001b[0m       \u001b[32m0.9369\u001b[0m        \u001b[35m0.2697\u001b[0m  104.3555\n","     12        \u001b[36m0.1046\u001b[0m       0.9356        \u001b[35m0.2693\u001b[0m  103.9328\n","     13        \u001b[36m0.0941\u001b[0m       \u001b[32m0.9413\u001b[0m        \u001b[35m0.2573\u001b[0m  104.2883\n","     14        \u001b[36m0.0859\u001b[0m       0.9113        0.4080  103.9706\n","     15        \u001b[36m0.0796\u001b[0m       \u001b[32m0.9432\u001b[0m        \u001b[35m0.2295\u001b[0m  104.5447\n","     16        \u001b[36m0.0679\u001b[0m       \u001b[32m0.9455\u001b[0m        \u001b[35m0.2153\u001b[0m  104.7868\n","     17        0.0687       \u001b[32m0.9525\u001b[0m        0.2321  104.8612\n","     18        \u001b[36m0.0627\u001b[0m       0.9430        0.2729  104.6740\n","     19        \u001b[36m0.0550\u001b[0m       0.9408        0.3018  104.5076\n","     20        0.0574       0.9454        0.2308  104.3492\n","drop ratio: 0.1\n","lowest train loss: 0.05504975003079847\n","lowest valid loss: 0.2153315214621666\n","max valid accuracy: 0.9525237381309345\n","time per epoch: 104.21171824932098\n","Total Time: 2087.39 s\n","fitting with drop ratio of 0.25\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4891\u001b[0m       \u001b[32m0.5386\u001b[0m        \u001b[35m1.2556\u001b[0m  104.7662\n","      2        \u001b[36m0.9953\u001b[0m       \u001b[32m0.6995\u001b[0m        \u001b[35m0.9468\u001b[0m  104.8314\n","      3        \u001b[36m0.7018\u001b[0m       \u001b[32m0.8269\u001b[0m        \u001b[35m0.6089\u001b[0m  104.8650\n","      4        \u001b[36m0.5267\u001b[0m       \u001b[32m0.8441\u001b[0m        \u001b[35m0.5861\u001b[0m  104.4744\n","      5        \u001b[36m0.4225\u001b[0m       \u001b[32m0.8776\u001b[0m        \u001b[35m0.4638\u001b[0m  104.8435\n","      6        \u001b[36m0.3434\u001b[0m       \u001b[32m0.8916\u001b[0m        \u001b[35m0.4203\u001b[0m  104.6814\n","      7        \u001b[36m0.2881\u001b[0m       0.8867        0.4800  105.0557\n","      8        \u001b[36m0.2423\u001b[0m       \u001b[32m0.9135\u001b[0m        \u001b[35m0.3514\u001b[0m  105.0794\n","      9        \u001b[36m0.2067\u001b[0m       0.8911        0.4395  104.7083\n","     10        \u001b[36m0.1783\u001b[0m       \u001b[32m0.9226\u001b[0m        \u001b[35m0.3136\u001b[0m  104.7793\n","     11        \u001b[36m0.1524\u001b[0m       \u001b[32m0.9286\u001b[0m        0.3441  105.2838\n","     12        \u001b[36m0.1336\u001b[0m       0.9275        \u001b[35m0.3063\u001b[0m  104.9600\n","     13        \u001b[36m0.1258\u001b[0m       0.9246        0.3482  104.6968\n","     14        \u001b[36m0.1094\u001b[0m       \u001b[32m0.9337\u001b[0m        \u001b[35m0.2925\u001b[0m  104.8428\n","     15        \u001b[36m0.1045\u001b[0m       \u001b[32m0.9418\u001b[0m        \u001b[35m0.2869\u001b[0m  104.6009\n","     16        \u001b[36m0.0920\u001b[0m       0.9296        0.3157  104.4854\n","     17        \u001b[36m0.0808\u001b[0m       \u001b[32m0.9431\u001b[0m        \u001b[35m0.2687\u001b[0m  104.8314\n","     18        \u001b[36m0.0785\u001b[0m       \u001b[32m0.9501\u001b[0m        \u001b[35m0.2479\u001b[0m  104.4213\n","     19        \u001b[36m0.0671\u001b[0m       0.9469        \u001b[35m0.2467\u001b[0m  104.6572\n","     20        \u001b[36m0.0645\u001b[0m       \u001b[32m0.9515\u001b[0m        \u001b[35m0.2372\u001b[0m  104.6703\n","drop ratio: 0.25\n","lowest train loss: 0.06448526302921385\n","lowest valid loss: 0.23717201659857715\n","max valid accuracy: 0.9515242378810594\n","time per epoch: 104.77672506570816\n","Total Time: 2098.67 s\n","fitting with drop ratio of 0.3\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.5003\u001b[0m       \u001b[32m0.4630\u001b[0m        \u001b[35m1.3633\u001b[0m  104.6237\n","      2        \u001b[36m1.0162\u001b[0m       \u001b[32m0.7280\u001b[0m        \u001b[35m0.8633\u001b[0m  104.9254\n","      3        \u001b[36m0.7219\u001b[0m       \u001b[32m0.8354\u001b[0m        \u001b[35m0.5888\u001b[0m  105.4712\n","      4        \u001b[36m0.5410\u001b[0m       \u001b[32m0.8569\u001b[0m        \u001b[35m0.5263\u001b[0m  104.7865\n","      5        \u001b[36m0.4137\u001b[0m       \u001b[32m0.8886\u001b[0m        \u001b[35m0.4233\u001b[0m  104.6179\n","      6        \u001b[36m0.3397\u001b[0m       \u001b[32m0.8947\u001b[0m        0.4264  104.5459\n","      7        \u001b[36m0.2726\u001b[0m       \u001b[32m0.9037\u001b[0m        \u001b[35m0.3770\u001b[0m  104.4017\n","      8        \u001b[36m0.2225\u001b[0m       0.9006        0.3979  105.2004\n","      9        \u001b[36m0.1889\u001b[0m       \u001b[32m0.9201\u001b[0m        \u001b[35m0.3273\u001b[0m  104.8005\n","     10        \u001b[36m0.1660\u001b[0m       0.9182        0.3508  104.7964\n","     11        \u001b[36m0.1380\u001b[0m       0.9120        0.4027  104.8186\n","     12        \u001b[36m0.1287\u001b[0m       \u001b[32m0.9347\u001b[0m        \u001b[35m0.2984\u001b[0m  104.8234\n","     13        \u001b[36m0.1114\u001b[0m       \u001b[32m0.9395\u001b[0m        \u001b[35m0.2702\u001b[0m  104.8711\n","     14        \u001b[36m0.0999\u001b[0m       \u001b[32m0.9399\u001b[0m        0.2703  105.0242\n","     15        \u001b[36m0.0918\u001b[0m       \u001b[32m0.9446\u001b[0m        0.2799  105.0624\n","     16        \u001b[36m0.0855\u001b[0m       0.9394        0.2848  104.4648\n","     17        \u001b[36m0.0740\u001b[0m       0.9445        \u001b[35m0.2569\u001b[0m  104.6445\n","     18        \u001b[36m0.0739\u001b[0m       0.9354        0.3303  104.5410\n","     19        \u001b[36m0.0677\u001b[0m       \u001b[32m0.9504\u001b[0m        \u001b[35m0.2365\u001b[0m  104.5374\n","     20        \u001b[36m0.0663\u001b[0m       \u001b[32m0.9570\u001b[0m        0.2457  104.6150\n","drop ratio: 0.3\n","lowest train loss: 0.06632654598667294\n","lowest valid loss: 0.2364620864602709\n","max valid accuracy: 0.9570214892553723\n","time per epoch: 104.77860035896302\n","Total Time: 2098.69 s\n","fitting with drop ratio of 0.5\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.5394\u001b[0m       \u001b[32m0.5562\u001b[0m        \u001b[35m1.1768\u001b[0m  104.8409\n","      2        \u001b[36m1.0846\u001b[0m       \u001b[32m0.7168\u001b[0m        \u001b[35m0.8814\u001b[0m  104.6216\n","      3        \u001b[36m0.8161\u001b[0m       \u001b[32m0.7435\u001b[0m        \u001b[35m0.8298\u001b[0m  104.7215\n","      4        \u001b[36m0.6139\u001b[0m       \u001b[32m0.8307\u001b[0m        \u001b[35m0.6015\u001b[0m  105.1066\n","      5        \u001b[36m0.4788\u001b[0m       \u001b[32m0.8606\u001b[0m        \u001b[35m0.5131\u001b[0m  104.8107\n","      6        \u001b[36m0.3942\u001b[0m       0.8592        0.5335  104.8885\n","      7        \u001b[36m0.3299\u001b[0m       \u001b[32m0.8975\u001b[0m        \u001b[35m0.4043\u001b[0m  105.2019\n","      8        \u001b[36m0.2884\u001b[0m       0.8780        0.4634  104.7717\n","      9        \u001b[36m0.2533\u001b[0m       \u001b[32m0.9086\u001b[0m        \u001b[35m0.3829\u001b[0m  104.8854\n","     10        \u001b[36m0.2211\u001b[0m       \u001b[32m0.9173\u001b[0m        \u001b[35m0.3570\u001b[0m  104.8611\n","     11        \u001b[36m0.1993\u001b[0m       \u001b[32m0.9210\u001b[0m        \u001b[35m0.3508\u001b[0m  104.8144\n","     12        \u001b[36m0.1751\u001b[0m       0.8936        0.4768  104.7458\n","     13        \u001b[36m0.1602\u001b[0m       \u001b[32m0.9242\u001b[0m        \u001b[35m0.3437\u001b[0m  104.8530\n","     14        \u001b[36m0.1492\u001b[0m       \u001b[32m0.9295\u001b[0m        \u001b[35m0.3058\u001b[0m  104.9057\n","     15        \u001b[36m0.1359\u001b[0m       \u001b[32m0.9455\u001b[0m        \u001b[35m0.2441\u001b[0m  104.6624\n","     16        \u001b[36m0.1271\u001b[0m       0.9322        0.3546  104.8295\n","     17        \u001b[36m0.1078\u001b[0m       0.9429        0.2723  104.7031\n","     18        \u001b[36m0.1072\u001b[0m       \u001b[32m0.9467\u001b[0m        0.2771  104.9765\n","     19        \u001b[36m0.0999\u001b[0m       \u001b[32m0.9471\u001b[0m        0.2890  104.4758\n","     20        \u001b[36m0.0919\u001b[0m       \u001b[32m0.9478\u001b[0m        0.2785  104.7067\n","drop ratio: 0.5\n","lowest train loss: 0.09190363255481371\n","lowest valid loss: 0.24407429614697132\n","max valid accuracy: 0.947776111944028\n","time per epoch: 104.81914029121398\n","Total Time: 2099.50 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g6S4-LXh9WjX"},"source":["epochs =[20, 17, 20, 20, 20]\n","\n","rawdata= {'drop_ratio': dropout_ratio, 'train_loss': train_loss, \n","          'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, \n","          'total_time': total_time, 'best_result_epoch': epochs}\n","drop_fixed_lr = pd.DataFrame(rawdata, columns = ['drop_ratio','train_loss',\n","                                            'valid_loss','valid_acc','time/epoch','total_time', 'best_result_epoch'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"DHP-0_7r6m8p","executionInfo":{"status":"ok","timestamp":1606853242128,"user_tz":300,"elapsed":362,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"cad7ae33-edac-436f-88af-2245117a3e2e"},"source":["drop_fixed_lr\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>drop_ratio</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","      <th>total_time</th>\n","      <th>best_result_epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.046039</td>\n","      <td>0.217834</td>\n","      <td>0.951108</td>\n","      <td>103.999839</td>\n","      <td>2083.041129</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.10</td>\n","      <td>0.055050</td>\n","      <td>0.215332</td>\n","      <td>0.952524</td>\n","      <td>104.211718</td>\n","      <td>2087.388805</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.25</td>\n","      <td>0.064485</td>\n","      <td>0.237172</td>\n","      <td>0.951524</td>\n","      <td>104.776725</td>\n","      <td>2098.668410</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.30</td>\n","      <td>0.066327</td>\n","      <td>0.236462</td>\n","      <td>0.957021</td>\n","      <td>104.778600</td>\n","      <td>2098.686213</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>0.091904</td>\n","      <td>0.244074</td>\n","      <td>0.947776</td>\n","      <td>104.819140</td>\n","      <td>2099.497806</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   drop_ratio  train_loss  ...   total_time  best_result_epoch\n","0        0.00    0.046039  ...  2083.041129                 20\n","1        0.10    0.055050  ...  2087.388805                 17\n","2        0.25    0.064485  ...  2098.668410                 20\n","3        0.30    0.066327  ...  2098.686213                 20\n","4        0.50    0.091904  ...  2099.497806                 20\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"hXHsvj8i7CT5"},"source":["drop_fixed_lr.to_csv(\"drop_fixed_lr.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pv0vOwJR68Lx"},"source":["#### 20 epochs, with LRScheduler=ReduceLROnPlateau"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KMXEHh6o6qt-","executionInfo":{"status":"ok","timestamp":1606863936877,"user_tz":300,"elapsed":10490625,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"2df30578-ba0a-466b-8776-9c42f3803660"},"source":["callbacks=[\n","        ('print', Monitor()), ('lr_scheduler',\n","                     LRScheduler(policy=ReduceLROnPlateau, \n","                     monitor = \"train_loss\"\n","                     ))\n","    ]\n","\n","drop_ratio_pool = [0, 0.1, 0.25, 0.3, 0.5]\n","dropout_ratio = []\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n","\n","for dr in drop_ratio_pool:\n","\n","  drop_ratio=dr\n","\n","  network = VGG_net().to(device)\n","  torch.manual_seed(0)\n","  cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=20,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n","  )\n","  print(f\"fitting with drop ratio of {dr}\")\n","\n","  startall = time.time()\n","  cnn.fit(dataset, y=y_data)\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  dropout_ratio.append(dr)\n","  total_time.append(timeall)\n","    \n","  print(f\"drop ratio: {drop_ratio}\")\n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fitting with drop ratio of 0\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4854\u001b[0m       \u001b[32m0.5222\u001b[0m        \u001b[35m1.2412\u001b[0m  104.7642\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9466\u001b[0m       \u001b[32m0.7038\u001b[0m        \u001b[35m0.8436\u001b[0m  104.3871\n","learning rate = 0.0001\n","      3        \u001b[36m0.6616\u001b[0m       \u001b[32m0.7714\u001b[0m        \u001b[35m0.7397\u001b[0m  104.7022\n","learning rate = 0.0001\n","      4        \u001b[36m0.5205\u001b[0m       \u001b[32m0.8116\u001b[0m        \u001b[35m0.6523\u001b[0m  104.9193\n","learning rate = 0.0001\n","      5        \u001b[36m0.4191\u001b[0m       \u001b[32m0.8726\u001b[0m        \u001b[35m0.4656\u001b[0m  104.5767\n","learning rate = 0.0001\n","      6        \u001b[36m0.3310\u001b[0m       0.8532        0.5554  104.4720\n","learning rate = 0.0001\n","      7        \u001b[36m0.2666\u001b[0m       \u001b[32m0.9117\u001b[0m        \u001b[35m0.3546\u001b[0m  104.5421\n","learning rate = 0.0001\n","      8        \u001b[36m0.2104\u001b[0m       0.8963        0.4044  104.5879\n","learning rate = 0.0001\n","      9        \u001b[36m0.1727\u001b[0m       \u001b[32m0.9264\u001b[0m        \u001b[35m0.3080\u001b[0m  104.8107\n","learning rate = 0.0001\n","     10        \u001b[36m0.1483\u001b[0m       0.9258        \u001b[35m0.2969\u001b[0m  104.8372\n","learning rate = 0.0001\n","     11        \u001b[36m0.1283\u001b[0m       \u001b[32m0.9352\u001b[0m        \u001b[35m0.2698\u001b[0m  104.8110\n","learning rate = 1e-05\n","     12        \u001b[36m0.0595\u001b[0m       \u001b[32m0.9554\u001b[0m        \u001b[35m0.1943\u001b[0m  104.8573\n","learning rate = 1e-05\n","     13        \u001b[36m0.0347\u001b[0m       \u001b[32m0.9586\u001b[0m        \u001b[35m0.1897\u001b[0m  104.9125\n","learning rate = 1e-05\n","     14        \u001b[36m0.0231\u001b[0m       \u001b[32m0.9598\u001b[0m        0.2029  104.4590\n","learning rate = 1e-05\n","     15        \u001b[36m0.0168\u001b[0m       0.9574        0.2194  104.5544\n","learning rate = 1e-05\n","     16        \u001b[36m0.0132\u001b[0m       \u001b[32m0.9616\u001b[0m        0.2110  104.5611\n","learning rate = 1e-05\n","     17        \u001b[36m0.0108\u001b[0m       0.9599        0.2190  104.8433\n","learning rate = 1e-05\n","     18        \u001b[36m0.0080\u001b[0m       0.9616        0.2358  104.6593\n","learning rate = 1e-05\n","     19        \u001b[36m0.0059\u001b[0m       \u001b[32m0.9634\u001b[0m        0.2313  104.6996\n","learning rate = 1e-05\n","     20        \u001b[36m0.0048\u001b[0m       0.9614        0.2515  104.2081\n","drop ratio: 0\n","lowest train loss: 0.004802921799350204\n","lowest valid loss: 0.18969026268258946\n","max valid accuracy: 0.9633516575045811\n","time per epoch: 104.6582457780838\n","Total Time: 2096.22 s\n","fitting with drop ratio of 0.1\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4864\u001b[0m       \u001b[32m0.4541\u001b[0m        \u001b[35m1.3852\u001b[0m  104.5692\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9769\u001b[0m       \u001b[32m0.7339\u001b[0m        \u001b[35m0.8229\u001b[0m  104.6314\n","learning rate = 0.0001\n","      3        \u001b[36m0.6760\u001b[0m       \u001b[32m0.8239\u001b[0m        \u001b[35m0.5840\u001b[0m  104.6059\n","learning rate = 0.0001\n","      4        \u001b[36m0.4941\u001b[0m       \u001b[32m0.8642\u001b[0m        \u001b[35m0.5002\u001b[0m  104.8816\n","learning rate = 0.0001\n","      5        \u001b[36m0.3743\u001b[0m       \u001b[32m0.8816\u001b[0m        \u001b[35m0.4526\u001b[0m  104.8687\n","learning rate = 0.0001\n","      6        \u001b[36m0.2909\u001b[0m       0.8732        0.4847  104.8273\n","learning rate = 0.0001\n","      7        \u001b[36m0.2381\u001b[0m       \u001b[32m0.9076\u001b[0m        \u001b[35m0.3715\u001b[0m  104.6065\n","learning rate = 0.0001\n","      8        \u001b[36m0.1996\u001b[0m       \u001b[32m0.9094\u001b[0m        0.3791  104.8484\n","learning rate = 0.0001\n","      9        \u001b[36m0.1622\u001b[0m       \u001b[32m0.9297\u001b[0m        \u001b[35m0.3041\u001b[0m  104.8477\n","learning rate = 0.0001\n","     10        \u001b[36m0.1392\u001b[0m       0.9221        0.3325  104.5585\n","learning rate = 0.0001\n","     11        \u001b[36m0.1270\u001b[0m       0.9285        0.3140  105.2119\n","learning rate = 1e-05\n","     12        \u001b[36m0.0562\u001b[0m       \u001b[32m0.9558\u001b[0m        \u001b[35m0.1930\u001b[0m  104.7522\n","learning rate = 1e-05\n","     13        \u001b[36m0.0324\u001b[0m       \u001b[32m0.9589\u001b[0m        \u001b[35m0.1897\u001b[0m  104.5187\n","learning rate = 1e-05\n","     14        \u001b[36m0.0222\u001b[0m       \u001b[32m0.9605\u001b[0m        0.1964  104.4049\n","learning rate = 1e-05\n","     15        \u001b[36m0.0164\u001b[0m       \u001b[32m0.9615\u001b[0m        0.2029  104.4046\n","learning rate = 1e-05\n","     16        \u001b[36m0.0138\u001b[0m       \u001b[32m0.9633\u001b[0m        0.2067  104.4117\n","learning rate = 1e-05\n","     17        \u001b[36m0.0100\u001b[0m       0.9623        0.2233  104.8370\n","learning rate = 1e-05\n","     18        \u001b[36m0.0084\u001b[0m       0.9610        0.2359  104.4621\n","learning rate = 1e-05\n","     19        \u001b[36m0.0065\u001b[0m       0.9618        0.2424  104.8907\n","learning rate = 1e-05\n","     20        \u001b[36m0.0052\u001b[0m       0.9633        0.2474  104.6410\n","drop ratio: 0.1\n","lowest train loss: 0.00517630290476829\n","lowest valid loss: 0.18974606849709258\n","max valid accuracy: 0.9632683658170914\n","time per epoch: 104.68900489807129\n","Total Time: 2096.97 s\n","fitting with drop ratio of 0.25\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4833\u001b[0m       \u001b[32m0.5339\u001b[0m        \u001b[35m1.1615\u001b[0m  104.7909\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9905\u001b[0m       \u001b[32m0.7760\u001b[0m        \u001b[35m0.7476\u001b[0m  104.7033\n","learning rate = 0.0001\n","      3        \u001b[36m0.7062\u001b[0m       \u001b[32m0.8248\u001b[0m        \u001b[35m0.6101\u001b[0m  104.8406\n","learning rate = 0.0001\n","      4        \u001b[36m0.5245\u001b[0m       \u001b[32m0.8461\u001b[0m        \u001b[35m0.5461\u001b[0m  104.6380\n","learning rate = 0.0001\n","      5        \u001b[36m0.4003\u001b[0m       \u001b[32m0.8786\u001b[0m        \u001b[35m0.4501\u001b[0m  105.0559\n","learning rate = 0.0001\n","      6        \u001b[36m0.3205\u001b[0m       \u001b[32m0.8973\u001b[0m        \u001b[35m0.4082\u001b[0m  105.1430\n","learning rate = 0.0001\n","      7        \u001b[36m0.2542\u001b[0m       0.8853        0.4475  104.9433\n","learning rate = 0.0001\n","      8        \u001b[36m0.2133\u001b[0m       \u001b[32m0.9071\u001b[0m        \u001b[35m0.3602\u001b[0m  104.8580\n","learning rate = 0.0001\n","      9        \u001b[36m0.1830\u001b[0m       \u001b[32m0.9243\u001b[0m        \u001b[35m0.3218\u001b[0m  104.8132\n","learning rate = 0.0001\n","     10        \u001b[36m0.1533\u001b[0m       0.9216        0.3449  104.9353\n","learning rate = 0.0001\n","     11        \u001b[36m0.1345\u001b[0m       \u001b[32m0.9319\u001b[0m        \u001b[35m0.3034\u001b[0m  104.4508\n","learning rate = 1e-05\n","     12        \u001b[36m0.0628\u001b[0m       \u001b[32m0.9548\u001b[0m        \u001b[35m0.2047\u001b[0m  105.2287\n","learning rate = 1e-05\n","     13        \u001b[36m0.0374\u001b[0m       \u001b[32m0.9594\u001b[0m        \u001b[35m0.1986\u001b[0m  104.9530\n","learning rate = 1e-05\n","     14        \u001b[36m0.0270\u001b[0m       \u001b[32m0.9598\u001b[0m        0.2089  104.4033\n","learning rate = 1e-05\n","     15        \u001b[36m0.0209\u001b[0m       \u001b[32m0.9602\u001b[0m        0.2180  104.8767\n","learning rate = 1e-05\n","     16        \u001b[36m0.0163\u001b[0m       \u001b[32m0.9614\u001b[0m        0.2247  104.6631\n","learning rate = 1e-05\n","     17        \u001b[36m0.0143\u001b[0m       0.9597        0.2375  104.6050\n","learning rate = 1e-05\n","     18        \u001b[36m0.0111\u001b[0m       \u001b[32m0.9615\u001b[0m        0.2412  104.4516\n","learning rate = 1e-05\n","     19        \u001b[36m0.0104\u001b[0m       0.9613        0.2508  104.9159\n","learning rate = 1e-05\n","     20        \u001b[36m0.0076\u001b[0m       \u001b[32m0.9620\u001b[0m        0.2603  104.6586\n","drop ratio: 0.25\n","lowest train loss: 0.007645661886683508\n","lowest valid loss: 0.19864136016451836\n","max valid accuracy: 0.9620189905047476\n","time per epoch: 104.7964081287384\n","Total Time: 2099.17 s\n","fitting with drop ratio of 0.3\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4926\u001b[0m       \u001b[32m0.4649\u001b[0m        \u001b[35m1.3483\u001b[0m  104.5966\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m1.0078\u001b[0m       \u001b[32m0.7428\u001b[0m        \u001b[35m0.8197\u001b[0m  104.7944\n","learning rate = 0.0001\n","      3        \u001b[36m0.7226\u001b[0m       \u001b[32m0.8168\u001b[0m        \u001b[35m0.6119\u001b[0m  105.0044\n","learning rate = 0.0001\n","      4        \u001b[36m0.5405\u001b[0m       \u001b[32m0.8460\u001b[0m        \u001b[35m0.5469\u001b[0m  104.7554\n","learning rate = 0.0001\n","      5        \u001b[36m0.4352\u001b[0m       \u001b[32m0.8696\u001b[0m        \u001b[35m0.4780\u001b[0m  104.6913\n","learning rate = 0.0001\n","      6        \u001b[36m0.3487\u001b[0m       \u001b[32m0.9065\u001b[0m        \u001b[35m0.3686\u001b[0m  104.9059\n","learning rate = 0.0001\n","      7        \u001b[36m0.2851\u001b[0m       0.8989        0.4217  104.8451\n","learning rate = 0.0001\n","      8        \u001b[36m0.2345\u001b[0m       \u001b[32m0.9226\u001b[0m        \u001b[35m0.3033\u001b[0m  104.6869\n","learning rate = 0.0001\n","      9        \u001b[36m0.1983\u001b[0m       0.9058        0.3856  104.8674\n","learning rate = 0.0001\n","     10        \u001b[36m0.1783\u001b[0m       \u001b[32m0.9348\u001b[0m        0.3102  104.9601\n","learning rate = 0.0001\n","     11        \u001b[36m0.1474\u001b[0m       0.9217        0.3398  104.6924\n","learning rate = 1e-05\n","     12        \u001b[36m0.0670\u001b[0m       \u001b[32m0.9573\u001b[0m        \u001b[35m0.2063\u001b[0m  104.9319\n","learning rate = 1e-05\n","     13        \u001b[36m0.0389\u001b[0m       \u001b[32m0.9596\u001b[0m        0.2133  104.6104\n","learning rate = 1e-05\n","     14        \u001b[36m0.0287\u001b[0m       0.9596        0.2187  104.6848\n","learning rate = 1e-05\n","     15        \u001b[36m0.0217\u001b[0m       0.9596        0.2345  104.6974\n","learning rate = 1e-05\n","     16        \u001b[36m0.0182\u001b[0m       \u001b[32m0.9608\u001b[0m        0.2412  104.6752\n","learning rate = 1e-05\n","     17        \u001b[36m0.0143\u001b[0m       0.9608        0.2447  104.8249\n","learning rate = 1e-05\n","     18        \u001b[36m0.0110\u001b[0m       \u001b[32m0.9614\u001b[0m        0.2481  104.4973\n","learning rate = 1e-05\n","     19        0.0111       0.9601        0.2730  104.7136\n","learning rate = 1e-05\n","     20        \u001b[36m0.0086\u001b[0m       \u001b[32m0.9632\u001b[0m        0.2650  104.5348\n","drop ratio: 0.3\n","lowest train loss: 0.008578071979454456\n","lowest valid loss: 0.20632258874945053\n","max valid accuracy: 0.9631850741296019\n","time per epoch: 104.74851802587509\n","Total Time: 2098.08 s\n","fitting with drop ratio of 0.5\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.5382\u001b[0m       \u001b[32m0.4472\u001b[0m        \u001b[35m1.4032\u001b[0m  104.7197\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m1.0800\u001b[0m       \u001b[32m0.7115\u001b[0m        \u001b[35m0.9016\u001b[0m  104.6191\n","learning rate = 0.0001\n","      3        \u001b[36m0.8071\u001b[0m       \u001b[32m0.7896\u001b[0m        \u001b[35m0.7037\u001b[0m  104.6434\n","learning rate = 0.0001\n","      4        \u001b[36m0.6117\u001b[0m       \u001b[32m0.8323\u001b[0m        \u001b[35m0.6446\u001b[0m  104.8750\n","learning rate = 0.0001\n","      5        \u001b[36m0.4837\u001b[0m       \u001b[32m0.8686\u001b[0m        \u001b[35m0.5176\u001b[0m  104.7413\n","learning rate = 0.0001\n","      6        \u001b[36m0.4011\u001b[0m       \u001b[32m0.8693\u001b[0m        \u001b[35m0.4976\u001b[0m  104.8253\n","learning rate = 0.0001\n","      7        \u001b[36m0.3325\u001b[0m       \u001b[32m0.8878\u001b[0m        \u001b[35m0.4857\u001b[0m  105.0211\n","learning rate = 0.0001\n","      8        \u001b[36m0.2867\u001b[0m       0.8629        0.5571  104.6365\n","learning rate = 0.0001\n","      9        \u001b[36m0.2479\u001b[0m       \u001b[32m0.9103\u001b[0m        \u001b[35m0.3726\u001b[0m  104.9153\n","learning rate = 0.0001\n","     10        \u001b[36m0.2157\u001b[0m       0.9051        0.4213  104.9643\n","learning rate = 0.0001\n","     11        \u001b[36m0.1934\u001b[0m       \u001b[32m0.9293\u001b[0m        \u001b[35m0.2923\u001b[0m  104.7849\n","learning rate = 1e-05\n","     12        \u001b[36m0.0922\u001b[0m       \u001b[32m0.9539\u001b[0m        \u001b[35m0.2177\u001b[0m  104.4243\n","learning rate = 1e-05\n","     13        \u001b[36m0.0590\u001b[0m       \u001b[32m0.9561\u001b[0m        0.2187  104.8057\n","learning rate = 1e-05\n","     14        \u001b[36m0.0432\u001b[0m       \u001b[32m0.9576\u001b[0m        0.2238  104.5831\n","learning rate = 1e-05\n","     15        \u001b[36m0.0370\u001b[0m       0.9569        0.2317  104.6431\n","learning rate = 1e-05\n","     16        \u001b[36m0.0282\u001b[0m       0.9567        0.2464  104.6955\n","learning rate = 1e-05\n","     17        \u001b[36m0.0236\u001b[0m       \u001b[32m0.9586\u001b[0m        0.2474  104.5274\n","learning rate = 1e-05\n","     18        \u001b[36m0.0214\u001b[0m       \u001b[32m0.9597\u001b[0m        0.2667  104.3630\n","learning rate = 1e-05\n","     19        \u001b[36m0.0186\u001b[0m       0.9595        0.2517  104.8140\n","learning rate = 1e-05\n","     20        \u001b[36m0.0169\u001b[0m       0.9584        0.2693  104.8371\n","drop ratio: 0.5\n","lowest train loss: 0.01690665643051263\n","lowest valid loss: 0.2177428342599013\n","max valid accuracy: 0.9596868232550392\n","time per epoch: 104.72195727825165\n","Total Time: 2097.57 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HKfFK1hvF8Io"},"source":["epochs =[19, 20, 20, 20, 18]\n","\n","rawdata= {'drop_ratio': dropout_ratio, 'train_loss': train_loss, \n","          'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, \n","          'total_time': total_time, 'best_result_epoch': epochs}\n","drop_lrschedule = pd.DataFrame(rawdata, columns = ['drop_ratio','train_loss',\n","                                            'valid_loss','valid_acc','time/epoch','total_time', 'best_result_epoch'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"iQu6edUIkGzR","executionInfo":{"status":"ok","timestamp":1606864111989,"user_tz":300,"elapsed":309,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"b4c2afac-3c7e-4231-b9ac-69b3c75e80e6"},"source":["drop_lrschedule"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>drop_ratio</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","      <th>total_time</th>\n","      <th>best_result_epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.004803</td>\n","      <td>0.189690</td>\n","      <td>0.963352</td>\n","      <td>104.658246</td>\n","      <td>2096.220825</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.10</td>\n","      <td>0.005176</td>\n","      <td>0.189746</td>\n","      <td>0.963268</td>\n","      <td>104.689005</td>\n","      <td>2096.965207</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.25</td>\n","      <td>0.007646</td>\n","      <td>0.198641</td>\n","      <td>0.962019</td>\n","      <td>104.796408</td>\n","      <td>2099.172104</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.30</td>\n","      <td>0.008578</td>\n","      <td>0.206323</td>\n","      <td>0.963185</td>\n","      <td>104.748518</td>\n","      <td>2098.075379</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>0.016907</td>\n","      <td>0.217743</td>\n","      <td>0.959687</td>\n","      <td>104.721957</td>\n","      <td>2097.573800</td>\n","      <td>18</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   drop_ratio  train_loss  ...   total_time  best_result_epoch\n","0        0.00    0.004803  ...  2096.220825                 19\n","1        0.10    0.005176  ...  2096.965207                 20\n","2        0.25    0.007646  ...  2099.172104                 20\n","3        0.30    0.008578  ...  2098.075379                 20\n","4        0.50    0.016907  ...  2097.573800                 18\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"wCDk6hNWkIgY"},"source":["drop_lrschedule.to_csv(\"drop_lrschedule.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8b6LEyp37Fxb"},"source":["#### 30 epochs, with LRScheduler=ReduceLROnPlateau"]},{"cell_type":"code","metadata":{"id":"QfhDv48pkcLv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606956704901,"user_tz":300,"elapsed":15199841,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"43bae514-cda7-4832-a129-c36d153ab99e"},"source":["drop_ratio_pool = [0, 0.1, 0.25, 0.3, 0.5]\n","dropout_ratio = []\n","train_loss = []\n","valid_loss = []\n","valid_loss_last_epoch = []\n","valid_acc = []\n","dur = []\n","total_time = []\n","callbacks=[\n","        ('print', Monitor()), ('lr_scheduler',\n","                     LRScheduler(policy=ReduceLROnPlateau, \n","                     monitor = \"train_loss\"\n","                     ))\n","    ]\n","\n","for dr in drop_ratio_pool:\n","\n","  drop_ratio=dr\n","\n","  network = VGG_net().to(device)\n","  torch.manual_seed(0)\n","  cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=30,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n","  )\n","  print(f\"fitting with drop ratio of {dr}\")\n","\n","  startall = time.time()\n","  cnn.fit(dataset, y=y_data)\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_loss_last_epoch.append(cnn.history[-1, 'valid_loss'])\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  dropout_ratio.append(dr)\n","  total_time.append(timeall)\n","    \n","  print(f\"drop ratio: {drop_ratio}\")\n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"valid loss at last epoch: {cnn.history[-1, 'valid_loss']}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fitting with drop ratio of 0\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4528\u001b[0m       \u001b[32m0.5633\u001b[0m        \u001b[35m1.1595\u001b[0m  101.2906\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9276\u001b[0m       \u001b[32m0.7458\u001b[0m        \u001b[35m0.8048\u001b[0m  101.1053\n","learning rate = 0.0001\n","      3        \u001b[36m0.6488\u001b[0m       \u001b[32m0.8469\u001b[0m        \u001b[35m0.5315\u001b[0m  101.2327\n","learning rate = 0.0001\n","      4        \u001b[36m0.4869\u001b[0m       0.8272        0.5945  101.3634\n","learning rate = 0.0001\n","      5        \u001b[36m0.3697\u001b[0m       \u001b[32m0.8876\u001b[0m        \u001b[35m0.4027\u001b[0m  101.2822\n","learning rate = 0.0001\n","      6        \u001b[36m0.2904\u001b[0m       \u001b[32m0.8896\u001b[0m        0.4086  101.2766\n","learning rate = 0.0001\n","      7        \u001b[36m0.2327\u001b[0m       \u001b[32m0.8972\u001b[0m        \u001b[35m0.3866\u001b[0m  101.3423\n","learning rate = 0.0001\n","      8        \u001b[36m0.1945\u001b[0m       \u001b[32m0.9189\u001b[0m        \u001b[35m0.3069\u001b[0m  101.2877\n","learning rate = 0.0001\n","      9        \u001b[36m0.1649\u001b[0m       \u001b[32m0.9197\u001b[0m        \u001b[35m0.3066\u001b[0m  101.1229\n","learning rate = 0.0001\n","     10        \u001b[36m0.1410\u001b[0m       \u001b[32m0.9227\u001b[0m        0.3074  101.2238\n","learning rate = 0.0001\n","     11        \u001b[36m0.1180\u001b[0m       \u001b[32m0.9315\u001b[0m        \u001b[35m0.2838\u001b[0m  101.3312\n","learning rate = 1e-05\n","     12        \u001b[36m0.0556\u001b[0m       \u001b[32m0.9541\u001b[0m        \u001b[35m0.1888\u001b[0m  101.2071\n","learning rate = 1e-05\n","     13        \u001b[36m0.0306\u001b[0m       \u001b[32m0.9560\u001b[0m        0.1940  101.2678\n","learning rate = 1e-05\n","     14        \u001b[36m0.0209\u001b[0m       \u001b[32m0.9589\u001b[0m        0.1958  101.3030\n","learning rate = 1e-05\n","     15        \u001b[36m0.0148\u001b[0m       \u001b[32m0.9598\u001b[0m        0.2109  101.0161\n","learning rate = 1e-05\n","     16        \u001b[36m0.0105\u001b[0m       \u001b[32m0.9608\u001b[0m        0.2209  101.4153\n","learning rate = 1e-05\n","     17        \u001b[36m0.0084\u001b[0m       \u001b[32m0.9610\u001b[0m        0.2262  101.0773\n","learning rate = 1e-05\n","     18        \u001b[36m0.0064\u001b[0m       0.9609        0.2392  101.1948\n","learning rate = 1e-05\n","     19        \u001b[36m0.0052\u001b[0m       \u001b[32m0.9611\u001b[0m        0.2402  101.0683\n","learning rate = 1e-05\n","     20        \u001b[36m0.0036\u001b[0m       \u001b[32m0.9616\u001b[0m        0.2592  100.8814\n","learning rate = 1e-05\n","     21        \u001b[36m0.0030\u001b[0m       0.9609        0.2857  100.8479\n","learning rate = 1e-05\n","     22        0.0035       0.9580        0.3018  100.9846\n","learning rate = 1.0000000000000002e-06\n","     23        \u001b[36m0.0024\u001b[0m       \u001b[32m0.9619\u001b[0m        0.2697  100.9621\n","learning rate = 1.0000000000000002e-06\n","     24        \u001b[36m0.0011\u001b[0m       \u001b[32m0.9624\u001b[0m        0.2703  100.8974\n","learning rate = 1.0000000000000002e-06\n","     25        \u001b[36m0.0008\u001b[0m       \u001b[32m0.9626\u001b[0m        0.2705  101.0294\n","learning rate = 1.0000000000000002e-06\n","     26        \u001b[36m0.0006\u001b[0m       \u001b[32m0.9627\u001b[0m        0.2725  100.9057\n","learning rate = 1.0000000000000002e-06\n","     27        \u001b[36m0.0004\u001b[0m       0.9624        0.2760  100.8651\n","learning rate = 1.0000000000000002e-06\n","     28        \u001b[36m0.0003\u001b[0m       0.9627        0.2791  101.0202\n","learning rate = 1.0000000000000002e-06\n","     29        \u001b[36m0.0003\u001b[0m       \u001b[32m0.9634\u001b[0m        0.2829  101.2068\n","learning rate = 1.0000000000000002e-06\n","     30        \u001b[36m0.0002\u001b[0m       \u001b[32m0.9634\u001b[0m        0.2858  101.1951\n","drop ratio: 0\n","lowest train loss: 0.00020684314065671413\n","lowest valid loss: 0.1887913434315543\n","valid loss at last epoch: 0.28575374625534483\n","max valid accuracy: 0.9634349491920706\n","time per epoch: 101.14014431635539\n","Total Time: 3039.55 s\n","fitting with drop ratio of 0.1\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4518\u001b[0m       \u001b[32m0.5834\u001b[0m        \u001b[35m1.1200\u001b[0m  101.3195\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9340\u001b[0m       \u001b[32m0.7495\u001b[0m        \u001b[35m0.7855\u001b[0m  101.2718\n","learning rate = 0.0001\n","      3        \u001b[36m0.6266\u001b[0m       \u001b[32m0.8468\u001b[0m        \u001b[35m0.5439\u001b[0m  101.2598\n","learning rate = 0.0001\n","      4        \u001b[36m0.4398\u001b[0m       \u001b[32m0.8859\u001b[0m        \u001b[35m0.4116\u001b[0m  101.5516\n","learning rate = 0.0001\n","      5        \u001b[36m0.3332\u001b[0m       0.8832        0.4332  101.4810\n","learning rate = 0.0001\n","      6        \u001b[36m0.2647\u001b[0m       \u001b[32m0.9107\u001b[0m        \u001b[35m0.3418\u001b[0m  101.3394\n","learning rate = 0.0001\n","      7        \u001b[36m0.2143\u001b[0m       \u001b[32m0.9117\u001b[0m        0.3551  101.4121\n","learning rate = 0.0001\n","      8        \u001b[36m0.1736\u001b[0m       \u001b[32m0.9249\u001b[0m        \u001b[35m0.3135\u001b[0m  101.5349\n","learning rate = 0.0001\n","      9        \u001b[36m0.1498\u001b[0m       \u001b[32m0.9250\u001b[0m        \u001b[35m0.2918\u001b[0m  101.2416\n","learning rate = 0.0001\n","     10        \u001b[36m0.1304\u001b[0m       0.9184        0.3463  101.2155\n","learning rate = 0.0001\n","     11        \u001b[36m0.1127\u001b[0m       \u001b[32m0.9327\u001b[0m        \u001b[35m0.2677\u001b[0m  101.3213\n","learning rate = 1e-05\n","     12        \u001b[36m0.0532\u001b[0m       \u001b[32m0.9564\u001b[0m        \u001b[35m0.1844\u001b[0m  101.3390\n","learning rate = 1e-05\n","     13        \u001b[36m0.0282\u001b[0m       \u001b[32m0.9594\u001b[0m        0.1900  101.2187\n","learning rate = 1e-05\n","     14        \u001b[36m0.0185\u001b[0m       0.9581        0.2082  101.3011\n","learning rate = 1e-05\n","     15        \u001b[36m0.0136\u001b[0m       0.9584        0.2175  101.2705\n","learning rate = 1e-05\n","     16        \u001b[36m0.0101\u001b[0m       \u001b[32m0.9601\u001b[0m        0.2305  101.2953\n","learning rate = 1e-05\n","     17        \u001b[36m0.0077\u001b[0m       \u001b[32m0.9609\u001b[0m        0.2367  101.3252\n","learning rate = 1e-05\n","     18        \u001b[36m0.0069\u001b[0m       0.9605        0.2568  101.1588\n","learning rate = 1e-05\n","     19        \u001b[36m0.0053\u001b[0m       0.9599        0.2693  101.1733\n","learning rate = 1e-05\n","     20        \u001b[36m0.0044\u001b[0m       \u001b[32m0.9621\u001b[0m        0.2549  101.3643\n","learning rate = 1e-05\n","     21        \u001b[36m0.0041\u001b[0m       0.9597        0.2729  100.9939\n","learning rate = 1e-05\n","     22        0.0042       0.9619        0.2655  101.1883\n","learning rate = 1.0000000000000002e-06\n","     23        \u001b[36m0.0022\u001b[0m       0.9614        0.2642  101.2639\n","learning rate = 1.0000000000000002e-06\n","     24        \u001b[36m0.0013\u001b[0m       \u001b[32m0.9622\u001b[0m        0.2650  101.2298\n","learning rate = 1.0000000000000002e-06\n","     25        \u001b[36m0.0011\u001b[0m       \u001b[32m0.9631\u001b[0m        0.2662  101.2375\n","learning rate = 1.0000000000000002e-06\n","     26        \u001b[36m0.0008\u001b[0m       0.9629        0.2687  101.1778\n","learning rate = 1.0000000000000002e-06\n","     27        \u001b[36m0.0007\u001b[0m       0.9629        0.2716  101.1707\n","learning rate = 1.0000000000000002e-06\n","     28        \u001b[36m0.0006\u001b[0m       0.9630        0.2771  101.0768\n","learning rate = 1.0000000000000002e-06\n","     29        \u001b[36m0.0006\u001b[0m       \u001b[32m0.9637\u001b[0m        0.2785  101.2512\n","learning rate = 1.0000000000000002e-06\n","     30        \u001b[36m0.0005\u001b[0m       0.9637        0.2830  101.2176\n","drop ratio: 0.1\n","lowest train loss: 0.0004659749094139995\n","lowest valid loss: 0.18440779417740188\n","valid loss at last epoch: 0.2830179816916607\n","max valid accuracy: 0.9636848242545394\n","time per epoch: 101.27340269883474\n","Total Time: 3043.41 s\n","fitting with drop ratio of 0.25\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4784\u001b[0m       \u001b[32m0.5417\u001b[0m        \u001b[35m1.1696\u001b[0m  101.1532\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9842\u001b[0m       \u001b[32m0.7385\u001b[0m        \u001b[35m0.8247\u001b[0m  101.3270\n","learning rate = 0.0001\n","      3        \u001b[36m0.6953\u001b[0m       \u001b[32m0.8193\u001b[0m        \u001b[35m0.5947\u001b[0m  101.2892\n","learning rate = 0.0001\n","      4        \u001b[36m0.5239\u001b[0m       0.8053        0.6824  101.1808\n","learning rate = 0.0001\n","      5        \u001b[36m0.4157\u001b[0m       \u001b[32m0.8736\u001b[0m        \u001b[35m0.4580\u001b[0m  101.2079\n","learning rate = 0.0001\n","      6        \u001b[36m0.3232\u001b[0m       \u001b[32m0.8836\u001b[0m        \u001b[35m0.4423\u001b[0m  101.0847\n","learning rate = 0.0001\n","      7        \u001b[36m0.2644\u001b[0m       \u001b[32m0.9142\u001b[0m        \u001b[35m0.3705\u001b[0m  101.0568\n","learning rate = 0.0001\n","      8        \u001b[36m0.2193\u001b[0m       \u001b[32m0.9191\u001b[0m        \u001b[35m0.3408\u001b[0m  101.2194\n","learning rate = 0.0001\n","      9        \u001b[36m0.1862\u001b[0m       \u001b[32m0.9254\u001b[0m        \u001b[35m0.3066\u001b[0m  101.1153\n","learning rate = 0.0001\n","     10        \u001b[36m0.1646\u001b[0m       0.8791        0.5717  101.1016\n","learning rate = 0.0001\n","     11        \u001b[36m0.1439\u001b[0m       \u001b[32m0.9339\u001b[0m        0.3161  101.0101\n","learning rate = 1e-05\n","     12        \u001b[36m0.0649\u001b[0m       \u001b[32m0.9575\u001b[0m        \u001b[35m0.1978\u001b[0m  101.0634\n","learning rate = 1e-05\n","     13        \u001b[36m0.0391\u001b[0m       \u001b[32m0.9599\u001b[0m        0.2054  100.9993\n","learning rate = 1e-05\n","     14        \u001b[36m0.0285\u001b[0m       \u001b[32m0.9611\u001b[0m        0.2072  100.9570\n","learning rate = 1e-05\n","     15        \u001b[36m0.0210\u001b[0m       \u001b[32m0.9635\u001b[0m        0.2069  101.0620\n","learning rate = 1e-05\n","     16        \u001b[36m0.0170\u001b[0m       0.9629        0.2192  101.0622\n","learning rate = 1e-05\n","     17        \u001b[36m0.0131\u001b[0m       0.9617        0.2355  101.0790\n","learning rate = 1e-05\n","     18        \u001b[36m0.0112\u001b[0m       0.9609        0.2590  100.8240\n","learning rate = 1e-05\n","     19        \u001b[36m0.0090\u001b[0m       0.9634        0.2469  100.8837\n","learning rate = 1e-05\n","     20        \u001b[36m0.0078\u001b[0m       0.9626        0.2515  100.9247\n","learning rate = 1e-05\n","     21        0.0078       0.9630        0.2632  100.8761\n","learning rate = 1e-05\n","     22        \u001b[36m0.0063\u001b[0m       0.9626        0.2658  100.9939\n","learning rate = 1.0000000000000002e-06\n","     23        \u001b[36m0.0056\u001b[0m       \u001b[32m0.9664\u001b[0m        0.2421  101.0871\n","learning rate = 1.0000000000000002e-06\n","     24        \u001b[36m0.0034\u001b[0m       \u001b[32m0.9667\u001b[0m        0.2431  101.0091\n","learning rate = 1.0000000000000002e-06\n","     25        \u001b[36m0.0029\u001b[0m       \u001b[32m0.9674\u001b[0m        0.2444  100.8166\n","learning rate = 1.0000000000000002e-06\n","     26        \u001b[36m0.0024\u001b[0m       0.9667        0.2486  100.8391\n","learning rate = 1.0000000000000002e-06\n","     27        \u001b[36m0.0021\u001b[0m       0.9667        0.2524  101.1750\n","learning rate = 1.0000000000000002e-06\n","     28        \u001b[36m0.0019\u001b[0m       0.9661        0.2546  100.9266\n","learning rate = 1.0000000000000002e-06\n","     29        \u001b[36m0.0017\u001b[0m       0.9667        0.2577  101.1353\n","learning rate = 1.0000000000000002e-06\n","     30        0.0017       0.9669        0.2617  100.9990\n","drop ratio: 0.25\n","lowest train loss: 0.0016652479238777498\n","lowest valid loss: 0.1978080511112358\n","valid loss at last epoch: 0.26170264754189526\n","max valid accuracy: 0.9674329501915708\n","time per epoch: 101.04864262739817\n","Total Time: 3036.71 s\n","fitting with drop ratio of 0.3\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.4844\u001b[0m       \u001b[32m0.4829\u001b[0m        \u001b[35m1.2772\u001b[0m  101.1235\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9873\u001b[0m       \u001b[32m0.6867\u001b[0m        \u001b[35m0.8996\u001b[0m  101.4035\n","learning rate = 0.0001\n","      3        \u001b[36m0.7004\u001b[0m       \u001b[32m0.8080\u001b[0m        \u001b[35m0.6521\u001b[0m  101.3901\n","learning rate = 0.0001\n","      4        \u001b[36m0.5365\u001b[0m       \u001b[32m0.8416\u001b[0m        \u001b[35m0.5438\u001b[0m  101.3745\n","learning rate = 0.0001\n","      5        \u001b[36m0.3980\u001b[0m       \u001b[32m0.8872\u001b[0m        \u001b[35m0.3949\u001b[0m  100.9016\n","learning rate = 0.0001\n","      6        \u001b[36m0.3204\u001b[0m       \u001b[32m0.9045\u001b[0m        \u001b[35m0.3610\u001b[0m  101.0656\n","learning rate = 0.0001\n","      7        \u001b[36m0.2656\u001b[0m       0.8964        0.3904  101.3173\n","learning rate = 0.0001\n","      8        \u001b[36m0.2195\u001b[0m       \u001b[32m0.9143\u001b[0m        0.3631  101.2095\n","learning rate = 0.0001\n","      9        \u001b[36m0.1864\u001b[0m       \u001b[32m0.9262\u001b[0m        \u001b[35m0.3035\u001b[0m  101.2237\n","learning rate = 0.0001\n","     10        \u001b[36m0.1633\u001b[0m       \u001b[32m0.9339\u001b[0m        \u001b[35m0.3013\u001b[0m  101.0287\n","learning rate = 0.0001\n","     11        \u001b[36m0.1373\u001b[0m       \u001b[32m0.9369\u001b[0m        \u001b[35m0.2812\u001b[0m  101.0769\n","learning rate = 1e-05\n","     12        \u001b[36m0.0631\u001b[0m       \u001b[32m0.9592\u001b[0m        \u001b[35m0.1939\u001b[0m  101.0894\n","learning rate = 1e-05\n","     13        \u001b[36m0.0358\u001b[0m       \u001b[32m0.9599\u001b[0m        0.2030  101.1367\n","learning rate = 1e-05\n","     14        \u001b[36m0.0256\u001b[0m       \u001b[32m0.9625\u001b[0m        0.2099  101.0924\n","learning rate = 1e-05\n","     15        \u001b[36m0.0206\u001b[0m       \u001b[32m0.9637\u001b[0m        0.2125  101.1875\n","learning rate = 1e-05\n","     16        \u001b[36m0.0156\u001b[0m       \u001b[32m0.9644\u001b[0m        0.2136  101.1308\n","learning rate = 1e-05\n","     17        \u001b[36m0.0138\u001b[0m       0.9636        0.2250  100.9531\n","learning rate = 1e-05\n","     18        \u001b[36m0.0109\u001b[0m       \u001b[32m0.9645\u001b[0m        0.2287  100.8340\n","learning rate = 1e-05\n","     19        \u001b[36m0.0093\u001b[0m       0.9634        0.2473  100.9558\n","learning rate = 1e-05\n","     20        \u001b[36m0.0082\u001b[0m       \u001b[32m0.9661\u001b[0m        0.2211  100.8521\n","learning rate = 1e-05\n","     21        \u001b[36m0.0063\u001b[0m       0.9657        0.2615  101.1372\n","learning rate = 1e-05\n","     22        0.0065       \u001b[32m0.9673\u001b[0m        0.2420  101.0764\n","learning rate = 1.0000000000000002e-06\n","     23        \u001b[36m0.0047\u001b[0m       \u001b[32m0.9682\u001b[0m        0.2392  100.9852\n","learning rate = 1.0000000000000002e-06\n","     24        \u001b[36m0.0037\u001b[0m       0.9682        0.2395  100.9947\n","learning rate = 1.0000000000000002e-06\n","     25        \u001b[36m0.0033\u001b[0m       0.9680        0.2418  101.0474\n","learning rate = 1.0000000000000002e-06\n","     26        \u001b[36m0.0028\u001b[0m       0.9681        0.2422  101.1191\n","learning rate = 1.0000000000000002e-06\n","     27        \u001b[36m0.0025\u001b[0m       \u001b[32m0.9685\u001b[0m        0.2467  101.0358\n","learning rate = 1.0000000000000002e-06\n","     28        \u001b[36m0.0024\u001b[0m       0.9682        0.2474  101.0323\n","learning rate = 1.0000000000000002e-06\n","     29        \u001b[36m0.0023\u001b[0m       0.9684        0.2502  100.9288\n","learning rate = 1.0000000000000002e-06\n","     30        \u001b[36m0.0022\u001b[0m       \u001b[32m0.9687\u001b[0m        0.2509  101.1694\n","drop ratio: 0.3\n","lowest train loss: 0.0021644323983211\n","lowest valid loss: 0.1939292974969738\n","valid loss at last epoch: 0.2508816008964257\n","max valid accuracy: 0.9686823255039148\n","time per epoch: 101.0957579612732\n","Total Time: 3037.96 s\n","fitting with drop ratio of 0.5\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss       dur\n","-------  ------------  -----------  ------------  --------\n","      1        \u001b[36m1.5259\u001b[0m       \u001b[32m0.4565\u001b[0m        \u001b[35m1.4312\u001b[0m  100.9894\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m1.0690\u001b[0m       \u001b[32m0.7096\u001b[0m        \u001b[35m0.9000\u001b[0m  100.9896\n","learning rate = 0.0001\n","      3        \u001b[36m0.8011\u001b[0m       \u001b[32m0.7929\u001b[0m        \u001b[35m0.6894\u001b[0m  101.0838\n","learning rate = 0.0001\n","      4        \u001b[36m0.6055\u001b[0m       \u001b[32m0.8357\u001b[0m        \u001b[35m0.5832\u001b[0m  101.0192\n","learning rate = 0.0001\n","      5        \u001b[36m0.4680\u001b[0m       \u001b[32m0.8733\u001b[0m        \u001b[35m0.4692\u001b[0m  101.0709\n","learning rate = 0.0001\n","      6        \u001b[36m0.3789\u001b[0m       0.8676        0.4770  101.1070\n","learning rate = 0.0001\n","      7        \u001b[36m0.3130\u001b[0m       \u001b[32m0.9014\u001b[0m        \u001b[35m0.3958\u001b[0m  101.1268\n","learning rate = 0.0001\n","      8        \u001b[36m0.2614\u001b[0m       \u001b[32m0.9133\u001b[0m        \u001b[35m0.3693\u001b[0m  101.3817\n","learning rate = 0.0001\n","      9        \u001b[36m0.2250\u001b[0m       \u001b[32m0.9290\u001b[0m        \u001b[35m0.3239\u001b[0m  101.2204\n","learning rate = 0.0001\n","     10        \u001b[36m0.1975\u001b[0m       0.9235        \u001b[35m0.3065\u001b[0m  101.3621\n","learning rate = 0.0001\n","     11        \u001b[36m0.1712\u001b[0m       0.9188        0.3474  101.2321\n","learning rate = 1e-05\n","     12        \u001b[36m0.0807\u001b[0m       \u001b[32m0.9543\u001b[0m        \u001b[35m0.2198\u001b[0m  101.4277\n","learning rate = 1e-05\n","     13        \u001b[36m0.0510\u001b[0m       \u001b[32m0.9559\u001b[0m        \u001b[35m0.2159\u001b[0m  101.3168\n","learning rate = 1e-05\n","     14        \u001b[36m0.0354\u001b[0m       0.9555        0.2442  101.0293\n","learning rate = 1e-05\n","     15        \u001b[36m0.0285\u001b[0m       \u001b[32m0.9593\u001b[0m        0.2282  101.4915\n","learning rate = 1e-05\n","     16        \u001b[36m0.0238\u001b[0m       0.9584        0.2412  101.6010\n","learning rate = 1e-05\n","     17        \u001b[36m0.0199\u001b[0m       0.9588        0.2563  101.1484\n","learning rate = 1e-05\n","     18        \u001b[36m0.0165\u001b[0m       \u001b[32m0.9618\u001b[0m        0.2440  101.0231\n","learning rate = 1e-05\n","     19        \u001b[36m0.0132\u001b[0m       \u001b[32m0.9619\u001b[0m        0.2513  101.2181\n","learning rate = 1e-05\n","     20        0.0140       0.9603        0.2637  101.1561\n","learning rate = 1e-05\n","     21        \u001b[36m0.0112\u001b[0m       0.9609        0.2834  101.0298\n","learning rate = 1e-05\n","     22        \u001b[36m0.0109\u001b[0m       0.9618        0.2769  101.2356\n","learning rate = 1.0000000000000002e-06\n","     23        \u001b[36m0.0075\u001b[0m       \u001b[32m0.9635\u001b[0m        0.2590  100.8805\n","learning rate = 1.0000000000000002e-06\n","     24        \u001b[36m0.0055\u001b[0m       \u001b[32m0.9642\u001b[0m        0.2618  100.9972\n","learning rate = 1.0000000000000002e-06\n","     25        \u001b[36m0.0050\u001b[0m       0.9638        0.2701  101.0792\n","learning rate = 1.0000000000000002e-06\n","     26        \u001b[36m0.0047\u001b[0m       0.9640        0.2702  101.1584\n","learning rate = 1.0000000000000002e-06\n","     27        0.0049       0.9629        0.2759  100.9255\n","learning rate = 1.0000000000000002e-06\n","     28        \u001b[36m0.0042\u001b[0m       0.9639        0.2749  101.1337\n","learning rate = 1.0000000000000002e-06\n","     29        \u001b[36m0.0040\u001b[0m       0.9637        0.2796  101.0836\n","learning rate = 1.0000000000000002e-06\n","     30        0.0042       0.9636        0.2837  100.9701\n","drop ratio: 0.5\n","lowest train loss: 0.003958561299559369\n","lowest valid loss: 0.21591790797209656\n","valid loss at last epoch: 0.28365220802843505\n","max valid accuracy: 0.9641845743794769\n","time per epoch: 101.1496144692103\n","Total Time: 3039.79 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tlh1ExTYUEqf"},"source":["\n","rawdata= {'drop_ratio': dropout_ratio, 'train_loss': train_loss, \n","          'valid_loss_min': valid_loss, 'last_epoch_val_loss': valid_loss_last_epoch, 'valid_acc': valid_acc, 'time/epoch': dur, \n","          'total_time': total_time}\n","drop_lrschedule_30e = pd.DataFrame(rawdata, columns = ['drop_ratio','train_loss',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc','time/epoch',\n","                                            'total_time'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"GX0RCeECTn05","executionInfo":{"status":"ok","timestamp":1606957538502,"user_tz":300,"elapsed":584,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"5a5d37ba-938e-4028-da39-1788cd7128bd"},"source":["drop_lrschedule_30e"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>drop_ratio</th>\n","      <th>train_loss</th>\n","      <th>valid_loss_min</th>\n","      <th>last_epoch_val_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","      <th>total_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.000207</td>\n","      <td>0.188791</td>\n","      <td>0.285754</td>\n","      <td>0.963435</td>\n","      <td>101.140144</td>\n","      <td>3039.548100</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.10</td>\n","      <td>0.000466</td>\n","      <td>0.184408</td>\n","      <td>0.283018</td>\n","      <td>0.963685</td>\n","      <td>101.273403</td>\n","      <td>3043.409904</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.25</td>\n","      <td>0.001665</td>\n","      <td>0.197808</td>\n","      <td>0.261703</td>\n","      <td>0.967433</td>\n","      <td>101.048643</td>\n","      <td>3036.710514</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.30</td>\n","      <td>0.002164</td>\n","      <td>0.193929</td>\n","      <td>0.250882</td>\n","      <td>0.968682</td>\n","      <td>101.095758</td>\n","      <td>3037.955635</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>0.003959</td>\n","      <td>0.215918</td>\n","      <td>0.283652</td>\n","      <td>0.964185</td>\n","      <td>101.149614</td>\n","      <td>3039.794898</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   drop_ratio  train_loss  valid_loss_min  ...  valid_acc  time/epoch   total_time\n","0        0.00    0.000207        0.188791  ...   0.963435  101.140144  3039.548100\n","1        0.10    0.000466        0.184408  ...   0.963685  101.273403  3043.409904\n","2        0.25    0.001665        0.197808  ...   0.967433  101.048643  3036.710514\n","3        0.30    0.002164        0.193929  ...   0.968682  101.095758  3037.955635\n","4        0.50    0.003959        0.215918  ...   0.964185  101.149614  3039.794898\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"jhLmXgqLcGye"},"source":["drop_lrschedule_30e.to_csv(\"drop_lrschedule_30e.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAQh-9VPzT8T"},"source":["### cross-validate"]},{"cell_type":"code","metadata":{"id":"5DoEPFhezYF5"},"source":["# Define callbacks to print results at end of each fold\n","\n","from skorch.callbacks import LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","from skorch.callbacks import Callback\n","\n","class Monitor(Callback):\n","    \n","\n","    def on_train_begin(self, net, **kwargs):\n","      print('************cross validate start****')\n","\n","    def on_train_end(self, net, **kwargs):\n","\n","      print(f\"drop ratio: {drop_ratio}\")\n","      print(f\"lowest train loss: {np.min(net.history[:, 'train_loss'])}\")\n","      print(f\"lowest valid loss: {np.min(net.history[:, 'valid_loss'])}\")\n","      print(f\"valid loss at last epoch: {net.history[-1, 'valid_loss']}\")\n","      print(f\"max valid accuracy: {np.max(net.history[:, 'valid_acc'])}\")\n","      print(f\"time per epoch: {np.average(net.history[:, 'dur'])}\")\n","\n","callbacks=[\n","        ('print', Monitor()), ('lr_scheduler',\n","                     LRScheduler(policy=ReduceLROnPlateau, \n","                     monitor = \"train_loss\",\n","                     ))\n","]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZL6pwytLyRZJ"},"source":["#### drop ratio = 0"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4_j9LPy0s3s","executionInfo":{"status":"ok","timestamp":1607018124903,"user_tz":300,"elapsed":8337474,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"286d98e6-2bb9-4e37-b29c-1fe767e0e26b"},"source":["from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","\n","from skorch.helper import SliceDataset\n","\n","from sklearn.metrics import log_loss, make_scorer\n","from sklearn.metrics import accuracy_score\n","\n","drop_ratio=0\n","network = VGG_net().to(device)\n","torch.manual_seed(0)\n","cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=20,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n",")\n","\n","data_train_sliceable = SliceDataset(dataset)\n","\n","startall = time.time()\n","scores = cross_validate(cnn, X=data_train_sliceable, y=y_data, cv=5, return_train_score=True)\n","endall = time.time()\n","timeall = endall-startall\n","\n","print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5685\u001b[0m       \u001b[32m0.4629\u001b[0m        \u001b[35m1.3432\u001b[0m  81.5616\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0581\u001b[0m       \u001b[32m0.7411\u001b[0m        \u001b[35m0.8290\u001b[0m  81.6673\n","      3        \u001b[36m0.7512\u001b[0m       \u001b[32m0.7713\u001b[0m        \u001b[35m0.7657\u001b[0m  81.7188\n","      4        \u001b[36m0.5693\u001b[0m       \u001b[32m0.8462\u001b[0m        \u001b[35m0.5364\u001b[0m  81.6070\n","      5        \u001b[36m0.4479\u001b[0m       \u001b[32m0.8712\u001b[0m        \u001b[35m0.4639\u001b[0m  81.6852\n","      6        \u001b[36m0.3500\u001b[0m       \u001b[32m0.8884\u001b[0m        \u001b[35m0.4246\u001b[0m  81.6097\n","      7        \u001b[36m0.2756\u001b[0m       \u001b[32m0.8924\u001b[0m        \u001b[35m0.4189\u001b[0m  81.6162\n","      8        \u001b[36m0.2271\u001b[0m       0.8903        0.4390  81.7184\n","      9        \u001b[36m0.1891\u001b[0m       \u001b[32m0.9093\u001b[0m        \u001b[35m0.3836\u001b[0m  81.6373\n","     10        \u001b[36m0.1611\u001b[0m       \u001b[32m0.9160\u001b[0m        \u001b[35m0.3506\u001b[0m  81.6850\n","     11        \u001b[36m0.1376\u001b[0m       0.9130        0.3921  81.4933\n","     12        \u001b[36m0.0677\u001b[0m       \u001b[32m0.9453\u001b[0m        \u001b[35m0.2409\u001b[0m  81.6527\n","     13        \u001b[36m0.0382\u001b[0m       \u001b[32m0.9457\u001b[0m        0.2562  81.5761\n","     14        \u001b[36m0.0268\u001b[0m       \u001b[32m0.9482\u001b[0m        0.2679  81.9101\n","     15        \u001b[36m0.0197\u001b[0m       \u001b[32m0.9485\u001b[0m        0.2791  81.6682\n","     16        \u001b[36m0.0160\u001b[0m       \u001b[32m0.9486\u001b[0m        0.2927  81.5742\n","     17        \u001b[36m0.0117\u001b[0m       0.9470        0.3122  81.4142\n","     18        \u001b[36m0.0098\u001b[0m       0.9463        0.3199  81.5226\n","     19        \u001b[36m0.0075\u001b[0m       \u001b[32m0.9493\u001b[0m        0.3302  81.5525\n","     20        \u001b[36m0.0062\u001b[0m       0.9472        0.3374  81.7776\n","drop ratio: 0\n","lowest train loss: 0.006197690331396532\n","lowest valid loss: 0.24092346353034116\n","valid loss at last epoch: 0.3374183915768757\n","max valid accuracy: 0.9492866812454441\n","time per epoch: 81.63240404129029\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4896\u001b[0m       \u001b[32m0.3669\u001b[0m        \u001b[35m1.7181\u001b[0m  81.7150\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0182\u001b[0m       \u001b[32m0.7234\u001b[0m        \u001b[35m0.8310\u001b[0m  81.4649\n","      3        \u001b[36m0.7168\u001b[0m       \u001b[32m0.8236\u001b[0m        \u001b[35m0.6145\u001b[0m  81.5624\n","      4        \u001b[36m0.5197\u001b[0m       0.8126        0.6329  81.4811\n","      5        \u001b[36m0.3854\u001b[0m       \u001b[32m0.8723\u001b[0m        \u001b[35m0.4491\u001b[0m  81.6546\n","      6        \u001b[36m0.3001\u001b[0m       0.8620        0.4879  81.9779\n","      7        \u001b[36m0.2452\u001b[0m       \u001b[32m0.8855\u001b[0m        \u001b[35m0.4090\u001b[0m  81.9032\n","      8        \u001b[36m0.2021\u001b[0m       \u001b[32m0.8991\u001b[0m        \u001b[35m0.3766\u001b[0m  81.8230\n","      9        \u001b[36m0.1764\u001b[0m       \u001b[32m0.9079\u001b[0m        \u001b[35m0.3757\u001b[0m  81.7780\n","     10        \u001b[36m0.1449\u001b[0m       \u001b[32m0.9140\u001b[0m        \u001b[35m0.3563\u001b[0m  81.7033\n","     11        \u001b[36m0.1303\u001b[0m       \u001b[32m0.9180\u001b[0m        \u001b[35m0.3373\u001b[0m  81.7544\n","     12        \u001b[36m0.0629\u001b[0m       \u001b[32m0.9452\u001b[0m        \u001b[35m0.2385\u001b[0m  81.8589\n","     13        \u001b[36m0.0346\u001b[0m       \u001b[32m0.9454\u001b[0m        0.2493  81.9296\n","     14        \u001b[36m0.0225\u001b[0m       \u001b[32m0.9479\u001b[0m        0.2655  81.6008\n","     15        \u001b[36m0.0164\u001b[0m       \u001b[32m0.9483\u001b[0m        0.2805  81.6419\n","     16        \u001b[36m0.0139\u001b[0m       0.9481        0.2929  81.8524\n","     17        \u001b[36m0.0093\u001b[0m       \u001b[32m0.9491\u001b[0m        0.3056  81.5743\n","     18        \u001b[36m0.0070\u001b[0m       0.9455        0.3307  81.5898\n","     19        \u001b[36m0.0066\u001b[0m       0.9442        0.3414  81.8908\n","     20        \u001b[36m0.0048\u001b[0m       0.9462        0.3482  81.4769\n","drop ratio: 0\n","lowest train loss: 0.004792624205760176\n","lowest valid loss: 0.23846989824871825\n","valid loss at last epoch: 0.34818137915630337\n","max valid accuracy: 0.9490784129959388\n","time per epoch: 81.71164293289185\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4951\u001b[0m       \u001b[32m0.4586\u001b[0m        \u001b[35m1.3545\u001b[0m  81.5104\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0289\u001b[0m       \u001b[32m0.7549\u001b[0m        \u001b[35m0.8044\u001b[0m  81.7238\n","      3        \u001b[36m0.7418\u001b[0m       \u001b[32m0.8173\u001b[0m        \u001b[35m0.6245\u001b[0m  81.9250\n","      4        \u001b[36m0.5224\u001b[0m       \u001b[32m0.8479\u001b[0m        \u001b[35m0.5413\u001b[0m  81.4738\n","      5        \u001b[36m0.3836\u001b[0m       \u001b[32m0.8755\u001b[0m        \u001b[35m0.4522\u001b[0m  81.5860\n","      6        \u001b[36m0.3030\u001b[0m       \u001b[32m0.8887\u001b[0m        \u001b[35m0.4136\u001b[0m  81.5012\n","      7        \u001b[36m0.2540\u001b[0m       0.8873        0.4179  81.6844\n","      8        \u001b[36m0.2098\u001b[0m       0.8864        0.4468  81.6414\n","      9        \u001b[36m0.1727\u001b[0m       \u001b[32m0.9061\u001b[0m        \u001b[35m0.3807\u001b[0m  81.4779\n","     10        \u001b[36m0.1496\u001b[0m       0.9054        0.3949  81.3019\n","     11        \u001b[36m0.1267\u001b[0m       0.8969        0.4488  81.2718\n","     12        \u001b[36m0.0603\u001b[0m       \u001b[32m0.9445\u001b[0m        \u001b[35m0.2422\u001b[0m  81.3293\n","     13        \u001b[36m0.0324\u001b[0m       \u001b[32m0.9473\u001b[0m        0.2476  81.2587\n","     14        \u001b[36m0.0224\u001b[0m       \u001b[32m0.9481\u001b[0m        0.2626  81.1739\n","     15        \u001b[36m0.0159\u001b[0m       \u001b[32m0.9491\u001b[0m        0.2815  81.3944\n","     16        \u001b[36m0.0127\u001b[0m       0.9489        0.2961  81.4294\n","     17        \u001b[36m0.0093\u001b[0m       0.9465        0.3149  81.3238\n","     18        \u001b[36m0.0087\u001b[0m       0.9486        0.3136  81.2196\n","     19        \u001b[36m0.0063\u001b[0m       0.9491        0.3162  81.2497\n","     20        \u001b[36m0.0041\u001b[0m       0.9444        0.3728  81.1272\n","drop ratio: 0\n","lowest train loss: 0.004068007882466797\n","lowest valid loss: 0.24223020164743259\n","valid loss at last epoch: 0.37281715414798844\n","max valid accuracy: 0.9490784129959388\n","time per epoch: 81.43018236160279\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4854\u001b[0m       \u001b[32m0.5490\u001b[0m        \u001b[35m1.1814\u001b[0m  81.1834\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0158\u001b[0m       \u001b[32m0.7219\u001b[0m        \u001b[35m0.8502\u001b[0m  81.4348\n","      3        \u001b[36m0.7256\u001b[0m       \u001b[32m0.8054\u001b[0m        \u001b[35m0.6582\u001b[0m  81.3757\n","      4        \u001b[36m0.5539\u001b[0m       \u001b[32m0.8385\u001b[0m        \u001b[35m0.5747\u001b[0m  81.2483\n","      5        \u001b[36m0.4559\u001b[0m       \u001b[32m0.8456\u001b[0m        \u001b[35m0.5632\u001b[0m  81.3479\n","      6        \u001b[36m0.3687\u001b[0m       \u001b[32m0.8654\u001b[0m        \u001b[35m0.4954\u001b[0m  81.3313\n","      7        \u001b[36m0.3080\u001b[0m       \u001b[32m0.8896\u001b[0m        \u001b[35m0.4108\u001b[0m  81.1546\n","      8        \u001b[36m0.2501\u001b[0m       \u001b[32m0.9060\u001b[0m        \u001b[35m0.3784\u001b[0m  81.3047\n","      9        \u001b[36m0.2093\u001b[0m       0.9017        0.4102  81.2406\n","     10        \u001b[36m0.1805\u001b[0m       \u001b[32m0.9128\u001b[0m        \u001b[35m0.3501\u001b[0m  81.4326\n","     11        \u001b[36m0.1521\u001b[0m       \u001b[32m0.9206\u001b[0m        \u001b[35m0.3201\u001b[0m  81.3409\n","     12        \u001b[36m0.0745\u001b[0m       \u001b[32m0.9414\u001b[0m        \u001b[35m0.2537\u001b[0m  81.4417\n","     13        \u001b[36m0.0456\u001b[0m       \u001b[32m0.9446\u001b[0m        0.2635  81.3387\n","     14        \u001b[36m0.0322\u001b[0m       \u001b[32m0.9461\u001b[0m        0.2709  81.3505\n","     15        \u001b[36m0.0247\u001b[0m       \u001b[32m0.9471\u001b[0m        0.2789  81.2597\n","     16        \u001b[36m0.0196\u001b[0m       0.9471        0.2961  81.2079\n","     17        \u001b[36m0.0159\u001b[0m       0.9464        0.2981  81.2333\n","     18        \u001b[36m0.0129\u001b[0m       0.9471        0.3170  81.1928\n","     19        \u001b[36m0.0118\u001b[0m       \u001b[32m0.9492\u001b[0m        0.3106  81.1009\n","     20        \u001b[36m0.0084\u001b[0m       0.9461        0.3457  81.0929\n","drop ratio: 0\n","lowest train loss: 0.008387556109992146\n","lowest valid loss: 0.2536571135775099\n","valid loss at last epoch: 0.3457220596536257\n","max valid accuracy: 0.9491825471206915\n","time per epoch: 81.28064823150635\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4952\u001b[0m       \u001b[32m0.2795\u001b[0m        \u001b[35m2.1100\u001b[0m  81.2321\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0184\u001b[0m       \u001b[32m0.6322\u001b[0m        \u001b[35m1.0313\u001b[0m  81.2355\n","      3        \u001b[36m0.7178\u001b[0m       \u001b[32m0.6470\u001b[0m        \u001b[35m1.0188\u001b[0m  81.2904\n","      4        \u001b[36m0.5533\u001b[0m       \u001b[32m0.8258\u001b[0m        \u001b[35m0.5909\u001b[0m  81.4916\n","      5        \u001b[36m0.4486\u001b[0m       0.8157        0.6209  81.2747\n","      6        \u001b[36m0.3541\u001b[0m       \u001b[32m0.8605\u001b[0m        \u001b[35m0.5175\u001b[0m  81.3050\n","      7        \u001b[36m0.2861\u001b[0m       0.8472        0.5267  81.1632\n","      8        \u001b[36m0.2348\u001b[0m       \u001b[32m0.8648\u001b[0m        \u001b[35m0.4684\u001b[0m  81.2097\n","      9        \u001b[36m0.1975\u001b[0m       \u001b[32m0.8787\u001b[0m        \u001b[35m0.4519\u001b[0m  81.1717\n","     10        \u001b[36m0.1736\u001b[0m       \u001b[32m0.8875\u001b[0m        \u001b[35m0.4248\u001b[0m  81.1806\n","     11        \u001b[36m0.1389\u001b[0m       \u001b[32m0.8947\u001b[0m        0.4572  81.3191\n","     12        \u001b[36m0.0728\u001b[0m       \u001b[32m0.9435\u001b[0m        \u001b[35m0.2479\u001b[0m  81.3754\n","     13        \u001b[36m0.0412\u001b[0m       \u001b[32m0.9461\u001b[0m        0.2511  81.4869\n","     14        \u001b[36m0.0291\u001b[0m       0.9461        0.2672  81.4376\n","     15        \u001b[36m0.0213\u001b[0m       0.9456        0.2762  81.3093\n","     16        \u001b[36m0.0167\u001b[0m       \u001b[32m0.9465\u001b[0m        0.2893  81.5236\n","     17        \u001b[36m0.0114\u001b[0m       \u001b[32m0.9482\u001b[0m        0.3045  81.3492\n","     18        \u001b[36m0.0087\u001b[0m       0.9453        0.3368  81.1947\n","     19        0.0090       0.9472        0.3231  81.1910\n","     20        \u001b[36m0.0049\u001b[0m       0.9472        0.3526  81.3805\n","drop ratio: 0\n","lowest train loss: 0.0049442962750163135\n","lowest valid loss: 0.24786561856426603\n","valid loss at last epoch: 0.3526332484311275\n","max valid accuracy: 0.9482453399979173\n","time per epoch: 81.30608751773835\n","Total Time: 8336.40 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iO81TISycLHo"},"source":["dropout = [0, 0.1, 0.25, 0.3, 0.5]\n","train_loss = []\n","train_acc = []\n","valid_loss = []\n","valid_loss_last_epoch = []\n","valid_acc = []\n","dur = []\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJvQeehWxFS5","executionInfo":{"status":"ok","timestamp":1607018505169,"user_tz":300,"elapsed":775,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"56a62326-19b7-4d85-de60-c3d8c877f736"},"source":["scores"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'fit_time': array([1634.80283785, 1636.36144304, 1630.69019079, 1627.62816143,\n","        1628.13712597]),\n"," 'score_time': array([7.3954885 , 7.40283465, 7.34829974, 7.32025337, 7.33438683]),\n"," 'test_score': array([0.94852574, 0.94593018, 0.94556973, 0.94423606, 0.95223806]),\n"," 'train_score': array([0.98731091, 0.98702002, 0.98460513, 0.98648001, 0.98752161])}"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"77l8Oozn1taf"},"source":["dr_0_train_loss = [0.006198, 0.004793, 0.004068, 0.008388, 0.004944]\n","dr_0_train_acc = scores['train_score']\n","dr_0_valid_loss = [0.2409, 0.2385, 0.2422, 0.2537, 0.2479]\n","dr_0_valid_loss_last_epoch = [0.3374, 0.3482, 0.3728, 0.3457, 0.3526]\n","dr_0_valid_acc = scores['test_score']\n","dr_0_dur = [81.63, 81.71, 81.43, 81.28, 81.31]\n","\n","train_loss.append(np.average(dr_0_train_loss))\n","train_acc.append(np.average(dr_0_train_acc))\n","valid_loss.append(np.average(dr_0_valid_loss))\n","valid_loss_last_epoch.append(np.average(dr_0_valid_loss_last_epoch))\n","valid_acc.append(np.average(dr_0_valid_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GuD1M_EZdqcB"},"source":["rawdata= {'train_loss': dr_0_train_loss, 'train_acc': dr_0_train_acc,\n","          'valid_loss_min': dr_0_valid_loss, 'last_epoch_val_loss': dr_0_valid_loss_last_epoch, \n","          'valid_acc': dr_0_valid_acc, 'time/epoch': dr_0_dur, \n","          }\n","drop_000 = pd.DataFrame(rawdata, columns = ['train_loss', 'train_acc',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc','time/epoch',\n","                                            ])\n","drop_000.to_csv(\"drop_000.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"FKU3oN2meI70","executionInfo":{"status":"ok","timestamp":1607018756874,"user_tz":300,"elapsed":1123,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"3365bdc4-c560-46bd-a89a-17df0c5dce84"},"source":["drop_000"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss_min</th>\n","      <th>last_epoch_val_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.006198</td>\n","      <td>0.987311</td>\n","      <td>0.2409</td>\n","      <td>0.3374</td>\n","      <td>0.948526</td>\n","      <td>81.63</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.004793</td>\n","      <td>0.987020</td>\n","      <td>0.2385</td>\n","      <td>0.3482</td>\n","      <td>0.945930</td>\n","      <td>81.71</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.004068</td>\n","      <td>0.984605</td>\n","      <td>0.2422</td>\n","      <td>0.3728</td>\n","      <td>0.945570</td>\n","      <td>81.43</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.008388</td>\n","      <td>0.986480</td>\n","      <td>0.2537</td>\n","      <td>0.3457</td>\n","      <td>0.944236</td>\n","      <td>81.28</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.004944</td>\n","      <td>0.987522</td>\n","      <td>0.2479</td>\n","      <td>0.3526</td>\n","      <td>0.952238</td>\n","      <td>81.31</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_loss  train_acc  ...  valid_acc  time/epoch\n","0    0.006198   0.987311  ...   0.948526       81.63\n","1    0.004793   0.987020  ...   0.945930       81.71\n","2    0.004068   0.984605  ...   0.945570       81.43\n","3    0.008388   0.986480  ...   0.944236       81.28\n","4    0.004944   0.987522  ...   0.952238       81.31\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"PmBPuKqLydJZ"},"source":["#### drop ratio = 0.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTbJvdGzeJG7","executionInfo":{"status":"ok","timestamp":1607027226181,"user_tz":300,"elapsed":8345160,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"2cf79f51-d34b-462c-8bbe-99fb4be5ff37"},"source":["from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","\n","from skorch.helper import SliceDataset\n","\n","from sklearn.metrics import log_loss, make_scorer\n","from sklearn.metrics import accuracy_score\n","\n","drop_ratio=0.1\n","network = VGG_net().to(device)\n","torch.manual_seed(0)\n","cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=20,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n",")\n","\n","data_train_sliceable = SliceDataset(dataset)\n","\n","startall = time.time()\n","scores = cross_validate(cnn, X=data_train_sliceable, y=y_data, cv=5, return_train_score=True)\n","endall = time.time()\n","timeall = endall-startall\n","\n","print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5144\u001b[0m       \u001b[32m0.4428\u001b[0m        \u001b[35m1.4603\u001b[0m  81.8647\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0659\u001b[0m       \u001b[32m0.7201\u001b[0m        \u001b[35m0.8953\u001b[0m  81.8166\n","      3        \u001b[36m0.7620\u001b[0m       \u001b[32m0.7961\u001b[0m        \u001b[35m0.6931\u001b[0m  81.7108\n","      4        \u001b[36m0.5867\u001b[0m       \u001b[32m0.8220\u001b[0m        \u001b[35m0.6276\u001b[0m  81.6303\n","      5        \u001b[36m0.4644\u001b[0m       \u001b[32m0.8595\u001b[0m        \u001b[35m0.5109\u001b[0m  81.7399\n","      6        \u001b[36m0.3674\u001b[0m       \u001b[32m0.8612\u001b[0m        0.5243  81.6388\n","      7        \u001b[36m0.3023\u001b[0m       0.8477        0.5911  81.5891\n","      8        \u001b[36m0.2483\u001b[0m       \u001b[32m0.9038\u001b[0m        \u001b[35m0.4094\u001b[0m  81.6610\n","      9        \u001b[36m0.2105\u001b[0m       0.8894        0.4249  81.8749\n","     10        \u001b[36m0.1786\u001b[0m       \u001b[32m0.9140\u001b[0m        \u001b[35m0.3436\u001b[0m  81.5699\n","     11        \u001b[36m0.1505\u001b[0m       0.9000        0.4147  81.5879\n","     12        \u001b[36m0.0760\u001b[0m       \u001b[32m0.9407\u001b[0m        \u001b[35m0.2688\u001b[0m  81.5716\n","     13        \u001b[36m0.0447\u001b[0m       \u001b[32m0.9449\u001b[0m        0.2707  81.6742\n","     14        \u001b[36m0.0305\u001b[0m       0.9443        0.2929  81.5626\n","     15        \u001b[36m0.0234\u001b[0m       \u001b[32m0.9464\u001b[0m        0.3087  81.6097\n","     16        \u001b[36m0.0191\u001b[0m       \u001b[32m0.9474\u001b[0m        0.3076  81.4210\n","     17        \u001b[36m0.0160\u001b[0m       0.9470        0.3257  81.5968\n","     18        \u001b[36m0.0109\u001b[0m       0.9471        0.3411  81.5027\n","     19        0.0111       0.9463        0.3510  81.6001\n","     20        \u001b[36m0.0089\u001b[0m       \u001b[32m0.9479\u001b[0m        0.3486  81.4344\n","drop ratio: 0.1\n","lowest train loss: 0.008860975921717378\n","lowest valid loss: 0.26876660382464734\n","valid loss at last epoch: 0.34861348872197406\n","max valid accuracy: 0.9479329376236593\n","time per epoch: 81.63284393548966\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5099\u001b[0m       \u001b[32m0.4104\u001b[0m        \u001b[35m1.4439\u001b[0m  81.5060\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0556\u001b[0m       \u001b[32m0.6674\u001b[0m        \u001b[35m0.9803\u001b[0m  81.6962\n","      3        \u001b[36m0.7649\u001b[0m       \u001b[32m0.7579\u001b[0m        \u001b[35m0.7930\u001b[0m  81.5676\n","      4        \u001b[36m0.5827\u001b[0m       \u001b[32m0.8252\u001b[0m        \u001b[35m0.6461\u001b[0m  81.6358\n","      5        \u001b[36m0.4775\u001b[0m       \u001b[32m0.8620\u001b[0m        \u001b[35m0.5123\u001b[0m  81.5473\n","      6        \u001b[36m0.3920\u001b[0m       \u001b[32m0.8630\u001b[0m        0.5126  81.7234\n","      7        \u001b[36m0.3356\u001b[0m       \u001b[32m0.8670\u001b[0m        \u001b[35m0.5054\u001b[0m  81.5696\n","      8        \u001b[36m0.2898\u001b[0m       0.8545        0.5361  81.8480\n","      9        \u001b[36m0.2407\u001b[0m       0.8581        0.6132  81.5933\n","     10        \u001b[36m0.2177\u001b[0m       \u001b[32m0.9034\u001b[0m        \u001b[35m0.4283\u001b[0m  81.5821\n","     11        \u001b[36m0.1820\u001b[0m       0.8805        0.5351  81.7189\n","     12        \u001b[36m0.0914\u001b[0m       \u001b[32m0.9362\u001b[0m        \u001b[35m0.2800\u001b[0m  81.6408\n","     13        \u001b[36m0.0596\u001b[0m       \u001b[32m0.9402\u001b[0m        \u001b[35m0.2799\u001b[0m  81.6465\n","     14        \u001b[36m0.0440\u001b[0m       \u001b[32m0.9415\u001b[0m        0.2963  81.8899\n","     15        \u001b[36m0.0339\u001b[0m       0.9392        0.3151  81.8979\n","     16        \u001b[36m0.0280\u001b[0m       \u001b[32m0.9424\u001b[0m        0.3227  81.5736\n","     17        \u001b[36m0.0247\u001b[0m       0.9415        0.3410  81.4757\n","     18        \u001b[36m0.0188\u001b[0m       \u001b[32m0.9441\u001b[0m        0.3397  81.4939\n","     19        \u001b[36m0.0164\u001b[0m       0.9437        0.3476  81.7373\n","     20        \u001b[36m0.0138\u001b[0m       0.9431        0.3422  81.4547\n","drop ratio: 0.1\n","lowest train loss: 0.013788946949927033\n","lowest valid loss: 0.27988815564731634\n","valid loss at last epoch: 0.3421541775827445\n","max valid accuracy: 0.94407997500781\n","time per epoch: 81.63991953134537\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5113\u001b[0m       \u001b[32m0.3389\u001b[0m        \u001b[35m1.6397\u001b[0m  81.5181\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0481\u001b[0m       \u001b[32m0.6848\u001b[0m        \u001b[35m0.9098\u001b[0m  81.8274\n","      3        \u001b[36m0.7618\u001b[0m       \u001b[32m0.8093\u001b[0m        \u001b[35m0.6470\u001b[0m  81.6555\n","      4        \u001b[36m0.5875\u001b[0m       0.7953        0.7043  81.6622\n","      5        \u001b[36m0.4703\u001b[0m       \u001b[32m0.8573\u001b[0m        \u001b[35m0.5032\u001b[0m  81.4772\n","      6        \u001b[36m0.3903\u001b[0m       \u001b[32m0.8663\u001b[0m        \u001b[35m0.4785\u001b[0m  81.6996\n","      7        \u001b[36m0.3136\u001b[0m       \u001b[32m0.8818\u001b[0m        \u001b[35m0.4655\u001b[0m  81.7227\n","      8        \u001b[36m0.2621\u001b[0m       \u001b[32m0.8860\u001b[0m        \u001b[35m0.4423\u001b[0m  81.4390\n","      9        \u001b[36m0.2170\u001b[0m       0.8836        \u001b[35m0.4385\u001b[0m  81.4011\n","     10        \u001b[36m0.1853\u001b[0m       \u001b[32m0.9056\u001b[0m        \u001b[35m0.3892\u001b[0m  81.6434\n","     11        \u001b[36m0.1629\u001b[0m       \u001b[32m0.9194\u001b[0m        \u001b[35m0.3643\u001b[0m  81.7105\n","     12        \u001b[36m0.0805\u001b[0m       \u001b[32m0.9435\u001b[0m        \u001b[35m0.2498\u001b[0m  81.4036\n","     13        \u001b[36m0.0474\u001b[0m       \u001b[32m0.9463\u001b[0m        0.2565  81.4141\n","     14        \u001b[36m0.0331\u001b[0m       \u001b[32m0.9466\u001b[0m        0.2648  81.7697\n","     15        \u001b[36m0.0260\u001b[0m       0.9452        0.2915  81.4464\n","     16        \u001b[36m0.0205\u001b[0m       \u001b[32m0.9487\u001b[0m        0.2874  81.6709\n","     17        \u001b[36m0.0172\u001b[0m       0.9475        0.3079  81.4078\n","     18        \u001b[36m0.0151\u001b[0m       \u001b[32m0.9491\u001b[0m        0.3038  81.4261\n","     19        \u001b[36m0.0109\u001b[0m       0.9488        0.3253  81.4323\n","     20        \u001b[36m0.0082\u001b[0m       0.9487        0.3385  81.2650\n","drop ratio: 0.1\n","lowest train loss: 0.008248520056483055\n","lowest valid loss: 0.24983759255985646\n","valid loss at last epoch: 0.33847581194679355\n","max valid accuracy: 0.9490784129959388\n","time per epoch: 81.54963507652283\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5086\u001b[0m       \u001b[32m0.4216\u001b[0m        \u001b[35m1.4241\u001b[0m  81.1980\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0390\u001b[0m       \u001b[32m0.7031\u001b[0m        \u001b[35m0.8873\u001b[0m  81.6813\n","      3        \u001b[36m0.7428\u001b[0m       \u001b[32m0.8014\u001b[0m        \u001b[35m0.6654\u001b[0m  81.7195\n","      4        \u001b[36m0.5668\u001b[0m       \u001b[32m0.8221\u001b[0m        \u001b[35m0.5993\u001b[0m  81.4415\n","      5        \u001b[36m0.4594\u001b[0m       \u001b[32m0.8617\u001b[0m        \u001b[35m0.4942\u001b[0m  81.8124\n","      6        \u001b[36m0.3697\u001b[0m       0.8611        0.5301  81.8868\n","      7        \u001b[36m0.3048\u001b[0m       \u001b[32m0.8781\u001b[0m        \u001b[35m0.4701\u001b[0m  81.4073\n","      8        \u001b[36m0.2485\u001b[0m       \u001b[32m0.8992\u001b[0m        \u001b[35m0.4210\u001b[0m  81.5038\n","      9        \u001b[36m0.2143\u001b[0m       0.8956        0.4288  81.7826\n","     10        \u001b[36m0.1819\u001b[0m       \u001b[32m0.9139\u001b[0m        \u001b[35m0.3703\u001b[0m  81.6349\n","     11        \u001b[36m0.1519\u001b[0m       \u001b[32m0.9180\u001b[0m        0.3727  81.4896\n","     12        \u001b[36m0.0776\u001b[0m       \u001b[32m0.9437\u001b[0m        \u001b[35m0.2588\u001b[0m  81.5103\n","     13        \u001b[36m0.0464\u001b[0m       \u001b[32m0.9486\u001b[0m        \u001b[35m0.2548\u001b[0m  81.5493\n","     14        \u001b[36m0.0322\u001b[0m       0.9472        0.2693  81.4126\n","     15        \u001b[36m0.0250\u001b[0m       0.9477        0.2841  81.3095\n","     16        \u001b[36m0.0206\u001b[0m       \u001b[32m0.9491\u001b[0m        0.2877  81.2770\n","     17        \u001b[36m0.0168\u001b[0m       0.9478        0.2966  81.3019\n","     18        \u001b[36m0.0132\u001b[0m       \u001b[32m0.9511\u001b[0m        0.2949  81.3276\n","     19        \u001b[36m0.0103\u001b[0m       0.9491        0.3202  81.2739\n","     20        0.0103       0.9497        0.3253  81.3560\n","drop ratio: 0.1\n","lowest train loss: 0.010275486622867902\n","lowest valid loss: 0.2548116624243629\n","valid loss at last epoch: 0.32532167822736335\n","max valid accuracy: 0.9510569613662397\n","time per epoch: 81.49379789829254\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5212\u001b[0m       \u001b[32m0.3522\u001b[0m        \u001b[35m1.6655\u001b[0m  81.6135\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0579\u001b[0m       \u001b[32m0.6116\u001b[0m        \u001b[35m1.0212\u001b[0m  81.5248\n","      3        \u001b[36m0.7589\u001b[0m       \u001b[32m0.7152\u001b[0m        \u001b[35m0.8576\u001b[0m  81.5956\n","      4        \u001b[36m0.5845\u001b[0m       \u001b[32m0.7815\u001b[0m        \u001b[35m0.6819\u001b[0m  81.7072\n","      5        \u001b[36m0.4745\u001b[0m       \u001b[32m0.8333\u001b[0m        \u001b[35m0.5771\u001b[0m  81.4080\n","      6        \u001b[36m0.3921\u001b[0m       \u001b[32m0.8736\u001b[0m        \u001b[35m0.4573\u001b[0m  81.2233\n","      7        \u001b[36m0.3204\u001b[0m       0.8416        0.5445  81.3913\n","      8        \u001b[36m0.2626\u001b[0m       0.8585        0.4758  81.4123\n","      9        \u001b[36m0.2213\u001b[0m       \u001b[32m0.8905\u001b[0m        \u001b[35m0.4056\u001b[0m  81.3337\n","     10        \u001b[36m0.1865\u001b[0m       \u001b[32m0.8937\u001b[0m        \u001b[35m0.4001\u001b[0m  81.4258\n","     11        \u001b[36m0.1611\u001b[0m       0.8365        0.6618  81.3838\n","     12        \u001b[36m0.0823\u001b[0m       \u001b[32m0.9393\u001b[0m        \u001b[35m0.2795\u001b[0m  81.3023\n","     13        \u001b[36m0.0475\u001b[0m       \u001b[32m0.9418\u001b[0m        0.2848  81.3919\n","     14        \u001b[36m0.0339\u001b[0m       \u001b[32m0.9426\u001b[0m        0.3009  81.3374\n","     15        \u001b[36m0.0242\u001b[0m       \u001b[32m0.9440\u001b[0m        0.3156  81.3603\n","     16        \u001b[36m0.0195\u001b[0m       \u001b[32m0.9441\u001b[0m        0.3319  81.1811\n","     17        \u001b[36m0.0166\u001b[0m       \u001b[32m0.9450\u001b[0m        0.3356  81.1435\n","     18        \u001b[36m0.0139\u001b[0m       0.9448        0.3405  81.1404\n","     19        \u001b[36m0.0101\u001b[0m       0.9437        0.3758  81.2690\n","     20        \u001b[36m0.0077\u001b[0m       0.9430        0.3977  81.2272\n","drop ratio: 0.1\n","lowest train loss: 0.007738359670183854\n","lowest valid loss: 0.2795273263948278\n","valid loss at last epoch: 0.39772212157806586\n","max valid accuracy: 0.9450171821305842\n","time per epoch: 81.36862494945527\n","Total Time: 8342.79 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ROpnVXSxyNCb"},"source":["dr_01_train_loss = [0.008861, 0.01379, 0.008249, 0.01028, 0.007738]\n","dr_01_train_acc = [0.987582, 0.985437, 0.987647, 0.987584, 0.985188]\n","dr_01_valid_loss = [0.2688, 0.2799, 0.2498, 0.2548, 0.2795]\n","dr_01_valid_loss_last_epoch = [0.3486, 0.3421, 0.3384, 0.3253, 0.3977]\n","dr_01_valid_acc = [0.946443, 0.947680, 0.950904, 0.948404, 0.945570]\n","dr_01_dur = [81.63, 81.63, 81.55, 81.49, 81.37]\n","\n","\n","rawdata= {'train_loss': dr_01_train_loss, 'train_acc': dr_01_train_acc,\n","          'valid_loss_min': dr_01_valid_loss, 'last_epoch_val_loss': dr_01_valid_loss_last_epoch, \n","          'valid_acc': dr_01_valid_acc, 'time/epoch': dr_01_dur, \n","          }\n","drop_010 = pd.DataFrame(rawdata, columns = ['train_loss', 'train_acc',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc','time/epoch',\n","                                            ])\n","drop_010.to_csv(\"drop_010.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"YxaTO7q_yq3P","executionInfo":{"status":"ok","timestamp":1607050814384,"user_tz":300,"elapsed":417,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"811ee744-b56a-4af4-8986-361d03fc5d59"},"source":["drop_010"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss_min</th>\n","      <th>last_epoch_val_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.008861</td>\n","      <td>0.987582</td>\n","      <td>0.2688</td>\n","      <td>0.3486</td>\n","      <td>0.946443</td>\n","      <td>81.63</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.013790</td>\n","      <td>0.985437</td>\n","      <td>0.2799</td>\n","      <td>0.3421</td>\n","      <td>0.947680</td>\n","      <td>81.63</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.008249</td>\n","      <td>0.987647</td>\n","      <td>0.2498</td>\n","      <td>0.3384</td>\n","      <td>0.950904</td>\n","      <td>81.55</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.010280</td>\n","      <td>0.987584</td>\n","      <td>0.2548</td>\n","      <td>0.3253</td>\n","      <td>0.948404</td>\n","      <td>81.49</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.007738</td>\n","      <td>0.985188</td>\n","      <td>0.2795</td>\n","      <td>0.3977</td>\n","      <td>0.945570</td>\n","      <td>81.37</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_loss  train_acc  ...  valid_acc  time/epoch\n","0    0.008861   0.987582  ...   0.946443       81.63\n","1    0.013790   0.985437  ...   0.947680       81.63\n","2    0.008249   0.987647  ...   0.950904       81.55\n","3    0.010280   0.987584  ...   0.948404       81.49\n","4    0.007738   0.985188  ...   0.945570       81.37\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"LOGovy1cUKx3"},"source":["#### drop ratio = 0.25"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNHKA7s9UOcU","executionInfo":{"status":"ok","timestamp":1607039230585,"user_tz":300,"elapsed":4825583,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"79182c77-2387-4b12-9cbc-084a60889405"},"source":["from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","\n","from skorch.helper import SliceDataset\n","\n","from sklearn.metrics import log_loss, make_scorer\n","from sklearn.metrics import accuracy_score\n","\n","drop_ratio=0.25\n","network = VGG_net().to(device)\n","torch.manual_seed(0)\n","cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=20,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n",")\n","\n","data_train_sliceable = SliceDataset(dataset)\n","\n","startall = time.time()\n","scores = cross_validate(cnn, X=data_train_sliceable, y=y_data, cv=5, return_train_score=True)\n","endall = time.time()\n","timeall = endall-startall\n","\n","print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5300\u001b[0m       \u001b[32m0.4743\u001b[0m        \u001b[35m1.3131\u001b[0m  47.4720\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1025\u001b[0m       \u001b[32m0.6754\u001b[0m        \u001b[35m1.0000\u001b[0m  47.1009\n","      3        \u001b[36m0.8262\u001b[0m       \u001b[32m0.7218\u001b[0m        \u001b[35m0.8638\u001b[0m  47.2532\n","      4        \u001b[36m0.6296\u001b[0m       \u001b[32m0.8245\u001b[0m        \u001b[35m0.6074\u001b[0m  47.2161\n","      5        \u001b[36m0.4778\u001b[0m       \u001b[32m0.8590\u001b[0m        \u001b[35m0.5011\u001b[0m  47.0415\n","      6        \u001b[36m0.3720\u001b[0m       \u001b[32m0.8868\u001b[0m        \u001b[35m0.4322\u001b[0m  47.2857\n","      7        \u001b[36m0.2997\u001b[0m       0.8813        0.4715  47.2882\n","      8        \u001b[36m0.2481\u001b[0m       \u001b[32m0.8965\u001b[0m        \u001b[35m0.4243\u001b[0m  47.2024\n","      9        \u001b[36m0.2105\u001b[0m       \u001b[32m0.8983\u001b[0m        0.4267  47.0652\n","     10        \u001b[36m0.1714\u001b[0m       \u001b[32m0.9039\u001b[0m        \u001b[35m0.3780\u001b[0m  47.2110\n","     11        \u001b[36m0.1571\u001b[0m       \u001b[32m0.9050\u001b[0m        0.3860  47.0633\n","     12        \u001b[36m0.0775\u001b[0m       \u001b[32m0.9420\u001b[0m        \u001b[35m0.2538\u001b[0m  47.0828\n","     13        \u001b[36m0.0453\u001b[0m       \u001b[32m0.9462\u001b[0m        0.2605  47.2106\n","     14        \u001b[36m0.0315\u001b[0m       0.9461        0.2842  47.0154\n","     15        \u001b[36m0.0246\u001b[0m       \u001b[32m0.9465\u001b[0m        0.2948  47.4172\n","     16        \u001b[36m0.0190\u001b[0m       0.9443        0.3192  46.9994\n","     17        \u001b[36m0.0156\u001b[0m       0.9465        0.3187  46.9780\n","     18        \u001b[36m0.0138\u001b[0m       0.9446        0.3509  47.0824\n","     19        \u001b[36m0.0104\u001b[0m       \u001b[32m0.9477\u001b[0m        0.3351  47.2346\n","     20        \u001b[36m0.0088\u001b[0m       \u001b[32m0.9494\u001b[0m        0.3452  47.1263\n","drop ratio: 0.25\n","lowest train loss: 0.008787081363308634\n","lowest valid loss: 0.25376870283698616\n","valid loss at last epoch: 0.3452381972430807\n","max valid accuracy: 0.9493908153701968\n","time per epoch: 47.16731672286987\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5329\u001b[0m       \u001b[32m0.4262\u001b[0m        \u001b[35m1.6068\u001b[0m  46.9920\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1017\u001b[0m       \u001b[32m0.6389\u001b[0m        \u001b[35m1.1189\u001b[0m  47.2566\n","      3        \u001b[36m0.8296\u001b[0m       \u001b[32m0.7525\u001b[0m        \u001b[35m0.7943\u001b[0m  47.2431\n","      4        \u001b[36m0.6398\u001b[0m       \u001b[32m0.7733\u001b[0m        \u001b[35m0.7758\u001b[0m  46.9937\n","      5        \u001b[36m0.5141\u001b[0m       \u001b[32m0.8330\u001b[0m        \u001b[35m0.6464\u001b[0m  47.2175\n","      6        \u001b[36m0.4329\u001b[0m       \u001b[32m0.8455\u001b[0m        \u001b[35m0.5825\u001b[0m  47.1400\n","      7        \u001b[36m0.3517\u001b[0m       \u001b[32m0.8805\u001b[0m        \u001b[35m0.4275\u001b[0m  47.0586\n","      8        \u001b[36m0.2990\u001b[0m       \u001b[32m0.8963\u001b[0m        \u001b[35m0.3954\u001b[0m  47.1420\n","      9        \u001b[36m0.2461\u001b[0m       0.8855        0.4606  47.1611\n","     10        \u001b[36m0.2080\u001b[0m       \u001b[32m0.9099\u001b[0m        \u001b[35m0.3832\u001b[0m  47.2232\n","     11        \u001b[36m0.1897\u001b[0m       0.8568        0.6207  47.1129\n","     12        \u001b[36m0.0941\u001b[0m       \u001b[32m0.9404\u001b[0m        \u001b[35m0.2516\u001b[0m  47.2122\n","     13        \u001b[36m0.0583\u001b[0m       \u001b[32m0.9448\u001b[0m        \u001b[35m0.2486\u001b[0m  47.2661\n","     14        \u001b[36m0.0425\u001b[0m       \u001b[32m0.9476\u001b[0m        0.2616  47.2999\n","     15        \u001b[36m0.0321\u001b[0m       0.9440        0.2907  47.4123\n","     16        \u001b[36m0.0266\u001b[0m       0.9472        0.2801  47.3457\n","     17        \u001b[36m0.0220\u001b[0m       0.9469        0.2977  47.3006\n","     18        \u001b[36m0.0185\u001b[0m       0.9464        0.3087  47.5027\n","     19        \u001b[36m0.0181\u001b[0m       0.9453        0.3183  47.2712\n","     20        \u001b[36m0.0137\u001b[0m       0.9468        0.3245  47.1878\n","drop ratio: 0.25\n","lowest train loss: 0.013727319884230304\n","lowest valid loss: 0.2486075405597214\n","valid loss at last epoch: 0.3245423700263433\n","max valid accuracy: 0.9476205352494013\n","time per epoch: 47.216965842247006\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5291\u001b[0m       \u001b[32m0.4640\u001b[0m        \u001b[35m1.3431\u001b[0m  46.9456\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0898\u001b[0m       \u001b[32m0.6989\u001b[0m        \u001b[35m0.9168\u001b[0m  47.1651\n","      3        \u001b[36m0.8144\u001b[0m       \u001b[32m0.7773\u001b[0m        \u001b[35m0.7299\u001b[0m  47.0459\n","      4        \u001b[36m0.6340\u001b[0m       0.7721        0.7379  47.3196\n","      5        \u001b[36m0.5021\u001b[0m       \u001b[32m0.8656\u001b[0m        \u001b[35m0.4952\u001b[0m  47.1112\n","      6        \u001b[36m0.4119\u001b[0m       \u001b[32m0.8670\u001b[0m        0.5032  47.3190\n","      7        \u001b[36m0.3399\u001b[0m       \u001b[32m0.8696\u001b[0m        0.5070  47.2679\n","      8        \u001b[36m0.2776\u001b[0m       \u001b[32m0.9021\u001b[0m        \u001b[35m0.3675\u001b[0m  47.3168\n","      9        \u001b[36m0.2334\u001b[0m       0.8947        0.4442  47.1300\n","     10        \u001b[36m0.2026\u001b[0m       \u001b[32m0.9034\u001b[0m        0.4461  47.0328\n","     11        \u001b[36m0.1702\u001b[0m       0.8976        0.4294  47.0406\n","     12        \u001b[36m0.0892\u001b[0m       \u001b[32m0.9411\u001b[0m        \u001b[35m0.2571\u001b[0m  47.1819\n","     13        \u001b[36m0.0542\u001b[0m       \u001b[32m0.9441\u001b[0m        0.2600  47.3146\n","     14        \u001b[36m0.0385\u001b[0m       \u001b[32m0.9445\u001b[0m        0.2683  46.9381\n","     15        \u001b[36m0.0307\u001b[0m       \u001b[32m0.9450\u001b[0m        0.2838  47.0976\n","     16        \u001b[36m0.0246\u001b[0m       \u001b[32m0.9488\u001b[0m        0.2776  47.2046\n","     17        \u001b[36m0.0215\u001b[0m       0.9480        0.2923  47.3771\n","     18        \u001b[36m0.0166\u001b[0m       0.9477        0.3027  47.0882\n","     19        \u001b[36m0.0155\u001b[0m       \u001b[32m0.9490\u001b[0m        0.3133  47.1797\n","     20        \u001b[36m0.0141\u001b[0m       \u001b[32m0.9496\u001b[0m        0.3130  47.5436\n","drop ratio: 0.25\n","lowest train loss: 0.01412909872344244\n","lowest valid loss: 0.25714976003383383\n","valid loss at last epoch: 0.31296582593210487\n","max valid accuracy: 0.9495990836197021\n","time per epoch: 47.18099893331528\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5215\u001b[0m       \u001b[32m0.5235\u001b[0m        \u001b[35m1.2077\u001b[0m  47.4267\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0725\u001b[0m       \u001b[32m0.6295\u001b[0m        \u001b[35m1.0376\u001b[0m  47.1781\n","      3        \u001b[36m0.7975\u001b[0m       \u001b[32m0.7320\u001b[0m        \u001b[35m0.8743\u001b[0m  47.0835\n","      4        \u001b[36m0.6045\u001b[0m       \u001b[32m0.7770\u001b[0m        \u001b[35m0.7324\u001b[0m  47.3707\n","      5        \u001b[36m0.4901\u001b[0m       \u001b[32m0.8381\u001b[0m        \u001b[35m0.6061\u001b[0m  46.9874\n","      6        \u001b[36m0.4098\u001b[0m       \u001b[32m0.8799\u001b[0m        \u001b[35m0.4784\u001b[0m  47.0194\n","      7        \u001b[36m0.3238\u001b[0m       \u001b[32m0.8878\u001b[0m        \u001b[35m0.4273\u001b[0m  46.8319\n","      8        \u001b[36m0.2773\u001b[0m       0.8820        0.4764  47.0918\n","      9        \u001b[36m0.2395\u001b[0m       \u001b[32m0.9056\u001b[0m        \u001b[35m0.3772\u001b[0m  47.1742\n","     10        \u001b[36m0.1974\u001b[0m       0.9035        0.3871  47.1643\n","     11        \u001b[36m0.1745\u001b[0m       0.9002        0.4432  47.0266\n","     12        \u001b[36m0.0842\u001b[0m       \u001b[32m0.9379\u001b[0m        \u001b[35m0.2849\u001b[0m  47.0578\n","     13        \u001b[36m0.0523\u001b[0m       \u001b[32m0.9422\u001b[0m        0.3002  47.1298\n","     14        \u001b[36m0.0390\u001b[0m       0.9416        0.3027  47.0823\n","     15        \u001b[36m0.0294\u001b[0m       \u001b[32m0.9448\u001b[0m        0.3097  47.1487\n","     16        \u001b[36m0.0246\u001b[0m       \u001b[32m0.9472\u001b[0m        0.3128  46.9763\n","     17        \u001b[36m0.0195\u001b[0m       \u001b[32m0.9478\u001b[0m        0.3207  47.2719\n","     18        \u001b[36m0.0166\u001b[0m       0.9467        0.3372  47.1279\n","     19        \u001b[36m0.0130\u001b[0m       0.9471        0.3449  47.0753\n","     20        \u001b[36m0.0118\u001b[0m       0.9454        0.3575  47.0917\n","drop ratio: 0.25\n","lowest train loss: 0.011807187919912395\n","lowest valid loss: 0.2848889130451591\n","valid loss at last epoch: 0.35745959178228703\n","max valid accuracy: 0.9478288034989066\n","time per epoch: 47.115806341171265\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5355\u001b[0m       \u001b[32m0.3806\u001b[0m        \u001b[35m1.5082\u001b[0m  46.9596\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1002\u001b[0m       \u001b[32m0.4580\u001b[0m        \u001b[35m1.4814\u001b[0m  47.2870\n","      3        \u001b[36m0.8204\u001b[0m       \u001b[32m0.6871\u001b[0m        \u001b[35m0.9513\u001b[0m  47.1470\n","      4        \u001b[36m0.6289\u001b[0m       \u001b[32m0.7968\u001b[0m        \u001b[35m0.6795\u001b[0m  47.0280\n","      5        \u001b[36m0.5052\u001b[0m       0.7765        0.7085  46.9922\n","      6        \u001b[36m0.4276\u001b[0m       \u001b[32m0.8570\u001b[0m        \u001b[35m0.5054\u001b[0m  46.9998\n","      7        \u001b[36m0.3565\u001b[0m       \u001b[32m0.8664\u001b[0m        \u001b[35m0.4953\u001b[0m  47.0357\n","      8        \u001b[36m0.2983\u001b[0m       \u001b[32m0.8744\u001b[0m        \u001b[35m0.4512\u001b[0m  47.0220\n","      9        \u001b[36m0.2529\u001b[0m       0.8379        0.5366  47.3507\n","     10        \u001b[36m0.2222\u001b[0m       0.8718        0.4932  47.0271\n","     11        \u001b[36m0.1957\u001b[0m       0.8721        0.4956  47.0388\n","     12        \u001b[36m0.0999\u001b[0m       \u001b[32m0.9370\u001b[0m        \u001b[35m0.2846\u001b[0m  46.8554\n","     13        \u001b[36m0.0653\u001b[0m       \u001b[32m0.9410\u001b[0m        0.2857  47.1332\n","     14        \u001b[36m0.0484\u001b[0m       \u001b[32m0.9425\u001b[0m        0.3007  46.9184\n","     15        \u001b[36m0.0394\u001b[0m       0.9410        0.3096  47.0459\n","     16        \u001b[36m0.0323\u001b[0m       0.9417        0.3371  47.0563\n","     17        \u001b[36m0.0262\u001b[0m       \u001b[32m0.9434\u001b[0m        0.3273  46.9992\n","     18        \u001b[36m0.0214\u001b[0m       0.9417        0.3605  47.0936\n","     19        \u001b[36m0.0186\u001b[0m       0.9428        0.3748  47.0907\n","     20        \u001b[36m0.0152\u001b[0m       \u001b[32m0.9435\u001b[0m        0.3740  47.0166\n","drop ratio: 0.25\n","lowest train loss: 0.01521305512412558\n","lowest valid loss: 0.28456854101998524\n","valid loss at last epoch: 0.3739577768390568\n","max valid accuracy: 0.9434551702592939\n","time per epoch: 47.05485717058182\n","Total Time: 4824.72 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7rBnBeSIUWIV"},"source":["dr_025_train_loss = [0.008787, 0.01373, 0.01413, 0.01181, 0.01521]\n","dr_025_train_acc = scores['train_score']\n","dr_025_valid_loss = [0.2537, 0.2486, 0.2571, 0.2849, 0.2846]\n","dr_025_valid_loss_last_epoch = [0.3452, 0.3245, 0.3129, 0.3575, 0.3739]\n","dr_025_valid_acc = scores['test_score']\n","dr_025_dur = [47.15, 47.21, 47.18, 47.12, 47.12]\n","\n","rawdata= {'train_loss': dr_025_train_loss, 'train_acc': dr_025_train_acc,\n","          'valid_loss_min': dr_025_valid_loss, 'last_epoch_val_loss': dr_025_valid_loss_last_epoch, \n","          'valid_acc': dr_025_valid_acc, 'time/epoch': dr_025_dur, \n","          }\n","drop_025 = pd.DataFrame(rawdata, columns = ['train_loss', 'train_acc',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc','time/epoch',\n","                                            ])\n","drop_025.to_csv(\"drop_025.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PC6ODy-UWw1"},"source":["drop_025.to_csv(\"drop_025.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaG4wnX7Uro0"},"source":["#### drop ratio = 0.3"]},{"cell_type":"code","metadata":{"id":"F98zsg5KUuFr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607045771205,"user_tz":300,"elapsed":4770106,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"757bb597-9f65-468a-d5e8-b40edb4d438e"},"source":["from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","\n","from skorch.helper import SliceDataset\n","\n","from sklearn.metrics import log_loss, make_scorer\n","from sklearn.metrics import accuracy_score\n","\n","drop_ratio=0.3\n","network = VGG_net().to(device)\n","torch.manual_seed(0)\n","cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=20,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n",")\n","\n","data_train_sliceable = SliceDataset(dataset)\n","\n","startall = time.time()\n","scores = cross_validate(cnn, X=data_train_sliceable, y=y_data, cv=5, return_train_score=True)\n","endall = time.time()\n","timeall = endall-startall\n","\n","print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5596\u001b[0m       \u001b[32m0.4416\u001b[0m        \u001b[35m1.3402\u001b[0m  46.8393\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1157\u001b[0m       \u001b[32m0.7274\u001b[0m        \u001b[35m0.8804\u001b[0m  46.8951\n","      3        \u001b[36m0.8396\u001b[0m       \u001b[32m0.7753\u001b[0m        \u001b[35m0.7267\u001b[0m  46.8190\n","      4        \u001b[36m0.6383\u001b[0m       \u001b[32m0.8296\u001b[0m        \u001b[35m0.6107\u001b[0m  47.1948\n","      5        \u001b[36m0.4998\u001b[0m       0.8267        \u001b[35m0.5964\u001b[0m  46.9201\n","      6        \u001b[36m0.4111\u001b[0m       \u001b[32m0.8587\u001b[0m        \u001b[35m0.5491\u001b[0m  46.9104\n","      7        \u001b[36m0.3461\u001b[0m       0.8268        0.6726  46.6802\n","      8        \u001b[36m0.2902\u001b[0m       \u001b[32m0.8927\u001b[0m        \u001b[35m0.4552\u001b[0m  46.7956\n","      9        \u001b[36m0.2532\u001b[0m       \u001b[32m0.8956\u001b[0m        \u001b[35m0.4412\u001b[0m  46.7865\n","     10        \u001b[36m0.2161\u001b[0m       \u001b[32m0.9000\u001b[0m        \u001b[35m0.4016\u001b[0m  46.9555\n","     11        \u001b[36m0.1978\u001b[0m       \u001b[32m0.9056\u001b[0m        0.4099  46.8993\n","     12        \u001b[36m0.0918\u001b[0m       \u001b[32m0.9374\u001b[0m        \u001b[35m0.2816\u001b[0m  46.7350\n","     13        \u001b[36m0.0586\u001b[0m       \u001b[32m0.9405\u001b[0m        0.2869  46.7382\n","     14        \u001b[36m0.0450\u001b[0m       \u001b[32m0.9421\u001b[0m        0.2972  46.6165\n","     15        \u001b[36m0.0363\u001b[0m       \u001b[32m0.9426\u001b[0m        0.3162  46.7551\n","     16        \u001b[36m0.0286\u001b[0m       \u001b[32m0.9434\u001b[0m        0.3310  46.6150\n","     17        \u001b[36m0.0263\u001b[0m       \u001b[32m0.9447\u001b[0m        0.3243  46.9618\n","     18        \u001b[36m0.0208\u001b[0m       \u001b[32m0.9466\u001b[0m        0.3228  46.7765\n","     19        \u001b[36m0.0184\u001b[0m       \u001b[32m0.9469\u001b[0m        0.3366  46.6505\n","     20        \u001b[36m0.0148\u001b[0m       0.9460        0.3363  46.7468\n","drop ratio: 0.3\n","lowest train loss: 0.01482181757365479\n","lowest valid loss: 0.2815704203052229\n","valid loss at last epoch: 0.33628309493942676\n","max valid accuracy: 0.9468915963761324\n","time per epoch: 46.81455523967743\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5426\u001b[0m       \u001b[32m0.4992\u001b[0m        \u001b[35m1.2973\u001b[0m  46.6611\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1001\u001b[0m       \u001b[32m0.6735\u001b[0m        \u001b[35m0.9585\u001b[0m  46.4790\n","      3        \u001b[36m0.8260\u001b[0m       \u001b[32m0.7683\u001b[0m        \u001b[35m0.7681\u001b[0m  46.6933\n","      4        \u001b[36m0.6332\u001b[0m       \u001b[32m0.8442\u001b[0m        \u001b[35m0.5596\u001b[0m  46.5607\n","      5        \u001b[36m0.5081\u001b[0m       0.8348        0.6301  46.5111\n","      6        \u001b[36m0.4224\u001b[0m       \u001b[32m0.8573\u001b[0m        \u001b[35m0.5585\u001b[0m  46.5421\n","      7        \u001b[36m0.3596\u001b[0m       \u001b[32m0.8611\u001b[0m        \u001b[35m0.5099\u001b[0m  46.6056\n","      8        \u001b[36m0.3030\u001b[0m       \u001b[32m0.8873\u001b[0m        \u001b[35m0.4200\u001b[0m  46.7070\n","      9        \u001b[36m0.2694\u001b[0m       \u001b[32m0.8939\u001b[0m        0.4361  46.7083\n","     10        \u001b[36m0.2362\u001b[0m       0.8930        0.4455  46.8049\n","     11        \u001b[36m0.2142\u001b[0m       0.8719        0.5177  46.7461\n","     12        \u001b[36m0.1069\u001b[0m       \u001b[32m0.9374\u001b[0m        \u001b[35m0.2776\u001b[0m  46.6285\n","     13        \u001b[36m0.0689\u001b[0m       \u001b[32m0.9427\u001b[0m        \u001b[35m0.2689\u001b[0m  46.6717\n","     14        \u001b[36m0.0533\u001b[0m       \u001b[32m0.9443\u001b[0m        0.2694  46.6716\n","     15        \u001b[36m0.0437\u001b[0m       0.9441        0.2856  46.7472\n","     16        \u001b[36m0.0375\u001b[0m       \u001b[32m0.9456\u001b[0m        0.2915  46.6027\n","     17        \u001b[36m0.0307\u001b[0m       \u001b[32m0.9464\u001b[0m        0.2949  46.6438\n","     18        \u001b[36m0.0277\u001b[0m       0.9462        0.3077  46.5609\n","     19        \u001b[36m0.0248\u001b[0m       \u001b[32m0.9471\u001b[0m        0.3104  46.6996\n","     20        \u001b[36m0.0201\u001b[0m       0.9449        0.3195  46.6219\n","drop ratio: 0.3\n","lowest train loss: 0.020100655817891167\n","lowest valid loss: 0.2688969382519557\n","valid loss at last epoch: 0.31953501657623146\n","max valid accuracy: 0.9470998646256378\n","time per epoch: 46.643348479270934\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5502\u001b[0m       \u001b[32m0.3381\u001b[0m        \u001b[35m1.6098\u001b[0m  46.6650\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1162\u001b[0m       \u001b[32m0.6619\u001b[0m        \u001b[35m1.0455\u001b[0m  46.7139\n","      3        \u001b[36m0.8488\u001b[0m       \u001b[32m0.7873\u001b[0m        \u001b[35m0.7026\u001b[0m  46.8373\n","      4        \u001b[36m0.6606\u001b[0m       \u001b[32m0.7978\u001b[0m        0.7871  46.4980\n","      5        \u001b[36m0.5273\u001b[0m       \u001b[32m0.8205\u001b[0m        \u001b[35m0.6529\u001b[0m  46.6386\n","      6        \u001b[36m0.4331\u001b[0m       \u001b[32m0.8514\u001b[0m        \u001b[35m0.5869\u001b[0m  46.3354\n","      7        \u001b[36m0.3616\u001b[0m       0.8479        \u001b[35m0.5747\u001b[0m  46.5692\n","      8        \u001b[36m0.3151\u001b[0m       \u001b[32m0.8634\u001b[0m        \u001b[35m0.5230\u001b[0m  46.5711\n","      9        \u001b[36m0.2617\u001b[0m       \u001b[32m0.9040\u001b[0m        \u001b[35m0.3851\u001b[0m  46.5749\n","     10        \u001b[36m0.2320\u001b[0m       0.9018        0.4069  46.7485\n","     11        \u001b[36m0.2057\u001b[0m       \u001b[32m0.9090\u001b[0m        0.4097  46.5027\n","     12        \u001b[36m0.0972\u001b[0m       \u001b[32m0.9360\u001b[0m        \u001b[35m0.2820\u001b[0m  46.6409\n","     13        \u001b[36m0.0624\u001b[0m       \u001b[32m0.9398\u001b[0m        \u001b[35m0.2778\u001b[0m  46.6174\n","     14        \u001b[36m0.0465\u001b[0m       \u001b[32m0.9406\u001b[0m        0.3002  46.7283\n","     15        \u001b[36m0.0385\u001b[0m       \u001b[32m0.9429\u001b[0m        0.2978  46.9068\n","     16        \u001b[36m0.0313\u001b[0m       \u001b[32m0.9449\u001b[0m        0.3013  46.7281\n","     17        \u001b[36m0.0267\u001b[0m       0.9444        0.3065  46.8730\n","     18        \u001b[36m0.0231\u001b[0m       0.9441        0.3256  46.5918\n","     19        \u001b[36m0.0216\u001b[0m       \u001b[32m0.9455\u001b[0m        0.3219  46.6292\n","     20        \u001b[36m0.0166\u001b[0m       0.9446        0.3281  46.6745\n","drop ratio: 0.3\n","lowest train loss: 0.016617110157785647\n","lowest valid loss: 0.2778448656455432\n","valid loss at last epoch: 0.32811460934574016\n","max valid accuracy: 0.9455378527543477\n","time per epoch: 46.65223177671432\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5331\u001b[0m       \u001b[32m0.3686\u001b[0m        \u001b[35m1.5694\u001b[0m  46.5348\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0822\u001b[0m       \u001b[32m0.6975\u001b[0m        \u001b[35m0.9063\u001b[0m  46.8437\n","      3        \u001b[36m0.8049\u001b[0m       \u001b[32m0.7453\u001b[0m        \u001b[35m0.8435\u001b[0m  46.9277\n","      4        \u001b[36m0.6124\u001b[0m       \u001b[32m0.8088\u001b[0m        \u001b[35m0.6624\u001b[0m  46.7503\n","      5        \u001b[36m0.4916\u001b[0m       \u001b[32m0.8284\u001b[0m        \u001b[35m0.6484\u001b[0m  46.5876\n","      6        \u001b[36m0.3973\u001b[0m       \u001b[32m0.8669\u001b[0m        \u001b[35m0.4986\u001b[0m  46.9963\n","      7        \u001b[36m0.3271\u001b[0m       \u001b[32m0.8761\u001b[0m        \u001b[35m0.4838\u001b[0m  47.0363\n","      8        \u001b[36m0.2678\u001b[0m       0.8715        0.5193  46.7414\n","      9        \u001b[36m0.2369\u001b[0m       \u001b[32m0.8901\u001b[0m        \u001b[35m0.4527\u001b[0m  46.8115\n","     10        \u001b[36m0.2038\u001b[0m       \u001b[32m0.9091\u001b[0m        \u001b[35m0.3791\u001b[0m  47.0883\n","     11        \u001b[36m0.1740\u001b[0m       \u001b[32m0.9189\u001b[0m        \u001b[35m0.3781\u001b[0m  46.7812\n","     12        \u001b[36m0.0827\u001b[0m       \u001b[32m0.9428\u001b[0m        \u001b[35m0.2720\u001b[0m  46.6687\n","     13        \u001b[36m0.0518\u001b[0m       \u001b[32m0.9445\u001b[0m        0.2804  46.4679\n","     14        \u001b[36m0.0379\u001b[0m       \u001b[32m0.9470\u001b[0m        0.2866  46.5259\n","     15        \u001b[36m0.0298\u001b[0m       0.9448        0.3106  46.4328\n","     16        \u001b[36m0.0265\u001b[0m       \u001b[32m0.9471\u001b[0m        0.2992  46.5466\n","     17        \u001b[36m0.0218\u001b[0m       \u001b[32m0.9485\u001b[0m        0.3099  46.5352\n","     18        \u001b[36m0.0186\u001b[0m       0.9467        0.3391  46.4932\n","     19        \u001b[36m0.0162\u001b[0m       \u001b[32m0.9503\u001b[0m        0.3171  46.6076\n","     20        \u001b[36m0.0141\u001b[0m       0.9487        0.3280  46.3458\n","drop ratio: 0.3\n","lowest train loss: 0.014097166550915669\n","lowest valid loss: 0.2719851644835326\n","valid loss at last epoch: 0.32801611201940944\n","max valid accuracy: 0.950328022492971\n","time per epoch: 46.68614259958267\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5371\u001b[0m       \u001b[32m0.3925\u001b[0m        \u001b[35m1.5348\u001b[0m  46.2552\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.0908\u001b[0m       \u001b[32m0.4604\u001b[0m        \u001b[35m1.4646\u001b[0m  46.6373\n","      3        \u001b[36m0.8209\u001b[0m       \u001b[32m0.7049\u001b[0m        \u001b[35m0.8457\u001b[0m  46.3120\n","      4        \u001b[36m0.6267\u001b[0m       \u001b[32m0.7310\u001b[0m        0.8662  46.2639\n","      5        \u001b[36m0.5058\u001b[0m       \u001b[32m0.8413\u001b[0m        \u001b[35m0.5550\u001b[0m  46.3881\n","      6        \u001b[36m0.4074\u001b[0m       \u001b[32m0.8683\u001b[0m        \u001b[35m0.5026\u001b[0m  46.4056\n","      7        \u001b[36m0.3387\u001b[0m       \u001b[32m0.8774\u001b[0m        \u001b[35m0.4450\u001b[0m  46.1395\n","      8        \u001b[36m0.2827\u001b[0m       \u001b[32m0.8907\u001b[0m        \u001b[35m0.4011\u001b[0m  46.2505\n","      9        \u001b[36m0.2396\u001b[0m       0.8100        0.7269  46.3270\n","     10        \u001b[36m0.2085\u001b[0m       0.7833        0.7811  46.2099\n","     11        \u001b[36m0.1840\u001b[0m       0.7991        0.8815  46.2547\n","     12        \u001b[36m0.0920\u001b[0m       \u001b[32m0.9437\u001b[0m        \u001b[35m0.2728\u001b[0m  46.3614\n","     13        \u001b[36m0.0564\u001b[0m       \u001b[32m0.9460\u001b[0m        \u001b[35m0.2719\u001b[0m  46.2678\n","     14        \u001b[36m0.0418\u001b[0m       0.9454        0.2989  46.1885\n","     15        \u001b[36m0.0301\u001b[0m       0.9450        0.3170  46.2872\n","     16        \u001b[36m0.0257\u001b[0m       \u001b[32m0.9474\u001b[0m        0.3116  46.4589\n","     17        \u001b[36m0.0200\u001b[0m       0.9452        0.3429  46.3005\n","     18        \u001b[36m0.0188\u001b[0m       0.9467        0.3485  46.2816\n","     19        \u001b[36m0.0156\u001b[0m       0.9457        0.3556  46.2851\n","     20        \u001b[36m0.0119\u001b[0m       \u001b[32m0.9476\u001b[0m        0.3723  46.2471\n","drop ratio: 0.3\n","lowest train loss: 0.011925911190028273\n","lowest valid loss: 0.2718966334031071\n","valid loss at last epoch: 0.3723021493135319\n","max valid accuracy: 0.9476205352494013\n","time per epoch: 46.30609656572342\n","Total Time: 4769.28 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4LvNeBbyU1gX"},"source":["dr_03_train_loss = [0.01482, 0.02010, 0.01662, 0.01409, 0.01193]\n","dr_03_train_acc = scores['train_score']\n","dr_03_valid_loss = [0.2816, 0.2688, 0.2778, 0.2720, 0.2719]\n","dr_03_valid_loss_last_epoch = [0.3363, 0.3195, 0.3281, 0.3280, 0.3723]\n","dr_03_valid_acc = scores['test_score']\n","dr_03_dur = [46.81, 46.64, 46.65, 46.69, 46.31]\n","\n","rawdata= {'train_loss': dr_03_train_loss, 'train_acc': dr_03_train_acc,\n","          'valid_loss_min': dr_03_valid_loss, 'last_epoch_val_loss': dr_03_valid_loss_last_epoch, \n","          'valid_acc': dr_03_valid_acc, 'time/epoch': dr_03_dur, \n","          }\n","drop_03 = pd.DataFrame(rawdata, columns = ['train_loss', 'train_acc',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc','time/epoch',\n","                                            ])\n","drop_03.to_csv(\"drop_030.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylDmgrzVVJ41","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1607045923864,"user_tz":300,"elapsed":440,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"db3bc0f8-65aa-488b-9a3b-e2d76391c056"},"source":["drop_03"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss_min</th>\n","      <th>last_epoch_val_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.01482</td>\n","      <td>0.985686</td>\n","      <td>0.2816</td>\n","      <td>0.3363</td>\n","      <td>0.947693</td>\n","      <td>46.81</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02010</td>\n","      <td>0.984853</td>\n","      <td>0.2688</td>\n","      <td>0.3195</td>\n","      <td>0.948263</td>\n","      <td>46.64</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.01662</td>\n","      <td>0.985105</td>\n","      <td>0.2778</td>\n","      <td>0.3281</td>\n","      <td>0.948070</td>\n","      <td>46.65</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.01409</td>\n","      <td>0.986313</td>\n","      <td>0.2720</td>\n","      <td>0.3280</td>\n","      <td>0.948404</td>\n","      <td>46.69</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.01193</td>\n","      <td>0.986126</td>\n","      <td>0.2719</td>\n","      <td>0.3723</td>\n","      <td>0.949404</td>\n","      <td>46.31</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_loss  train_acc  ...  valid_acc  time/epoch\n","0     0.01482   0.985686  ...   0.947693       46.81\n","1     0.02010   0.984853  ...   0.948263       46.64\n","2     0.01662   0.985105  ...   0.948070       46.65\n","3     0.01409   0.986313  ...   0.948404       46.69\n","4     0.01193   0.986126  ...   0.949404       46.31\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"eLTYfxgvVLl4"},"source":["#### drop ratio = 0.5"]},{"cell_type":"code","metadata":{"id":"1k1CkDnNU1rh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607050694150,"user_tz":300,"elapsed":4758362,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"c16da513-77e0-4405-a8ff-a06edcc29ab7"},"source":["from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","\n","from skorch.helper import SliceDataset\n","\n","from sklearn.metrics import log_loss, make_scorer\n","from sklearn.metrics import accuracy_score\n","\n","drop_ratio=0.5\n","network = VGG_net().to(device)\n","torch.manual_seed(0)\n","cnn = NeuralNetClassifier(\n","      \n","      network,\n","      max_epochs=20,\n","      lr=1e-4,\n","      optimizer=torch.optim.Adam,\n","      batch_size=32,\n","      device=device,\n","      iterator_train__num_workers=4,\n","      iterator_valid__num_workers=4,\n","      callbacks=callbacks,\n",")\n","\n","data_train_sliceable = SliceDataset(dataset)\n","\n","startall = time.time()\n","scores = cross_validate(cnn, X=data_train_sliceable, y=y_data, cv=5, return_train_score=True)\n","endall = time.time()\n","timeall = endall-startall\n","\n","print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.6195\u001b[0m       \u001b[32m0.3296\u001b[0m        \u001b[35m1.5311\u001b[0m  46.3373\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1808\u001b[0m       \u001b[32m0.6457\u001b[0m        \u001b[35m1.0988\u001b[0m  46.2927\n","      3        \u001b[36m0.9299\u001b[0m       \u001b[32m0.7816\u001b[0m        \u001b[35m0.7479\u001b[0m  46.4692\n","      4        \u001b[36m0.7233\u001b[0m       \u001b[32m0.8138\u001b[0m        \u001b[35m0.6621\u001b[0m  46.4739\n","      5        \u001b[36m0.5750\u001b[0m       \u001b[32m0.8337\u001b[0m        0.6731  46.6310\n","      6        \u001b[36m0.4808\u001b[0m       0.8037        0.7819  46.4340\n","      7        \u001b[36m0.3939\u001b[0m       \u001b[32m0.8495\u001b[0m        \u001b[35m0.5808\u001b[0m  46.5924\n","      8        \u001b[36m0.3401\u001b[0m       \u001b[32m0.8811\u001b[0m        \u001b[35m0.4929\u001b[0m  46.3360\n","      9        \u001b[36m0.2944\u001b[0m       \u001b[32m0.8935\u001b[0m        \u001b[35m0.4189\u001b[0m  46.4183\n","     10        \u001b[36m0.2530\u001b[0m       \u001b[32m0.9063\u001b[0m        0.4210  46.4885\n","     11        \u001b[36m0.2246\u001b[0m       0.9025        0.4719  46.2952\n","     12        \u001b[36m0.1154\u001b[0m       \u001b[32m0.9371\u001b[0m        \u001b[35m0.2986\u001b[0m  46.5979\n","     13        \u001b[36m0.0758\u001b[0m       \u001b[32m0.9405\u001b[0m        0.3005  46.4016\n","     14        \u001b[36m0.0585\u001b[0m       \u001b[32m0.9439\u001b[0m        0.2988  46.3777\n","     15        \u001b[36m0.0493\u001b[0m       0.9429        0.3129  46.4363\n","     16        \u001b[36m0.0412\u001b[0m       0.9413        0.3197  46.4705\n","     17        \u001b[36m0.0351\u001b[0m       0.9399        0.3424  46.4962\n","     18        \u001b[36m0.0313\u001b[0m       0.9415        0.3367  46.3386\n","     19        \u001b[36m0.0283\u001b[0m       \u001b[32m0.9468\u001b[0m        0.3252  46.5243\n","     20        \u001b[36m0.0236\u001b[0m       0.9445        0.3482  46.3076\n","drop ratio: 0.5\n","lowest train loss: 0.023644702474967375\n","lowest valid loss: 0.29860409606516974\n","valid loss at last epoch: 0.34815678503448455\n","max valid accuracy: 0.9467874622513798\n","time per epoch: 46.43595603704453\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5922\u001b[0m       \u001b[32m0.4764\u001b[0m        \u001b[35m1.2770\u001b[0m  46.2560\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1752\u001b[0m       \u001b[32m0.5627\u001b[0m        \u001b[35m1.2199\u001b[0m  46.4041\n","      3        \u001b[36m0.9218\u001b[0m       \u001b[32m0.7415\u001b[0m        \u001b[35m0.8105\u001b[0m  46.3573\n","      4        \u001b[36m0.7264\u001b[0m       \u001b[32m0.8201\u001b[0m        \u001b[35m0.6824\u001b[0m  46.3628\n","      5        \u001b[36m0.5831\u001b[0m       \u001b[32m0.8340\u001b[0m        \u001b[35m0.6160\u001b[0m  46.5727\n","      6        \u001b[36m0.4801\u001b[0m       \u001b[32m0.8563\u001b[0m        \u001b[35m0.5507\u001b[0m  46.4255\n","      7        \u001b[36m0.4032\u001b[0m       \u001b[32m0.8603\u001b[0m        0.5850  46.3602\n","      8        \u001b[36m0.3477\u001b[0m       \u001b[32m0.8816\u001b[0m        \u001b[35m0.4883\u001b[0m  46.5635\n","      9        \u001b[36m0.2966\u001b[0m       \u001b[32m0.8895\u001b[0m        \u001b[35m0.4424\u001b[0m  46.5512\n","     10        \u001b[36m0.2655\u001b[0m       0.8874        0.4510  46.4824\n","     11        \u001b[36m0.2358\u001b[0m       \u001b[32m0.9066\u001b[0m        \u001b[35m0.3828\u001b[0m  46.7301\n","     12        \u001b[36m0.1212\u001b[0m       \u001b[32m0.9366\u001b[0m        \u001b[35m0.3030\u001b[0m  46.5873\n","     13        \u001b[36m0.0819\u001b[0m       \u001b[32m0.9398\u001b[0m        \u001b[35m0.2960\u001b[0m  46.4072\n","     14        \u001b[36m0.0634\u001b[0m       \u001b[32m0.9434\u001b[0m        \u001b[35m0.2919\u001b[0m  46.5507\n","     15        \u001b[36m0.0545\u001b[0m       0.9422        0.2983  46.4698\n","     16        \u001b[36m0.0460\u001b[0m       0.9434        0.3092  46.2665\n","     17        \u001b[36m0.0389\u001b[0m       0.9427        0.3127  46.2857\n","     18        \u001b[36m0.0345\u001b[0m       0.9422        0.3316  46.7324\n","     19        \u001b[36m0.0310\u001b[0m       \u001b[32m0.9469\u001b[0m        0.3159  46.3044\n","     20        \u001b[36m0.0260\u001b[0m       0.9427        0.3465  46.3482\n","drop ratio: 0.5\n","lowest train loss: 0.026003324540051356\n","lowest valid loss: 0.2919086176335266\n","valid loss at last epoch: 0.3465260807575565\n","max valid accuracy: 0.9468915963761324\n","time per epoch: 46.450904643535615\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.6466\u001b[0m       \u001b[32m0.3997\u001b[0m        \u001b[35m1.4195\u001b[0m  46.3258\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1830\u001b[0m       \u001b[32m0.6446\u001b[0m        \u001b[35m1.0266\u001b[0m  46.2434\n","      3        \u001b[36m0.9266\u001b[0m       \u001b[32m0.6853\u001b[0m        \u001b[35m1.0145\u001b[0m  46.4221\n","      4        \u001b[36m0.7272\u001b[0m       \u001b[32m0.7964\u001b[0m        \u001b[35m0.6634\u001b[0m  46.5259\n","      5        \u001b[36m0.5807\u001b[0m       \u001b[32m0.8374\u001b[0m        \u001b[35m0.5721\u001b[0m  46.3981\n","      6        \u001b[36m0.4699\u001b[0m       \u001b[32m0.8583\u001b[0m        \u001b[35m0.5253\u001b[0m  46.3545\n","      7        \u001b[36m0.4019\u001b[0m       0.8466        0.6098  46.2885\n","      8        \u001b[36m0.3433\u001b[0m       0.8487        0.6238  46.7197\n","      9        \u001b[36m0.2965\u001b[0m       \u001b[32m0.8852\u001b[0m        \u001b[35m0.4802\u001b[0m  46.4488\n","     10        \u001b[36m0.2613\u001b[0m       \u001b[32m0.8865\u001b[0m        0.5139  46.2460\n","     11        \u001b[36m0.2302\u001b[0m       \u001b[32m0.8963\u001b[0m        \u001b[35m0.4109\u001b[0m  46.3886\n","     12        \u001b[36m0.1159\u001b[0m       \u001b[32m0.9331\u001b[0m        \u001b[35m0.3005\u001b[0m  46.2752\n","     13        \u001b[36m0.0795\u001b[0m       \u001b[32m0.9395\u001b[0m        \u001b[35m0.2982\u001b[0m  46.3975\n","     14        \u001b[36m0.0628\u001b[0m       0.9393        0.3106  46.4033\n","     15        \u001b[36m0.0511\u001b[0m       \u001b[32m0.9425\u001b[0m        0.3105  46.4858\n","     16        \u001b[36m0.0427\u001b[0m       0.9406        0.3228  46.3910\n","     17        \u001b[36m0.0374\u001b[0m       0.9424        0.3296  46.6037\n","     18        \u001b[36m0.0345\u001b[0m       0.9389        0.3544  46.4387\n","     19        \u001b[36m0.0299\u001b[0m       \u001b[32m0.9448\u001b[0m        0.3390  46.4707\n","     20        \u001b[36m0.0261\u001b[0m       0.9418        0.3684  46.6500\n","drop ratio: 0.5\n","lowest train loss: 0.026081653460896633\n","lowest valid loss: 0.29821930566958665\n","valid loss at last epoch: 0.36838211718997815\n","max valid accuracy: 0.9448089138810788\n","time per epoch: 46.42387241125107\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5816\u001b[0m       \u001b[32m0.3000\u001b[0m        \u001b[35m1.6784\u001b[0m  46.5942\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1683\u001b[0m       \u001b[32m0.6220\u001b[0m        \u001b[35m1.0801\u001b[0m  46.6481\n","      3        \u001b[36m0.9201\u001b[0m       \u001b[32m0.7387\u001b[0m        \u001b[35m0.8126\u001b[0m  46.7294\n","      4        \u001b[36m0.7236\u001b[0m       \u001b[32m0.8115\u001b[0m        \u001b[35m0.6917\u001b[0m  46.9151\n","      5        \u001b[36m0.5841\u001b[0m       \u001b[32m0.8437\u001b[0m        \u001b[35m0.5779\u001b[0m  46.6738\n","      6        \u001b[36m0.4671\u001b[0m       0.8189        0.7803  46.7693\n","      7        \u001b[36m0.3914\u001b[0m       \u001b[32m0.8692\u001b[0m        \u001b[35m0.5154\u001b[0m  46.7699\n","      8        \u001b[36m0.3342\u001b[0m       \u001b[32m0.8748\u001b[0m        \u001b[35m0.5123\u001b[0m  46.6487\n","      9        \u001b[36m0.2992\u001b[0m       \u001b[32m0.9037\u001b[0m        \u001b[35m0.4322\u001b[0m  46.5320\n","     10        \u001b[36m0.2573\u001b[0m       0.8872        0.4939  46.6363\n","     11        \u001b[36m0.2310\u001b[0m       \u001b[32m0.9101\u001b[0m        \u001b[35m0.4138\u001b[0m  46.7222\n","     12        \u001b[36m0.1150\u001b[0m       \u001b[32m0.9370\u001b[0m        \u001b[35m0.2966\u001b[0m  46.7658\n","     13        \u001b[36m0.0782\u001b[0m       \u001b[32m0.9399\u001b[0m        \u001b[35m0.2922\u001b[0m  46.6003\n","     14        \u001b[36m0.0594\u001b[0m       0.9396        0.3241  46.8230\n","     15        \u001b[36m0.0487\u001b[0m       \u001b[32m0.9416\u001b[0m        0.3151  46.5637\n","     16        \u001b[36m0.0406\u001b[0m       0.9410        0.3355  46.5273\n","     17        \u001b[36m0.0347\u001b[0m       0.9399        0.3548  46.9375\n","     18        \u001b[36m0.0310\u001b[0m       \u001b[32m0.9437\u001b[0m        0.3440  46.6094\n","     19        \u001b[36m0.0270\u001b[0m       \u001b[32m0.9442\u001b[0m        0.3581  46.6726\n","     20        \u001b[36m0.0229\u001b[0m       \u001b[32m0.9449\u001b[0m        0.3652  46.5798\n","drop ratio: 0.5\n","lowest train loss: 0.022903012915545938\n","lowest valid loss: 0.29217483056740384\n","valid loss at last epoch: 0.36518837296459483\n","max valid accuracy: 0.9449130480058315\n","time per epoch: 46.68591257333755\n","************cross validate start****\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5935\u001b[0m       \u001b[32m0.3517\u001b[0m        \u001b[35m1.6526\u001b[0m  46.6266\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m1.1648\u001b[0m       \u001b[32m0.5570\u001b[0m        \u001b[35m1.1338\u001b[0m  46.8070\n","      3        \u001b[36m0.9025\u001b[0m       \u001b[32m0.6269\u001b[0m        \u001b[35m1.0001\u001b[0m  46.7136\n","      4        \u001b[36m0.7092\u001b[0m       \u001b[32m0.7973\u001b[0m        \u001b[35m0.6594\u001b[0m  46.7896\n","      5        \u001b[36m0.5638\u001b[0m       \u001b[32m0.8509\u001b[0m        \u001b[35m0.5284\u001b[0m  46.7235\n","      6        \u001b[36m0.4693\u001b[0m       0.8109        0.6340  46.7545\n","      7        \u001b[36m0.3907\u001b[0m       0.8064        0.6534  46.5605\n","      8        \u001b[36m0.3364\u001b[0m       0.8322        0.6447  46.4917\n","      9        \u001b[36m0.2914\u001b[0m       \u001b[32m0.8642\u001b[0m        \u001b[35m0.5198\u001b[0m  46.5219\n","     10        \u001b[36m0.2563\u001b[0m       \u001b[32m0.8643\u001b[0m        \u001b[35m0.5127\u001b[0m  46.5808\n","     11        \u001b[36m0.2196\u001b[0m       0.8407        0.6049  46.3962\n","     12        \u001b[36m0.1171\u001b[0m       \u001b[32m0.9343\u001b[0m        \u001b[35m0.3029\u001b[0m  46.6163\n","     13        \u001b[36m0.0768\u001b[0m       \u001b[32m0.9347\u001b[0m        0.3169  46.3693\n","     14        \u001b[36m0.0608\u001b[0m       \u001b[32m0.9356\u001b[0m        0.3433  46.3025\n","     15        \u001b[36m0.0467\u001b[0m       \u001b[32m0.9384\u001b[0m        0.3292  46.5553\n","     16        \u001b[36m0.0403\u001b[0m       \u001b[32m0.9413\u001b[0m        0.3230  46.4436\n","     17        \u001b[36m0.0343\u001b[0m       \u001b[32m0.9423\u001b[0m        0.3303  46.5443\n","     18        \u001b[36m0.0298\u001b[0m       0.9411        0.3623  46.3160\n","     19        \u001b[36m0.0262\u001b[0m       0.9417        0.3748  46.4580\n","     20        \u001b[36m0.0244\u001b[0m       \u001b[32m0.9426\u001b[0m        0.3895  46.3641\n","drop ratio: 0.5\n","lowest train loss: 0.02436769516810576\n","lowest valid loss: 0.30287125373638724\n","valid loss at last epoch: 0.3895324926790068\n","max valid accuracy: 0.9426220972612726\n","time per epoch: 46.546771347522736\n","Total Time: 4757.56 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2WRS4xViVWKR"},"source":["dr_05_train_loss = [0.02364, 0.02600, 0.02608, 0.02290, 0.02437]\n","dr_05_train_acc = scores['train_score']\n","dr_05_valid_loss = [0.2986, 0.2919, 0.2982, 0.2922, 0.3029]\n","dr_05_valid_loss_last_epoch = [0.3481, 0.3465, 0.3684, 0.3652, 0.3895]\n","dr_05_valid_acc = scores['test_score']\n","dr_05_dur = [46.44, 46.45, 46.42, 46.68, 46.54]\n","\n","rawdata= {'train_loss': dr_05_train_loss, 'train_acc': dr_05_train_acc,\n","          'valid_loss_min': dr_05_valid_loss, 'last_epoch_val_loss': dr_05_valid_loss_last_epoch, \n","          'valid_acc': dr_05_valid_acc, 'time/epoch': dr_05_dur, \n","          }\n","drop_05 = pd.DataFrame(rawdata, columns = ['train_loss', 'train_acc',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc','time/epoch',\n","                                            ])\n","drop_05.to_csv(\"drop_050.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1MtweXnVPBm"},"source":["#### final data"]},{"cell_type":"code","metadata":{"id":"6Vw92ZJNACpg"},"source":["drop_000 = pd.read_csv(\"drop_000.csv\")\n","drop_010 = pd.read_csv(\"drop_010.csv\")\n","drop_025 = pd.read_csv(\"drop_025.csv\")\n","drop_030 = pd.read_csv(\"drop_030.csv\")\n","drop_050 = pd.read_csv(\"drop_050.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"zfppmR6UBXUA","executionInfo":{"status":"ok","timestamp":1607089882544,"user_tz":300,"elapsed":747,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"5ec5a64c-c9f7-43e8-d11a-2d995dcbd3d4"},"source":["drop_000"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss_min</th>\n","      <th>last_epoch_val_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.006198</td>\n","      <td>0.987311</td>\n","      <td>0.2409</td>\n","      <td>0.3374</td>\n","      <td>0.948526</td>\n","      <td>81.63</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.004793</td>\n","      <td>0.987020</td>\n","      <td>0.2385</td>\n","      <td>0.3482</td>\n","      <td>0.945930</td>\n","      <td>81.71</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.004068</td>\n","      <td>0.984605</td>\n","      <td>0.2422</td>\n","      <td>0.3728</td>\n","      <td>0.945570</td>\n","      <td>81.43</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.008388</td>\n","      <td>0.986480</td>\n","      <td>0.2537</td>\n","      <td>0.3457</td>\n","      <td>0.944236</td>\n","      <td>81.28</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.004944</td>\n","      <td>0.987522</td>\n","      <td>0.2479</td>\n","      <td>0.3526</td>\n","      <td>0.952238</td>\n","      <td>81.31</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_loss  train_acc  ...  valid_acc  time/epoch\n","0    0.006198   0.987311  ...   0.948526       81.63\n","1    0.004793   0.987020  ...   0.945930       81.71\n","2    0.004068   0.984605  ...   0.945570       81.43\n","3    0.008388   0.986480  ...   0.944236       81.28\n","4    0.004944   0.987522  ...   0.952238       81.31\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"InsSD7hAAJaD"},"source":["drop = [0, 0.1, 0.25, 0.3, 0.5]\n","train_loss = [np.average(drop_000['train_loss']),\n","              np.average(drop_010['train_loss']),\n","              np.average(drop_025['train_loss']),\n","              np.average(drop_030['train_loss']),\n","              np.average(drop_050['train_loss'])]\n","train_acc = [np.average(drop_000['train_acc']),\n","             np.average(drop_010['train_acc']),\n","             np.average(drop_025['train_acc']),\n","             np.average(drop_030['train_acc']),\n","             np.average(drop_050['train_acc'])]\n","valid_loss = [np.average(drop_000['valid_loss_min']),\n","              np.average(drop_010['valid_loss_min']),\n","              np.average(drop_025['valid_loss_min']),\n","              np.average(drop_030['valid_loss_min']),\n","              np.average(drop_050['valid_loss_min'])]\n","valid_loss_last_epoch = [np.average(drop_000['last_epoch_val_loss']),\n","                         np.average(drop_010['last_epoch_val_loss']),\n","                         np.average(drop_025['last_epoch_val_loss']),\n","                         np.average(drop_030['last_epoch_val_loss']),\n","                         np.average(drop_050['last_epoch_val_loss'])]\n","valid_acc = [np.average(drop_000['valid_acc']),\n","             np.average(drop_010['valid_acc']),\n","             np.average(drop_025['valid_acc']),\n","             np.average(drop_030['valid_acc']),\n","             np.average(drop_050['valid_acc'])]\n","\n","rawdata= {'drop_ratio': drop, 'train_loss': train_loss, 'train_acc': train_acc,\n","          'valid_loss_min': valid_loss, 'last_epoch_val_loss': valid_loss_last_epoch, 'valid_acc': valid_acc,\n","          }\n","drop_cv = pd.DataFrame(rawdata, columns = ['drop_ratio','train_loss', 'train_acc',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc'\n","                                            ])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"h-2hZifkCFWH","executionInfo":{"status":"ok","timestamp":1607090095413,"user_tz":300,"elapsed":731,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"34ccd80e-546c-474f-faf1-984acb4d544c"},"source":["drop_cv"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>drop_ratio</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss_min</th>\n","      <th>last_epoch_val_loss</th>\n","      <th>valid_acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.005678</td>\n","      <td>0.986588</td>\n","      <td>0.24464</td>\n","      <td>0.35134</td>\n","      <td>0.947300</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.10</td>\n","      <td>0.009784</td>\n","      <td>0.986688</td>\n","      <td>0.26656</td>\n","      <td>0.35042</td>\n","      <td>0.947800</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.25</td>\n","      <td>0.012733</td>\n","      <td>0.986596</td>\n","      <td>0.26578</td>\n","      <td>0.34280</td>\n","      <td>0.949067</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.30</td>\n","      <td>0.015512</td>\n","      <td>0.985617</td>\n","      <td>0.27442</td>\n","      <td>0.33684</td>\n","      <td>0.948367</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.50</td>\n","      <td>0.024598</td>\n","      <td>0.983983</td>\n","      <td>0.29676</td>\n","      <td>0.36354</td>\n","      <td>0.945084</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   drop_ratio  train_loss  ...  last_epoch_val_loss  valid_acc\n","0        0.00    0.005678  ...              0.35134   0.947300\n","1        0.10    0.009784  ...              0.35042   0.947800\n","2        0.25    0.012733  ...              0.34280   0.949067\n","3        0.30    0.015512  ...              0.33684   0.948367\n","4        0.50    0.024598  ...              0.36354   0.945084\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"k_P2LU40CVsm"},"source":["drop_cv.to_csv(\"drop_cv.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"joZHTIERDG1U","executionInfo":{"status":"ok","timestamp":1607093786666,"user_tz":300,"elapsed":850,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"e159d7e4-8441-442d-932b-ef4166685aff"},"source":["fig, ax = plt.subplots() \n","\n","ax2 = ax.twinx()\n","\n","drop_cv.plot(x = \"drop_ratio\", y = \"valid_acc\", kind='line', color='C1', ax=ax, label=\"Val. Acc.\")\n","drop_cv.plot(x = \"drop_ratio\", y = \"train_acc\", kind='line', color='C2', ax=ax2, label=\"train. Acc.\")\n","ax.set_xlabel(\"Drop ratio\")\n","ax.set_ylabel('Validation Accuracy')\n","ax2.set_ylabel('Train Accuracy')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Train Accuracy')"]},"metadata":{"tags":[]},"execution_count":28},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"36FMUXFLDYJK","executionInfo":{"status":"ok","timestamp":1607093595367,"user_tz":300,"elapsed":727,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"91ad98f4-a5a0-4641-e8ed-ad5e3852e313"},"source":["fig, ax = plt.subplots() \n","\n","ax2 = ax.twinx()\n","\n","width = 0.2\n","\n","drop_cv.plot(x = \"drop_ratio\", y = \"valid_loss_min\", kind='line', color='C1', ax=ax, label=\"Val. Loss.\")\n","drop_cv.plot(x = \"drop_ratio\", y = \"train_loss\", kind='line', color='C2', ax=ax2, label=\"train. Loss.\")\n","ax.set_xlabel(\"Drop ratio\")\n","ax.set_ylabel('Validation Loss')\n","ax2.set_ylabel('Train Loss')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Train Accuracy')"]},"metadata":{"tags":[]},"execution_count":26},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"d9Iz1LZF_Ym1"},"source":["\n","\n","fig, ax = plt.subplots() \n","\n","ax2 = ax.twinx()\n","\n","width = 0.2\n","\n","drop_cv.plot(x = \"drop_ratio\", y = \"valid_acc\", kind='line', color='C1', ax=ax, width=width, position=1, label=\"Val. Acc.\")\n","drop_cv.plot(x = \"Network_type\", y = \"total_time\", kind='bar', color='C2', ax=ax2, width=width, position=0,label=\"Time\")\n","ax.set_xlabel(\"VGG Type\")\n","ax.set_ylabel('Validation Accuracy')\n","ax2.set_ylabel('Time [s]')\n","ax.set_ylim(0.9)\n","ax.legend(loc=9);\n","plt.tight_layout()\n","plt.savefig(\"plots/vgg_type.pdf\", bbox_inches=\"tight\", pad_inches=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KROYC30M7jhR"},"source":["# final data\n","\n","rawdata= {'drop_ratio': dropout, 'train_loss': train_loss, 'train_acc': train_acc,\n","          'valid_loss_min': valid_loss, 'last_epoch_val_loss': valid_loss_last_epoch, 'valid_acc': valid_acc, 'time/epoch': dur, \n","          }\n","drop_cv = pd.DataFrame(rawdata, columns = ['drop_ratio','train_loss', 'train_acc',\n","                                            'valid_loss_min', 'last_epoch_val_loss','valid_acc','time/epoch',\n","                                            ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1DvtY5MUFNd"},"source":["# Test weight decay + drop ratio"]},{"cell_type":"markdown","metadata":{"id":"naHNbd4qfuD6"},"source":["Fixed parameters:\n","- VGG-16\n","- batch size = 32\n","- learning rate: LRScheduler=ReduceLROnPlateau, initial lr=1e-4\n","- Optimizer = AdamW\n","- epochs = 20\n","\n","Testing parameters:\n","- drop ratio = 0.25,0.3,0.5\n","- weight decay = 0.005, 0.01 0.02"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YolpwLimUJwK","executionInfo":{"status":"ok","timestamp":1607110896832,"user_tz":300,"elapsed":11657106,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"4da0dff3-0dfb-4014-e43d-d738ff87bdcf"},"source":["import skorch\n","\n","weight_decay_pool = [0.005, 0.01, 0.02]\n","drop_out_pool = [0.25, 0.3, 0.5]\n","drop = []\n","weight_decay = []\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n","callbacks = callbacks=[\n","        ('print', Monitor()), ('lr_scheduler',\n","                     LRScheduler(policy=ReduceLROnPlateau, \n","                     monitor = \"train_loss\",\n","                     ))\n","    ]\n","\n","for wd in weight_decay_pool:\n","\n","  for dr in drop_out_pool:\n","\n","    drop_ratio = dr\n","    network = VGG_net().to(device)\n","    torch.manual_seed(0)\n","    cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs=20,\n","        lr=1e-4,\n","        optimizer=torch.optim.AdamW,\n","        optimizer__weight_decay=wd,\n","        batch_size=32,\n","        device=device,\n","        iterator_train__num_workers=4,\n","        iterator_valid__num_workers=4,\n","        callbacks=callbacks,\n","        train_split = skorch.dataset.CVSplit(10)\n","    )\n","    print(f\"fitting with weight decay of {wd} and drop ratio of {dr}\")\n","\n","    startall = time.time()\n","    cnn.fit(dataset, y=y_data)\n","    endall = time.time()\n","    timeall = endall-startall\n","\n","    train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","    valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","    valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","    dur.append(np.average(cnn.history[:, 'dur']))\n","    drop.append(dr)\n","    weight_decay.append(wd)\n","    total_time.append(timeall)\n","    \n","    print(f\"drop ratio: {dr}\")\n","    print(f\"weight decay: {wd}\")\n","    print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","    print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","    print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","    print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","    print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fitting with weight decay of 0.005 and drop ratio of 0.25\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4429\u001b[0m       \u001b[32m0.6062\u001b[0m        \u001b[35m1.1083\u001b[0m  64.8704\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["learning rate = 0.0001\n","      2        \u001b[36m0.9211\u001b[0m       \u001b[32m0.7695\u001b[0m        \u001b[35m0.7227\u001b[0m  64.9704\n","learning rate = 0.0001\n","      3        \u001b[36m0.6211\u001b[0m       \u001b[32m0.8327\u001b[0m        \u001b[35m0.5789\u001b[0m  64.7473\n","learning rate = 0.0001\n","      4        \u001b[36m0.4391\u001b[0m       \u001b[32m0.8950\u001b[0m        \u001b[35m0.3877\u001b[0m  64.3247\n","learning rate = 0.0001\n","      5        \u001b[36m0.3302\u001b[0m       \u001b[32m0.8978\u001b[0m        \u001b[35m0.3779\u001b[0m  64.3076\n","learning rate = 0.0001\n","      6        \u001b[36m0.2583\u001b[0m       0.8787        0.5199  64.7483\n","learning rate = 0.0001\n","      7        \u001b[36m0.2092\u001b[0m       0.8918        0.3963  64.4641\n","learning rate = 0.0001\n","      8        \u001b[36m0.1751\u001b[0m       \u001b[32m0.9105\u001b[0m        0.3789  64.6481\n","learning rate = 0.0001\n","      9        \u001b[36m0.1512\u001b[0m       \u001b[32m0.9373\u001b[0m        \u001b[35m0.2709\u001b[0m  64.6969\n","learning rate = 0.0001\n","     10        \u001b[36m0.1244\u001b[0m       \u001b[32m0.9387\u001b[0m        0.2742  64.8738\n","learning rate = 0.0001\n","     11        \u001b[36m0.1107\u001b[0m       \u001b[32m0.9442\u001b[0m        \u001b[35m0.2387\u001b[0m  64.7810\n","learning rate = 1e-05\n","     12        \u001b[36m0.0460\u001b[0m       \u001b[32m0.9670\u001b[0m        \u001b[35m0.1440\u001b[0m  64.5995\n","learning rate = 1e-05\n","     13        \u001b[36m0.0250\u001b[0m       \u001b[32m0.9690\u001b[0m        \u001b[35m0.1407\u001b[0m  64.3134\n","learning rate = 1e-05\n","     14        \u001b[36m0.0167\u001b[0m       0.9685        0.1572  64.5831\n","learning rate = 1e-05\n","     15        \u001b[36m0.0123\u001b[0m       \u001b[32m0.9710\u001b[0m        0.1640  64.8339\n","learning rate = 1e-05\n","     16        \u001b[36m0.0089\u001b[0m       0.9705        0.1829  64.5870\n","learning rate = 1e-05\n","     17        \u001b[36m0.0069\u001b[0m       \u001b[32m0.9715\u001b[0m        0.1671  64.4864\n","learning rate = 1e-05\n","     18        \u001b[36m0.0058\u001b[0m       \u001b[32m0.9717\u001b[0m        0.1697  64.7275\n","learning rate = 1e-05\n","     19        \u001b[36m0.0037\u001b[0m       0.9700        0.1952  64.5935\n","learning rate = 1e-05\n","     20        0.0053       0.9700        0.1997  64.6400\n","drop ratio: 0.25\n","weight decay: 0.005\n","lowest train loss: 0.003668456250862585\n","lowest valid loss: 0.14072280377071972\n","max valid accuracy: 0.9716666666666667\n","time per epoch: 64.63984868526458\n","Total Time: 1295.40 s\n","fitting with weight decay of 0.005 and drop ratio of 0.3\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4502\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m1.1384\u001b[0m  64.8569\n","learning rate = 0.0001\n","      2        \u001b[36m0.9319\u001b[0m       \u001b[32m0.7957\u001b[0m        \u001b[35m0.7019\u001b[0m  64.5265\n","learning rate = 0.0001\n","      3        \u001b[36m0.6470\u001b[0m       \u001b[32m0.8065\u001b[0m        \u001b[35m0.6434\u001b[0m  64.8419\n","learning rate = 0.0001\n","      4        \u001b[36m0.4710\u001b[0m       \u001b[32m0.8800\u001b[0m        \u001b[35m0.4439\u001b[0m  64.5369\n","learning rate = 0.0001\n","      5        \u001b[36m0.3571\u001b[0m       \u001b[32m0.8950\u001b[0m        \u001b[35m0.3926\u001b[0m  64.4721\n","learning rate = 0.0001\n","      6        \u001b[36m0.2750\u001b[0m       \u001b[32m0.9148\u001b[0m        \u001b[35m0.3400\u001b[0m  64.7104\n","learning rate = 0.0001\n","      7        \u001b[36m0.2243\u001b[0m       0.9025        0.3802  64.4759\n","learning rate = 0.0001\n","      8        \u001b[36m0.1841\u001b[0m       \u001b[32m0.9248\u001b[0m        \u001b[35m0.3112\u001b[0m  64.6765\n","learning rate = 0.0001\n","      9        \u001b[36m0.1594\u001b[0m       \u001b[32m0.9322\u001b[0m        \u001b[35m0.2949\u001b[0m  64.8015\n","learning rate = 0.0001\n","     10        \u001b[36m0.1391\u001b[0m       0.9302        0.3134  64.6412\n","learning rate = 0.0001\n","     11        \u001b[36m0.1233\u001b[0m       \u001b[32m0.9452\u001b[0m        \u001b[35m0.2636\u001b[0m  64.7567\n","learning rate = 1e-05\n","     12        \u001b[36m0.0486\u001b[0m       \u001b[32m0.9603\u001b[0m        \u001b[35m0.1862\u001b[0m  64.6768\n","learning rate = 1e-05\n","     13        \u001b[36m0.0262\u001b[0m       \u001b[32m0.9630\u001b[0m        0.1937  64.6850\n","learning rate = 1e-05\n","     14        \u001b[36m0.0173\u001b[0m       \u001b[32m0.9667\u001b[0m        0.1928  64.6172\n","learning rate = 1e-05\n","     15        \u001b[36m0.0135\u001b[0m       \u001b[32m0.9680\u001b[0m        0.1881  64.8781\n","learning rate = 1e-05\n","     16        \u001b[36m0.0109\u001b[0m       \u001b[32m0.9687\u001b[0m        0.2125  64.6979\n","learning rate = 1e-05\n","     17        \u001b[36m0.0084\u001b[0m       \u001b[32m0.9702\u001b[0m        0.2026  64.6304\n","learning rate = 1e-05\n","     18        \u001b[36m0.0069\u001b[0m       0.9667        0.2371  64.7086\n","learning rate = 1e-05\n","     19        0.0072       0.9670        0.2296  65.0115\n","learning rate = 1e-05\n","     20        \u001b[36m0.0043\u001b[0m       0.9675        0.2596  64.8998\n","drop ratio: 0.3\n","weight decay: 0.005\n","lowest train loss: 0.004286202942456401\n","lowest valid loss: 0.18623314584993447\n","max valid accuracy: 0.9701666666666666\n","time per epoch: 64.7050912976265\n","Total Time: 1296.71 s\n","fitting with weight decay of 0.005 and drop ratio of 0.5\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5251\u001b[0m       \u001b[32m0.5212\u001b[0m        \u001b[35m1.2150\u001b[0m  64.7351\n","learning rate = 0.0001\n","      2        \u001b[36m1.0341\u001b[0m       \u001b[32m0.7528\u001b[0m        \u001b[35m0.8101\u001b[0m  64.7488\n","learning rate = 0.0001\n","      3        \u001b[36m0.7494\u001b[0m       \u001b[32m0.7648\u001b[0m        \u001b[35m0.7960\u001b[0m  64.7008\n","learning rate = 0.0001\n","      4        \u001b[36m0.5555\u001b[0m       \u001b[32m0.8450\u001b[0m        \u001b[35m0.5408\u001b[0m  65.2089\n","learning rate = 0.0001\n","      5        \u001b[36m0.4360\u001b[0m       \u001b[32m0.8752\u001b[0m        \u001b[35m0.4895\u001b[0m  64.7207\n","learning rate = 0.0001\n","      6        \u001b[36m0.3445\u001b[0m       \u001b[32m0.9025\u001b[0m        \u001b[35m0.3681\u001b[0m  64.5423\n","learning rate = 0.0001\n","      7        \u001b[36m0.2789\u001b[0m       0.8812        0.4805  64.6094\n","learning rate = 0.0001\n","      8        \u001b[36m0.2332\u001b[0m       \u001b[32m0.9212\u001b[0m        \u001b[35m0.3367\u001b[0m  64.7010\n","learning rate = 0.0001\n","      9        \u001b[36m0.2031\u001b[0m       \u001b[32m0.9213\u001b[0m        0.3859  64.3652\n","learning rate = 0.0001\n","     10        \u001b[36m0.1755\u001b[0m       \u001b[32m0.9358\u001b[0m        \u001b[35m0.2957\u001b[0m  64.7987\n","learning rate = 0.0001\n","     11        \u001b[36m0.1567\u001b[0m       \u001b[32m0.9397\u001b[0m        \u001b[35m0.2644\u001b[0m  64.4855\n","learning rate = 1e-05\n","     12        \u001b[36m0.0639\u001b[0m       \u001b[32m0.9585\u001b[0m        \u001b[35m0.2068\u001b[0m  64.8660\n","learning rate = 1e-05\n","     13        \u001b[36m0.0398\u001b[0m       \u001b[32m0.9623\u001b[0m        \u001b[35m0.2020\u001b[0m  64.8833\n","learning rate = 1e-05\n","     14        \u001b[36m0.0285\u001b[0m       \u001b[32m0.9632\u001b[0m        0.2166  65.1583\n","learning rate = 1e-05\n","     15        \u001b[36m0.0216\u001b[0m       \u001b[32m0.9635\u001b[0m        0.2064  64.8717\n","learning rate = 1e-05\n","     16        \u001b[36m0.0169\u001b[0m       \u001b[32m0.9655\u001b[0m        0.2093  64.5864\n","learning rate = 1e-05\n","     17        \u001b[36m0.0147\u001b[0m       \u001b[32m0.9660\u001b[0m        0.2135  64.8036\n","learning rate = 1e-05\n","     18        \u001b[36m0.0131\u001b[0m       \u001b[32m0.9662\u001b[0m        0.2175  64.7527\n","learning rate = 1e-05\n","     19        \u001b[36m0.0101\u001b[0m       0.9605        0.2840  64.6156\n","learning rate = 1e-05\n","     20        0.0107       \u001b[32m0.9678\u001b[0m        0.2275  64.6860\n","drop ratio: 0.5\n","weight decay: 0.005\n","lowest train loss: 0.010103653841223503\n","lowest valid loss: 0.20196710441269292\n","max valid accuracy: 0.9678333333333333\n","time per epoch: 64.74199492931366\n","Total Time: 1297.57 s\n","fitting with weight decay of 0.01 and drop ratio of 0.25\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4527\u001b[0m       \u001b[32m0.6050\u001b[0m        \u001b[35m1.0908\u001b[0m  64.8714\n","learning rate = 0.0001\n","      2        \u001b[36m0.9288\u001b[0m       \u001b[32m0.7675\u001b[0m        \u001b[35m0.7322\u001b[0m  64.6569\n","learning rate = 0.0001\n","      3        \u001b[36m0.6368\u001b[0m       \u001b[32m0.8388\u001b[0m        \u001b[35m0.5715\u001b[0m  64.5302\n","learning rate = 0.0001\n","      4        \u001b[36m0.4532\u001b[0m       \u001b[32m0.8803\u001b[0m        \u001b[35m0.4634\u001b[0m  64.9760\n","learning rate = 0.0001\n","      5        \u001b[36m0.3443\u001b[0m       \u001b[32m0.8993\u001b[0m        \u001b[35m0.3811\u001b[0m  64.6186\n","learning rate = 0.0001\n","      6        \u001b[36m0.2717\u001b[0m       \u001b[32m0.9158\u001b[0m        \u001b[35m0.3457\u001b[0m  64.6846\n","learning rate = 0.0001\n","      7        \u001b[36m0.2253\u001b[0m       \u001b[32m0.9167\u001b[0m        \u001b[35m0.3396\u001b[0m  64.7253\n","learning rate = 0.0001\n","      8        \u001b[36m0.1795\u001b[0m       \u001b[32m0.9203\u001b[0m        0.3455  64.8235\n","learning rate = 0.0001\n","      9        \u001b[36m0.1547\u001b[0m       \u001b[32m0.9338\u001b[0m        \u001b[35m0.2603\u001b[0m  64.9731\n","learning rate = 0.0001\n","     10        \u001b[36m0.1346\u001b[0m       \u001b[32m0.9413\u001b[0m        \u001b[35m0.2578\u001b[0m  64.6512\n","learning rate = 0.0001\n","     11        \u001b[36m0.1243\u001b[0m       \u001b[32m0.9423\u001b[0m        \u001b[35m0.2508\u001b[0m  65.0256\n","learning rate = 1e-05\n","     12        \u001b[36m0.0525\u001b[0m       \u001b[32m0.9602\u001b[0m        \u001b[35m0.1728\u001b[0m  64.6445\n","learning rate = 1e-05\n","     13        \u001b[36m0.0283\u001b[0m       \u001b[32m0.9633\u001b[0m        0.1739  64.8161\n","learning rate = 1e-05\n","     14        \u001b[36m0.0193\u001b[0m       \u001b[32m0.9680\u001b[0m        \u001b[35m0.1718\u001b[0m  64.6473\n","learning rate = 1e-05\n","     15        \u001b[36m0.0141\u001b[0m       0.9650        0.1850  64.6384\n","learning rate = 1e-05\n","     16        \u001b[36m0.0116\u001b[0m       \u001b[32m0.9692\u001b[0m        0.1921  64.8529\n","learning rate = 1e-05\n","     17        \u001b[36m0.0093\u001b[0m       \u001b[32m0.9712\u001b[0m        0.1818  64.8571\n","learning rate = 1e-05\n","     18        \u001b[36m0.0067\u001b[0m       0.9688        0.1995  64.8000\n","learning rate = 1e-05\n","     19        \u001b[36m0.0058\u001b[0m       0.9665        0.2236  64.5651\n","learning rate = 1e-05\n","     20        \u001b[36m0.0047\u001b[0m       0.9700        0.2081  64.7479\n","drop ratio: 0.25\n","weight decay: 0.01\n","lowest train loss: 0.004672427908256025\n","lowest valid loss: 0.17180114636958266\n","max valid accuracy: 0.9711666666666666\n","time per epoch: 64.75527688264847\n","Total Time: 1297.75 s\n","fitting with weight decay of 0.01 and drop ratio of 0.3\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4512\u001b[0m       \u001b[32m0.5913\u001b[0m        \u001b[35m1.0871\u001b[0m  64.7216\n","learning rate = 0.0001\n","      2        \u001b[36m0.9363\u001b[0m       \u001b[32m0.7758\u001b[0m        \u001b[35m0.7412\u001b[0m  64.7310\n","learning rate = 0.0001\n","      3        \u001b[36m0.6397\u001b[0m       \u001b[32m0.8513\u001b[0m        \u001b[35m0.5366\u001b[0m  64.9036\n","learning rate = 0.0001\n","      4        \u001b[36m0.4563\u001b[0m       \u001b[32m0.8832\u001b[0m        \u001b[35m0.4230\u001b[0m  64.7993\n","learning rate = 0.0001\n","      5        \u001b[36m0.3454\u001b[0m       \u001b[32m0.8928\u001b[0m        \u001b[35m0.3924\u001b[0m  64.7822\n","learning rate = 0.0001\n","      6        \u001b[36m0.2675\u001b[0m       \u001b[32m0.8957\u001b[0m        0.3939  64.6559\n","learning rate = 0.0001\n","      7        \u001b[36m0.2173\u001b[0m       \u001b[32m0.9178\u001b[0m        \u001b[35m0.3346\u001b[0m  64.5719\n","learning rate = 0.0001\n","      8        \u001b[36m0.1832\u001b[0m       \u001b[32m0.9328\u001b[0m        \u001b[35m0.2878\u001b[0m  65.1354\n","learning rate = 0.0001\n","      9        \u001b[36m0.1569\u001b[0m       0.9328        \u001b[35m0.2784\u001b[0m  64.8341\n","learning rate = 0.0001\n","     10        \u001b[36m0.1351\u001b[0m       \u001b[32m0.9390\u001b[0m        \u001b[35m0.2581\u001b[0m  64.8821\n","learning rate = 0.0001\n","     11        \u001b[36m0.1163\u001b[0m       \u001b[32m0.9395\u001b[0m        0.2590  64.8180\n","learning rate = 1e-05\n","     12        \u001b[36m0.0475\u001b[0m       \u001b[32m0.9645\u001b[0m        \u001b[35m0.1568\u001b[0m  64.8331\n","learning rate = 1e-05\n","     13        \u001b[36m0.0260\u001b[0m       \u001b[32m0.9693\u001b[0m        0.1594  65.0016\n","learning rate = 1e-05\n","     14        \u001b[36m0.0174\u001b[0m       0.9683        0.1707  64.6561\n","learning rate = 1e-05\n","     15        \u001b[36m0.0134\u001b[0m       0.9687        0.1778  64.6955\n","learning rate = 1e-05\n","     16        \u001b[36m0.0104\u001b[0m       0.9685        0.1910  64.6717\n","learning rate = 1e-05\n","     17        \u001b[36m0.0087\u001b[0m       \u001b[32m0.9712\u001b[0m        0.1702  64.7126\n","learning rate = 1e-05\n","     18        \u001b[36m0.0068\u001b[0m       0.9697        0.1853  65.0216\n","learning rate = 1e-05\n","     19        \u001b[36m0.0064\u001b[0m       \u001b[32m0.9718\u001b[0m        0.1845  65.0651\n","learning rate = 1e-05\n","     20        \u001b[36m0.0047\u001b[0m       0.9713        0.1985  64.7393\n","drop ratio: 0.3\n","weight decay: 0.01\n","lowest train loss: 0.004685295946918982\n","lowest valid loss: 0.15676283587305806\n","max valid accuracy: 0.9718333333333333\n","time per epoch: 64.81158592700959\n","Total Time: 1298.89 s\n","fitting with weight decay of 0.01 and drop ratio of 0.5\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5197\u001b[0m       \u001b[32m0.5920\u001b[0m        \u001b[35m1.0742\u001b[0m  64.5454\n","learning rate = 0.0001\n","      2        \u001b[36m1.0305\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.9046\u001b[0m  64.9711\n","learning rate = 0.0001\n","      3        \u001b[36m0.7442\u001b[0m       \u001b[32m0.8167\u001b[0m        \u001b[35m0.6432\u001b[0m  64.7941\n","learning rate = 0.0001\n","      4        \u001b[36m0.5589\u001b[0m       \u001b[32m0.8637\u001b[0m        \u001b[35m0.5123\u001b[0m  64.7341\n","learning rate = 0.0001\n","      5        \u001b[36m0.4389\u001b[0m       \u001b[32m0.8715\u001b[0m        \u001b[35m0.5005\u001b[0m  64.9114\n","learning rate = 0.0001\n","      6        \u001b[36m0.3570\u001b[0m       \u001b[32m0.8997\u001b[0m        \u001b[35m0.3795\u001b[0m  64.9174\n","learning rate = 0.0001\n","      7        \u001b[36m0.2945\u001b[0m       \u001b[32m0.9083\u001b[0m        \u001b[35m0.3527\u001b[0m  64.8889\n","learning rate = 0.0001\n","      8        \u001b[36m0.2511\u001b[0m       \u001b[32m0.9128\u001b[0m        0.3733  64.6284\n","learning rate = 0.0001\n","      9        \u001b[36m0.2197\u001b[0m       \u001b[32m0.9258\u001b[0m        \u001b[35m0.3077\u001b[0m  64.6938\n","learning rate = 0.0001\n","     10        \u001b[36m0.1899\u001b[0m       0.9210        0.3460  64.7492\n","learning rate = 0.0001\n","     11        \u001b[36m0.1704\u001b[0m       \u001b[32m0.9335\u001b[0m        \u001b[35m0.2972\u001b[0m  64.8970\n","learning rate = 1e-05\n","     12        \u001b[36m0.0741\u001b[0m       \u001b[32m0.9573\u001b[0m        \u001b[35m0.2109\u001b[0m  64.8570\n","learning rate = 1e-05\n","     13        \u001b[36m0.0453\u001b[0m       \u001b[32m0.9583\u001b[0m        \u001b[35m0.2099\u001b[0m  64.7858\n","learning rate = 1e-05\n","     14        \u001b[36m0.0329\u001b[0m       \u001b[32m0.9630\u001b[0m        \u001b[35m0.2090\u001b[0m  64.8564\n","learning rate = 1e-05\n","     15        \u001b[36m0.0260\u001b[0m       0.9615        0.2219  64.6398\n","learning rate = 1e-05\n","     16        \u001b[36m0.0214\u001b[0m       0.9608        0.2357  64.7522\n","learning rate = 1e-05\n","     17        \u001b[36m0.0167\u001b[0m       0.9628        0.2411  64.9572\n","learning rate = 1e-05\n","     18        \u001b[36m0.0157\u001b[0m       \u001b[32m0.9648\u001b[0m        0.2261  64.6631\n","learning rate = 1e-05\n","     19        \u001b[36m0.0124\u001b[0m       \u001b[32m0.9667\u001b[0m        0.2310  64.7884\n","learning rate = 1e-05\n","     20        \u001b[36m0.0121\u001b[0m       0.9658        0.2491  64.7920\n","drop ratio: 0.5\n","weight decay: 0.01\n","lowest train loss: 0.012142243019463624\n","lowest valid loss: 0.20904692930767002\n","max valid accuracy: 0.9666666666666667\n","time per epoch: 64.79113637208938\n","Total Time: 1298.47 s\n","fitting with weight decay of 0.02 and drop ratio of 0.25\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4457\u001b[0m       \u001b[32m0.6223\u001b[0m        \u001b[35m1.0451\u001b[0m  64.8302\n","learning rate = 0.0001\n","      2        \u001b[36m0.9253\u001b[0m       \u001b[32m0.7910\u001b[0m        \u001b[35m0.6847\u001b[0m  64.7767\n","learning rate = 0.0001\n","      3        \u001b[36m0.6265\u001b[0m       \u001b[32m0.8302\u001b[0m        \u001b[35m0.5943\u001b[0m  64.7957\n","learning rate = 0.0001\n","      4        \u001b[36m0.4432\u001b[0m       \u001b[32m0.8675\u001b[0m        \u001b[35m0.4996\u001b[0m  64.6797\n","learning rate = 0.0001\n","      5        \u001b[36m0.3341\u001b[0m       \u001b[32m0.9017\u001b[0m        \u001b[35m0.3512\u001b[0m  64.7377\n","learning rate = 0.0001\n","      6        \u001b[36m0.2608\u001b[0m       \u001b[32m0.9092\u001b[0m        0.3638  65.0668\n","learning rate = 0.0001\n","      7        \u001b[36m0.2149\u001b[0m       0.8878        0.4442  64.7803\n","learning rate = 0.0001\n","      8        \u001b[36m0.1803\u001b[0m       \u001b[32m0.9213\u001b[0m        \u001b[35m0.3253\u001b[0m  64.7269\n","learning rate = 0.0001\n","      9        \u001b[36m0.1567\u001b[0m       0.9165        \u001b[35m0.3159\u001b[0m  64.9380\n","learning rate = 0.0001\n","     10        \u001b[36m0.1412\u001b[0m       \u001b[32m0.9277\u001b[0m        \u001b[35m0.3001\u001b[0m  64.6154\n","learning rate = 0.0001\n","     11        \u001b[36m0.1132\u001b[0m       0.9253        0.3344  64.8259\n","learning rate = 1e-05\n","     12        \u001b[36m0.0491\u001b[0m       \u001b[32m0.9603\u001b[0m        \u001b[35m0.1793\u001b[0m  64.6136\n","learning rate = 1e-05\n","     13        \u001b[36m0.0249\u001b[0m       \u001b[32m0.9638\u001b[0m        0.1803  64.4966\n","learning rate = 1e-05\n","     14        \u001b[36m0.0166\u001b[0m       \u001b[32m0.9660\u001b[0m        0.1841  64.3626\n","learning rate = 1e-05\n","     15        \u001b[36m0.0132\u001b[0m       0.9630        0.2020  64.4951\n","learning rate = 1e-05\n","     16        \u001b[36m0.0103\u001b[0m       0.9653        0.2005  64.6173\n","learning rate = 1e-05\n","     17        \u001b[36m0.0089\u001b[0m       0.9657        0.2114  64.5818\n","learning rate = 1e-05\n","     18        \u001b[36m0.0060\u001b[0m       0.9650        0.2233  64.1616\n","learning rate = 1e-05\n","     19        0.0063       \u001b[32m0.9687\u001b[0m        0.2252  64.4074\n","learning rate = 1e-05\n","     20        \u001b[36m0.0052\u001b[0m       0.9665        0.2327  64.4024\n","drop ratio: 0.25\n","weight decay: 0.02\n","lowest train loss: 0.005192202356982442\n","lowest valid loss: 0.17931605140833806\n","max valid accuracy: 0.9686666666666667\n","time per epoch: 64.64558013677598\n","Total Time: 1295.46 s\n","fitting with weight decay of 0.02 and drop ratio of 0.3\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4562\u001b[0m       \u001b[32m0.6203\u001b[0m        \u001b[35m1.0530\u001b[0m  64.5295\n","learning rate = 0.0001\n","      2        \u001b[36m0.9382\u001b[0m       \u001b[32m0.7523\u001b[0m        \u001b[35m0.8014\u001b[0m  64.4185\n","learning rate = 0.0001\n","      3        \u001b[36m0.6524\u001b[0m       \u001b[32m0.8415\u001b[0m        \u001b[35m0.5599\u001b[0m  64.3272\n","learning rate = 0.0001\n","      4        \u001b[36m0.4889\u001b[0m       0.8378        0.6142  64.1857\n","learning rate = 0.0001\n","      5        \u001b[36m0.3729\u001b[0m       \u001b[32m0.8893\u001b[0m        \u001b[35m0.4219\u001b[0m  64.1052\n","learning rate = 0.0001\n","      6        \u001b[36m0.2939\u001b[0m       \u001b[32m0.9035\u001b[0m        \u001b[35m0.3735\u001b[0m  64.1061\n","learning rate = 0.0001\n","      7        \u001b[36m0.2399\u001b[0m       \u001b[32m0.9228\u001b[0m        \u001b[35m0.3330\u001b[0m  64.1172\n","learning rate = 0.0001\n","      8        \u001b[36m0.1951\u001b[0m       0.9222        \u001b[35m0.3069\u001b[0m  64.1844\n","learning rate = 0.0001\n","      9        \u001b[36m0.1722\u001b[0m       \u001b[32m0.9238\u001b[0m        0.3338  64.1134\n","learning rate = 0.0001\n","     10        \u001b[36m0.1456\u001b[0m       0.9117        0.3862  64.4008\n","learning rate = 0.0001\n","     11        \u001b[36m0.1283\u001b[0m       \u001b[32m0.9373\u001b[0m        \u001b[35m0.2704\u001b[0m  64.1401\n","learning rate = 1e-05\n","     12        \u001b[36m0.0505\u001b[0m       \u001b[32m0.9640\u001b[0m        \u001b[35m0.1637\u001b[0m  64.0492\n","learning rate = 1e-05\n","     13        \u001b[36m0.0268\u001b[0m       0.9635        0.1798  63.8960\n","learning rate = 1e-05\n","     14        \u001b[36m0.0183\u001b[0m       \u001b[32m0.9643\u001b[0m        0.1883  64.0546\n","learning rate = 1e-05\n","     15        \u001b[36m0.0133\u001b[0m       \u001b[32m0.9690\u001b[0m        0.1765  64.0803\n","learning rate = 1e-05\n","     16        \u001b[36m0.0100\u001b[0m       0.9665        0.1974  64.1605\n","learning rate = 1e-05\n","     17        \u001b[36m0.0087\u001b[0m       0.9683        0.1853  63.8733\n","learning rate = 1e-05\n","     18        \u001b[36m0.0064\u001b[0m       \u001b[32m0.9692\u001b[0m        0.1975  64.0594\n","learning rate = 1e-05\n","     19        \u001b[36m0.0058\u001b[0m       0.9690        0.2002  64.0101\n","learning rate = 1e-05\n","     20        \u001b[36m0.0057\u001b[0m       \u001b[32m0.9705\u001b[0m        0.1981  64.0446\n","drop ratio: 0.3\n","weight decay: 0.02\n","lowest train loss: 0.005706848986091414\n","lowest valid loss: 0.1636883688949359\n","max valid accuracy: 0.9705\n","time per epoch: 64.14280554056168\n","Total Time: 1285.27 s\n","fitting with weight decay of 0.02 and drop ratio of 0.5\n","learning rate = 0.0001\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.5131\u001b[0m       \u001b[32m0.5777\u001b[0m        \u001b[35m1.1249\u001b[0m  64.0134\n","learning rate = 0.0001\n","      2        \u001b[36m1.0254\u001b[0m       \u001b[32m0.6985\u001b[0m        \u001b[35m0.9668\u001b[0m  63.8103\n","learning rate = 0.0001\n","      3        \u001b[36m0.7374\u001b[0m       \u001b[32m0.8142\u001b[0m        \u001b[35m0.6523\u001b[0m  63.5392\n","learning rate = 0.0001\n","      4        \u001b[36m0.5490\u001b[0m       \u001b[32m0.8663\u001b[0m        \u001b[35m0.4963\u001b[0m  63.9305\n","learning rate = 0.0001\n","      5        \u001b[36m0.4336\u001b[0m       \u001b[32m0.8770\u001b[0m        \u001b[35m0.4639\u001b[0m  64.3337\n","learning rate = 0.0001\n","      6        \u001b[36m0.3435\u001b[0m       \u001b[32m0.8935\u001b[0m        \u001b[35m0.4119\u001b[0m  64.0596\n","learning rate = 0.0001\n","      7        \u001b[36m0.2944\u001b[0m       \u001b[32m0.9020\u001b[0m        0.4170  63.8582\n","learning rate = 0.0001\n","      8        \u001b[36m0.2438\u001b[0m       \u001b[32m0.9132\u001b[0m        \u001b[35m0.3705\u001b[0m  63.9289\n","learning rate = 0.0001\n","      9        \u001b[36m0.2135\u001b[0m       0.9100        0.3744  64.6374\n","learning rate = 0.0001\n","     10        \u001b[36m0.1809\u001b[0m       \u001b[32m0.9182\u001b[0m        \u001b[35m0.3388\u001b[0m  64.4880\n","learning rate = 0.0001\n","     11        \u001b[36m0.1602\u001b[0m       0.8862        0.4924  64.4939\n","learning rate = 1e-05\n","     12        \u001b[36m0.0686\u001b[0m       \u001b[32m0.9597\u001b[0m        \u001b[35m0.1769\u001b[0m  64.4947\n","learning rate = 1e-05\n","     13        \u001b[36m0.0419\u001b[0m       \u001b[32m0.9645\u001b[0m        \u001b[35m0.1760\u001b[0m  64.3025\n","learning rate = 1e-05\n","     14        \u001b[36m0.0307\u001b[0m       \u001b[32m0.9653\u001b[0m        0.1842  64.3798\n","learning rate = 1e-05\n","     15        \u001b[36m0.0241\u001b[0m       \u001b[32m0.9658\u001b[0m        0.1790  64.5759\n","learning rate = 1e-05\n","     16        \u001b[36m0.0201\u001b[0m       \u001b[32m0.9667\u001b[0m        0.1842  64.2650\n","learning rate = 1e-05\n","     17        \u001b[36m0.0149\u001b[0m       \u001b[32m0.9680\u001b[0m        0.1914  64.3351\n","learning rate = 1e-05\n","     18        \u001b[36m0.0147\u001b[0m       \u001b[32m0.9693\u001b[0m        0.1937  64.2780\n","learning rate = 1e-05\n","     19        \u001b[36m0.0118\u001b[0m       0.9693        0.2082  64.4782\n","learning rate = 1e-05\n","     20        \u001b[36m0.0110\u001b[0m       \u001b[32m0.9705\u001b[0m        0.1971  64.6652\n","drop ratio: 0.5\n","weight decay: 0.02\n","lowest train loss: 0.010984990937800225\n","lowest valid loss: 0.1760495078919145\n","max valid accuracy: 0.9705\n","time per epoch: 64.24337890148163\n","Total Time: 1287.36 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JCzBhS5-do6Y"},"source":["rawdata= {'drop_ratio': drop,'weight_decay': weight_decay, 'train_loss': train_loss, \n","          'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, \n","          'total_time': total_time}\n","weight_decay_dropout = pd.DataFrame(rawdata, columns = ['drop_ratio','weight_decay','train_loss',\n","                                            'valid_loss','valid_acc','time/epoch','total_time'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"xKd5R_tZd0e_","executionInfo":{"status":"ok","timestamp":1607114154150,"user_tz":300,"elapsed":531,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"700a8e38-04f0-43d4-d2fe-54f8a10c39db"},"source":["weight_decay_dropout"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>drop_ratio</th>\n","      <th>weight_decay</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time/epoch</th>\n","      <th>total_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.25</td>\n","      <td>0.005</td>\n","      <td>0.003668</td>\n","      <td>0.140723</td>\n","      <td>0.971667</td>\n","      <td>64.639849</td>\n","      <td>1295.397648</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.30</td>\n","      <td>0.005</td>\n","      <td>0.004286</td>\n","      <td>0.186233</td>\n","      <td>0.970167</td>\n","      <td>64.705091</td>\n","      <td>1296.712166</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.50</td>\n","      <td>0.005</td>\n","      <td>0.010104</td>\n","      <td>0.201967</td>\n","      <td>0.967833</td>\n","      <td>64.741995</td>\n","      <td>1297.569389</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.25</td>\n","      <td>0.010</td>\n","      <td>0.004672</td>\n","      <td>0.171801</td>\n","      <td>0.971167</td>\n","      <td>64.755277</td>\n","      <td>1297.747154</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.30</td>\n","      <td>0.010</td>\n","      <td>0.004685</td>\n","      <td>0.156763</td>\n","      <td>0.971833</td>\n","      <td>64.811586</td>\n","      <td>1298.888552</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.50</td>\n","      <td>0.010</td>\n","      <td>0.012142</td>\n","      <td>0.209047</td>\n","      <td>0.966667</td>\n","      <td>64.791136</td>\n","      <td>1298.472522</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.25</td>\n","      <td>0.020</td>\n","      <td>0.005192</td>\n","      <td>0.179316</td>\n","      <td>0.968667</td>\n","      <td>64.645580</td>\n","      <td>1295.463487</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.30</td>\n","      <td>0.020</td>\n","      <td>0.005707</td>\n","      <td>0.163688</td>\n","      <td>0.970500</td>\n","      <td>64.142806</td>\n","      <td>1285.274523</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.50</td>\n","      <td>0.020</td>\n","      <td>0.010985</td>\n","      <td>0.176050</td>\n","      <td>0.970500</td>\n","      <td>64.243379</td>\n","      <td>1287.356465</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   drop_ratio  weight_decay  train_loss  ...  valid_acc  time/epoch   total_time\n","0        0.25         0.005    0.003668  ...   0.971667   64.639849  1295.397648\n","1        0.30         0.005    0.004286  ...   0.970167   64.705091  1296.712166\n","2        0.50         0.005    0.010104  ...   0.967833   64.741995  1297.569389\n","3        0.25         0.010    0.004672  ...   0.971167   64.755277  1297.747154\n","4        0.30         0.010    0.004685  ...   0.971833   64.811586  1298.888552\n","5        0.50         0.010    0.012142  ...   0.966667   64.791136  1298.472522\n","6        0.25         0.020    0.005192  ...   0.968667   64.645580  1295.463487\n","7        0.30         0.020    0.005707  ...   0.970500   64.142806  1285.274523\n","8        0.50         0.020    0.010985  ...   0.970500   64.243379  1287.356465\n","\n","[9 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"vBYfC4A3d95n"},"source":["weight_decay_dropout.to_csv(\"weight_decay_dropout.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VYKLQeP-YW4"},"source":["# Test VGG 11,13,16,19\n","hypeparameter:\n","\n","1. learning rate: 1e-4\n","2. batch size: 32\n","3. drop_ratio = 0.25\n","4. kernel_size_CNN = (3, 3)\n","5. optimizer: Adam\n","6. LR_scheduler: ReduceLROnPlateau(default)\n","\n","\n","run for 20 epoch\n","\n"]},{"cell_type":"code","metadata":{"id":"Oby7UYiY-YW4","executionInfo":{"status":"ok","timestamp":1607350554092,"user_tz":300,"elapsed":363,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["learning_rate = 1e-4\n","batch_size = 32\n","max_epochs = 20"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyVvudgf-YW4","executionInfo":{"status":"ok","timestamp":1607350565859,"user_tz":300,"elapsed":10126,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["sample_number = 60000\n","dataset = MyDataset('./Train.pkl', './TrainLabels.csv',transform=img_transform, idx=np.arange(sample_number))\n","y_data = np.array([y for x, y in iter(dataset)])"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9_-RwFN502f","executionInfo":{"status":"ok","timestamp":1607350575623,"user_tz":300,"elapsed":682,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}}},"source":["from skorch.callbacks import LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from skorch.callbacks import Callback\n","\n","\n","\n","\n","#  callbacks function to check and change lr \n","callbacks=[\n","           ('lr_scheduler',\n","            LRScheduler(\n","                policy=ReduceLROnPlateau, \n","                monitor = \"train_loss\",\n","                )\n","            )\n","          ]\n","          \n","Network_type = [\"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\"]\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEZZBW1N-YW4","colab":{"base_uri":"https://localhost:8080/","height":602},"executionInfo":{"status":"error","timestamp":1607350776146,"user_tz":300,"elapsed":198107,"user":{"displayName":"Anne Hazel","photoUrl":"","userId":"13506632921945652600"}},"outputId":"6e67fd40-3d7c-4dc1-bea1-3aa365628738"},"source":["for i in Network_type:\n","\n","  print (\"testing: \", i, \" now\")\n","  network = VGG_net(in_channels=num_in, num_classes=num_class, VGG_type = VGG_types[i]).to(device)\n","\n","  cnn = NeuralNetClassifier(\n","      network,\n","      max_epochs = max_epochs,\n","      lr= learning_rate,\n","\n","      # AdamW\n","      optimizer = torch.optim.Adam,\n","      # optimizer__weight_decay=weight_decay,\n","\n","      batch_size = 32,\n","      device=device,\n","      iterator_train__num_workers = 4,\n","      iterator_valid__num_workers = 4,\n","      callbacks=callbacks,\n","  )\n","\n","  # record start time\n","  startall = time.time()\n","\n","  # training\n","  cnn.fit(dataset, y=y_data)\n","\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  total_time.append(timeall)\n","  \n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["testing:  VGG11  now\n","  epoch    train_loss    valid_acc    valid_loss      dur\n","-------  ------------  -----------  ------------  -------\n","      1        \u001b[36m1.4828\u001b[0m       \u001b[32m0.4818\u001b[0m        \u001b[35m1.2965\u001b[0m  59.9495\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["      2        \u001b[36m0.8756\u001b[0m       \u001b[32m0.7857\u001b[0m        \u001b[35m0.6647\u001b[0m  60.6434\n","      3        \u001b[36m0.5055\u001b[0m       \u001b[32m0.8208\u001b[0m        \u001b[35m0.5734\u001b[0m  60.0242\n","lowest train loss: 0.5054536774843321\n","lowest valid loss: 0.5734252993407775\n","max valid accuracy: 0.820839580209895\n","time per epoch: 60.20568974812826\n","Total Time: 190.09 s\n","testing:  VGG13  now\n"],"name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-92140b8f897e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mtimeall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendall\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstartall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mvalid_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mvalid_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skorch/history.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    220\u001b[0m                      for batches in items]\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_filter_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyerror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# extract the epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Key 'train_loss' was not found in history.\""]}]},{"cell_type":"code","metadata":{"id":"bxEaNnWl-YW5"},"source":["import pandas as pd\n","rawdata= {'Network_type': Network_type, 'train_loss': train_loss, 'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, 'total_time': total_time}\n","VGG_compare = pd.DataFrame(rawdata, columns = ['Network_type','train_loss','valid_loss','valid_acc','time/epoch','total_time'])\n","print (VGG_compare)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Okv1DG0-YW5"},"source":["VGG_compare.to_csv(\"VGG_compare.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7XLz1ovRmZFF"},"source":["# Test optimizer: Adam, AdamW, SGD_momentum, SGD_momentum_nesterov\n","hypeparameter:\n","\n","1. learning rate: 1e-4\n","2. batch size: 32\n","3. drop_ratio = 0.25\n","4. kernel_size_CNN = (3, 3)\n","5. structure : VGG-16\n","6. LR_scheduler: ReduceLROnPlateau(default)\n","\n","run for 20 epoch\n","\n","# optimizer hyperparmeter\n","\n","1. Adam\n","2. Adam: weight decay = 1e-3\n","3. SGD_momentum: momentum =0.99, weight decay = 1e-3\n","4. SGD_momentum_nesterov : momentum =0.99, weight decay = 1e-3, nesterov = true\n","\n"]},{"cell_type":"code","metadata":{"id":"3Tt4sc2bmZFF"},"source":["learning_rate = 1e-4\n","batch_size = 32\n","max_epochs = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7gLnZECVmZFF"},"source":["sample_number = 60000\n","dataset = MyDataset('./Train.pkl', './TrainLabels.csv',transform=img_transform, idx=np.arange(sample_number))\n","y_data = np.array([y for x, y in iter(dataset)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oc-RUZlVmZFF"},"source":["from skorch.callbacks import LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from skorch.callbacks import Callback\n","\n","torch.manual_seed(0)\n","\n","#  callbacks function to check and change lr \n","callbacks=[\n","           ('lr_scheduler',\n","            LRScheduler(\n","                policy=ReduceLROnPlateau, \n","                monitor = \"train_loss\",\n","                )\n","            )\n","          ]\n","          \n","optimizer_type = [\"Adam\", \"AdamW\", \"SGD_momentum\", \"SGD_momentum_nesterov\"]\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOk15QqTmZFG"},"source":["for i in optimizer_type:\n","\n","  print (\"testing: \", i, \" now\")\n","  network = VGG_net(in_channels=num_in, num_classes=num_class, VGG_type = VGG_types[\"VGG16\"]).to(device)\n","\n","  if (i==\"Adam\"):\n","    cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.Adam,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  elif (i == \"AdamW\"):\n","     cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.AdamW,\n","        optimizer__weight_decay=weight_decay,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  elif (i == \"SGD_momentum\"):\n","     cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.SGD,\n","        optimizer__weight_decay = weight_decay,\n","        optimizer__momentum = 0.9,\n","        optimizer__nesterov = False,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  elif (i == \"SGD_momentum_nesterov\"):\n","    cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.SGD,\n","        optimizer__weight_decay = weight_decay,\n","        optimizer__momentum = 0.9,\n","        optimizer__nesterov = True,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","\n","  # record start time\n","  startall = time.time()\n","\n","  # training\n","  cnn.fit(dataset, y=y_data)\n","\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  total_time.append(timeall)\n","  \n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xwq2IWvdmZFG"},"source":["import pandas as pd\n","rawdata= {'optimizer_type': optimizer_type, 'train_loss': train_loss, 'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, 'total_time': total_time}\n","VGG_compare = pd.DataFrame(rawdata, columns = ['optimizer_type','train_loss','valid_loss','valid_acc','time/epoch','total_time'])\n","print (VGG_compare)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fz70VkrUmZFG"},"source":["VGG_compare.to_csv(\"optimizer_compare.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8w8wS9u-Neff"},"source":["Wait until SGD converges..."]},{"cell_type":"code","metadata":{"id":"f9OR3k74E3Kw"},"source":["optimizer_type = [\"SGD_momentum\", \"SGD_momentum_nesterov\"]\n","\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkFuP_5ENk_A"},"source":["max_epochs = 50\n","\n","callbacks=[\n","        ('lr_scheduler',\n","        LRScheduler(\n","            policy=ReduceLROnPlateau, \n","            factor = 0.5,\n","            monitor = \"train_loss\",\n","            )\n","        )\n","      ]\n","\n","\n","for i in optimizer_type:\n","\n","  print (\"testing: \", i, \" now\")\n","  network = VGG_net(in_channels=num_in, num_classes=num_class, VGG_type = VGG_types[\"VGG16\"]).to(device)\n","\n","  if (i==\"Adam\"):\n","    cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.Adam,\n","        batch_size = 20,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  elif (i == \"AdamW\"):\n","     cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.AdamW,\n","        optimizer__weight_decay=weight_decay,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  elif (i == \"SGD_momentum\"):\n","     cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.SGD,\n","        optimizer__weight_decay = 0,\n","        optimizer__momentum = 0,\n","        optimizer__nesterov = False,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  elif (i == \"SGD_momentum_nesterov\"):\n","    cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.SGD,\n","        optimizer__weight_decay = 0,\n","        optimizer__momentum = 0,\n","        optimizer__nesterov = True,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","\n","  # record start time\n","  startall = time.time()\n","\n","  # training\n","  cnn.fit(dataset, y=y_data)\n","\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  total_time.append(timeall)\n","  \n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gU0whRPt9gfW"},"source":["import pandas as pd\n","rawdata= {'train_loss': train_loss, 'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, 'total_time': total_time}\n","VGG_compare = pd.DataFrame(rawdata, columns = ['train_loss','valid_loss','valid_acc','time/epoch','total_time'])\n","print (VGG_compare)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3uWZ2pfE317"},"source":["# Test LR_scheduler: CyclicLR_triangular\n","hypeparameter:\n","\n","1. learning rate: 1e-4\n","2. batch size: 32\n","3. drop_ratio = 0.25\n","4. kernel_size_CNN = (3, 3)\n","5. structure : VGG-16\n","6. optimizer : Adam\n","\n","\n","run for 20 epoch\n","\n","# LR_scheduler hyperparmeter\n","referecne: https://medium.com/swlh/cyclical-learning-rates-the-ultimate-guide-for-setting-learning-rates-for-neural-networks-3104e906f0ae\n","\n","fixed parameter without momentum\n","\n","base_lr = 1e-5,\n","max_lr = 1e-4,\n","step_size_up = 5,\n","\n","\n","1. CyclicLR_triangular\n","2. CyclicLR_triangular2\n","3. CyclicLR_exp_range\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"xkrE2XHmE317"},"source":["learning_rate = 1e-4\n","batch_size = 32\n","max_epochs = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSS6zYoOE317"},"source":["sample_number = 60000\n","dataset = MyDataset('./Train.pkl', './TrainLabels.csv',transform=img_transform, idx=np.arange(sample_number))\n","y_data = np.array([y for x, y in iter(dataset)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KT-ps1fjE317"},"source":["from skorch.callbacks import LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from skorch.callbacks import Callback\n","\n","class Monitor(Callback):\n","    def on_epoch_end(self, network, **kwargs):\n","      print(\"current learning rate: \", network.optimizer_.param_groups[0]['lr'])\n","\n","torch.manual_seed(0) \n","CyclicLR_type = [\"CyclicLR_triangular\", \"CyclicLR_triangular2\", \"CyclicLR_exp_range\"]\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbUzy9YwE317"},"source":["for i in CyclicLR_type:\n","\n","  print (\"testing: \", i, \" now\")\n","  network = VGG_net(in_channels=num_in, num_classes=num_class, VGG_type = VGG_types[\"VGG16\"]).to(device)\n","\n","  if (i==\"CyclicLR_triangular\"):\n","    callbacks=[\n","        ('print', Monitor()), \n","        ('lr_scheduler',\n","        LRScheduler(\n","            policy=torch.optim.lr_scheduler.CyclicLR, \n","            base_lr = 1e-5,\n","            max_lr = 1e-4,\n","            step_size_up = 5,\n","            cycle_momentum=False,\n","            mode='triangular',\n","            monitor = \"train_loss\",\n","            )\n","        )\n","      ]\n","  elif (i == \"CyclicLR_triangular2\"):\n","    callbacks=[\n","        ('print', Monitor()), \n","        ('lr_scheduler',\n","        LRScheduler(\n","            policy=torch.optim.lr_scheduler.CyclicLR, \n","            base_lr = 1e-5,\n","            max_lr = 1e-4,\n","            step_size_up = 5,\n","            cycle_momentum=False,\n","            mode='triangular2',\n","            monitor = \"train_loss\",\n","            )\n","        )\n","      ]\n","  elif (i == \"CyclicLR_exp_range\"):\n","    callbacks=[\n","        ('print', Monitor()), \n","        ('lr_scheduler',\n","        LRScheduler(\n","            policy=torch.optim.lr_scheduler.CyclicLR, \n","            base_lr = 1e-5,\n","            max_lr = 1e-4,\n","            step_size_up = 5,\n","            cycle_momentum=False,\n","            mode='exp_range',\n","            monitor = \"train_loss\",\n","            )\n","        )\n","      ]\n","\n","  cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.Adam,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  # record start time\n","  startall = time.time()\n","\n","  # training\n","  cnn.fit(dataset, y=y_data)\n","\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  total_time.append(timeall)\n","  \n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HUIvfPME318"},"source":["import pandas as pd\n","rawdata= {'CyclicLR_type': CyclicLR_type, 'train_loss': train_loss, 'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, 'total_time': total_time}\n","VGG_compare = pd.DataFrame(rawdata, columns = ['CyclicLR_type','train_loss','valid_loss','valid_acc','time/epoch','total_time'])\n","print (VGG_compare)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxfp6mrZE318"},"source":["VGG_compare.to_csv(\"CyclicLR_type_compare.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRNFbIt8cf61"},"source":["# Test LR_scheduler: ReduceLROnPlateau\n","hypeparameter:\n","\n","1. learning rate: 1e-4\n","2. batch size: 32\n","3. drop_ratio = 0.25\n","4. kernel_size_CNN = (3, 3)\n","5. structure : VGG-16\n","6. optimizer : Adam\n","\n","\n","run for 20 epoch\n","\n","# ReduceLROnPlateau hyperparmeter\n","\n","1. ReduceLROnPlateau_default\n","2. ReduceLROnPlateau_fatcor_0.9_patience_4\n","3. ReduceLROnPlateau_fatcor_0.5_patience_4\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"iWx0V69Zcf61"},"source":["learning_rate = 1e-4\n","batch_size = 32\n","max_epochs = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nopOXElTcf61"},"source":["sample_number = 60000\n","dataset = MyDataset('./Train.pkl', './TrainLabels.csv',transform=img_transform, idx=np.arange(sample_number))\n","y_data = np.array([y for x, y in iter(dataset)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1B-hRezcf62"},"source":["from skorch.callbacks import LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from skorch.callbacks import Callback\n","\n","torch.manual_seed(0)\n","          \n","ReduceLROnPlateau_type = [\"ReduceLROnPlateau_default\", \"ReduceLROnPlateau_fatcor_0.9_patience_4\", \"ReduceLROnPlateau_fatcor_0.5_patience_4\"]\n","train_loss = []\n","valid_loss = []\n","valid_acc = []\n","dur = []\n","total_time = []\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3l_zkGn9cf62"},"source":["for i in ReduceLROnPlateau_type:\n","\n","  print (\"testing: \", i, \" now\")\n","  network = VGG_net(in_channels=num_in, num_classes=num_class, VGG_type = VGG_types[\"VGG16\"]).to(device)\n","\n","  if (i==\"ReduceLROnPlateau_default\"):\n","    callbacks=[\n","           ('lr_scheduler',\n","            LRScheduler(\n","                policy=ReduceLROnPlateau, \n","                monitor = \"train_loss\",\n","                )\n","            )\n","          ]\n","  elif (i == \"ReduceLROnPlateau_fatcor_0.9_patience_4\"):\n","    callbacks=[\n","           ('lr_scheduler',\n","            LRScheduler(\n","                policy=ReduceLROnPlateau, \n","                factor = 0.9,\n","                patience = 4, \n","                monitor = \"train_loss\",\n","                )\n","            )\n","          ]\n","  elif (i == \"ReduceLROnPlateau_fatcor_0.5_patience_4\"):\n","    callbacks=[\n","           ('lr_scheduler',\n","            LRScheduler(\n","                policy=ReduceLROnPlateau, \n","                factor = 0.5,\n","                patience = 4, \n","                monitor = \"train_loss\",\n","                )\n","            )\n","          ]\n","\n","  cnn = NeuralNetClassifier(\n","        network,\n","        max_epochs = max_epochs,\n","        lr= learning_rate,\n","        optimizer = torch.optim.Adam,\n","        batch_size = 32,\n","        device=device,\n","        iterator_train__num_workers = 4,\n","        iterator_valid__num_workers = 4,\n","        callbacks=callbacks,\n","    )\n","  # record start time\n","  startall = time.time()\n","\n","  # training\n","  cnn.fit(dataset, y=y_data)\n","\n","  endall = time.time()\n","  timeall = endall-startall\n","\n","  train_loss.append(np.min(cnn.history[:, 'train_loss']))\n","  valid_loss.append(np.min(cnn.history[:, 'valid_loss']))\n","  valid_acc.append(np.max(cnn.history[:, 'valid_acc']))\n","  dur.append(np.average(cnn.history[:, 'dur']))\n","  total_time.append(timeall)\n","  \n","  print(f\"lowest train loss: {np.min(cnn.history[:, 'train_loss'])}\")\n","  print(f\"lowest valid loss: {np.min(cnn.history[:, 'valid_loss'])}\")\n","  print(f\"max valid accuracy: {np.max(cnn.history[:, 'valid_acc'])}\")\n","  print(f\"time per epoch: {np.average(cnn.history[:, 'dur'])}\")\n","  print(f\"Total Time: {timeall:.2f} s\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nancv7vZcf63"},"source":["import pandas as pd\n","rawdata= {'ReduceLROnPlateau_type': ReduceLROnPlateau_type, 'train_loss': train_loss, 'valid_loss': valid_loss, 'valid_acc': valid_acc, 'time/epoch': dur, 'total_time': total_time}\n","VGG_compare = pd.DataFrame(rawdata, columns = ['ReduceLROnPlateau_type','train_loss','valid_loss','valid_acc','time/epoch','total_time'])\n","print (VGG_compare)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3R-2lAYRcf63"},"source":["VGG_compare.to_csv(\"ReduceLROnPlateau_type_compare.csv\",index=False, header=True)"],"execution_count":null,"outputs":[]}]}